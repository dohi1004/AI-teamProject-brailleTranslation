{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohi1004/AI-teamProject-brailleTranslation/blob/main/AI_FinalProject_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# **AI Team Project Final Presentation**\n",
        "\n",
        "### **Team 6**\n",
        "\n",
        "16102269 Kim Jong Gyu\n",
        "\n",
        "19102095 Lee Do Hui\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "VpRu5GlOLhbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Motivation For The Proeject**\n",
        "* Similar to general language, braille is different according to the world.\n",
        "* T\n",
        "\n",
        "시각 장애인이 외국어로 되어 있는 점자를 읽어야 하는 경우 해당 나라 언어의 점자를 배운 적이 없다면 읽을 수 없습니다. 따라서, 외국어 점자를 모국어 음성으로 변환해주는 application을 만들고자 합니다."
      ],
      "metadata": {
        "id": "xz2a97bVLqBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Problem Description**\n",
        "Here, we suppose the use case as follows. Here, we use similar word order languages (English, French) because of using basic NMT model.\n",
        "\n",
        "1) English blind \n",
        "\n",
        "2) French\n",
        "\n",
        "1. English blind write braille \n",
        "2. Translate English braille to English (with Braille Translation)\n",
        "3. Translate English to French (with Neural Machine Translation)\n",
        "\n",
        "\n",
        "**Braille Translation and Language Translation**\n",
        "\n",
        "### 1) Braille Translation\n",
        "\n",
        "Translate English braille to English.\n",
        "\n",
        "### 2) Language Translation\n",
        "\n",
        "Translate English to French.\n",
        "\n",
        "**Finally, our application can translate other language's braille to target person's language by connecting those two models.** \n",
        "\n"
      ],
      "metadata": {
        "id": "sXhdwLhc6aJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Description**\n",
        "\n",
        "### **Braille Translation**\n",
        "*   ~~Preferentially, I use braille notation in English dataset in Kaggle~~\n",
        "  *   ~~Reference : https://www.kaggle.com/code/kwisatzhaderach/braille-classifier-keras/data~~\n",
        "\n",
        "### **Neural Machine Translation**\n",
        "\n",
        "*   For neural machine translation model, we used dataset comprised of French phrases and their English counterparts. The dataset is available from the http://www.manythings.org/anki/, with examples drawn from the Tatoeba Project.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1zgHVKVLXsjyD4re6bWE5baV0q71J4yQo\" height = 250 width = 500>\n",
        "\n",
        "* The fra.txt file contains pairs of English to French phrases, one pair per line with a tab separating the language. Total pairs of sentences are 197,463.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1-WO41TvUqFickYyjN298293hUtBSZRUs\" height = 150 width = 600>\n",
        "\n",
        "------------------\n",
        "Preprocessing process of each data is explained in **5. Implementation Details**\n",
        "\n",
        "------------------"
      ],
      "metadata": {
        "id": "8U0pwhGVLr70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.Model Architecture**\n",
        "\n",
        "### 1) Model 1: Convolutional Neural Network for Braille Translation\n",
        "\n",
        "a model consisting of ~~~~> \n",
        "\n",
        "### 2) Model 2: Sequence-to-Sequence model - LSTM & GRU for Neural Machine Translation\n",
        "**Encoder-Decoder based model**\n",
        "다양한 길이의 input/output 을 동일한 길이의 memory에 할당한다.\n",
        "-> 그래서 input 의 길이는 다양해도 상관 없음 (Maps variable-length sequence to fixed-length memory)\n",
        "--------------------\n",
        "* Input Language: English\n",
        "\n",
        "* Output Language: French\n",
        "---------------\n",
        "\n",
        "1. Encoder\n",
        "\n",
        "  It reads the input sequence and summarizes the information in one vector, called context vector (in LSTM, hidden state and cell state vectors). This context vector aims to contain all information for all input elements to help the decoder make accurate predictions. The hidden and cell state of the network is passed along to the decoder as input. \n",
        "\n",
        "2. Decoder\n",
        "\n",
        "  It interprets the context vector obtained from the encoder. The context vecotr of the encoder's final cell is input to the first cell of the decoder network. Using these initial states, the decoder starts generating the output sequence, and these outputs are also considered for futrue predictions. \n",
        "\n",
        "  \n",
        "Our problem is multiclass classification, so we use softmax activation function and cross entropy for the output layer. \n",
        "\n",
        "Our decoder_outputs aren't one-hot encoded, so they are integer labels. Therefore, we use sparse_categorical_crossentropy for loss function to solve the multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1XChI2rJDoa6r8nL7hcBEELICLT-bDK8N\" height = 300 width = 800>\n",
        "\n",
        "----------------------\n",
        "\n",
        "\n",
        "3) Final Architecture\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1PRGd8u2JUUeMloKnRxjW9zLl3WaDxJlu\" height = 400 width = 850>"
      ],
      "metadata": {
        "id": "s9FaYZLN_vex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Implementation Details**"
      ],
      "metadata": {
        "id": "PzhphPvU7WkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part1. Braille Translation Model** "
      ],
      "metadata": {
        "id": "xOVbGzraM_qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Part2. NMT model (English -> French)**\n"
      ],
      "metadata": {
        "id": "sqeK8VzxOlmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZ0kYVb8ioM",
        "outputId": "72e7c395-e336-457e-be59-75956aec4dca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "gPZO7fF-huhk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**\n",
        "\n",
        "* Parallel corpus data \n",
        "\n",
        "- source: English\n",
        "\n",
        "- target: French\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-5Aa2bshpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 0\n",
        "with open(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", \"r\", encoding='UTF-8') as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "            length+=1\n",
        "print(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Y3A2UKmsSu",
        "outputId": "676668bb-ff77-4a33-98f4-41de38606f8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", delimiter = \"\\t\")\n",
        "data.columns = [\"en\", \"fr\", \"cc\"]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LLk6wspjpG2Q",
        "outputId": "f0ec3c95-84a6-40cc-b176-75463adf6180"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       en  \\\n",
              "0                                                     Go.   \n",
              "1                                                     Go.   \n",
              "2                                                     Go.   \n",
              "3                                                     Hi.   \n",
              "4                                                     Hi.   \n",
              "...                                                   ...   \n",
              "197457  A carbon footprint is the amount of carbon dio...   \n",
              "197458  Death is something that we're often discourage...   \n",
              "197459  Since there are usually multiple websites on a...   \n",
              "197460  If someone who doesn't know your background sa...   \n",
              "197461  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                       fr  \\\n",
              "0                                                 Marche.   \n",
              "1                                              En route !   \n",
              "2                                                 Bouge !   \n",
              "3                                                 Salut !   \n",
              "4                                                  Salut.   \n",
              "...                                                   ...   \n",
              "197457  Une empreinte carbone est la somme de pollutio...   \n",
              "197458  La mort est une chose qu'on nous décourage sou...   \n",
              "197459  Puisqu'il y a de multiples sites web sur chaqu...   \n",
              "197460  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
              "197461  Il est peut-être impossible d'obtenir un Corpu...   \n",
              "\n",
              "                                                       cc  \n",
              "0       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "1       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "2       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "3       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "4       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "...                                                   ...  \n",
              "197457  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "197458  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "197459  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "197460  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "197461  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "\n",
              "[197462 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-698f4821-f356-4620-81bd-ba5404e52a86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>cc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>En route !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197457</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197458</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197459</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197460</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197461</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197462 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-698f4821-f356-4620-81bd-ba5404e52a86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-698f4821-f356-4620-81bd-ba5404e52a86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-698f4821-f356-4620-81bd-ba5404e52a86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "JE0m29pUlf4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "    # delete accent in French\n",
        "    # EX) : 'déjà diné' -> deja dine\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "    # Call accent deletion function\n",
        "    sent = to_ascii(sent.lower())\n",
        "    \n",
        "    # Add blank between word and punctuation \n",
        "    # ex) \"I am a student.\" => \"I am a student .\"\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "    \n",
        "    # Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), transform others to blank\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    \n",
        "    # Transform multiple blanks to one blank\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    \n",
        "    return sent"
      ],
      "metadata": {
        "id": "x0WkR49X9Qq8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Test\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('Before preprocessing English sentence :', en_sent)\n",
        "print('After preprocessing English sentence :',preprocess_sentence(en_sent))\n",
        "print('Before preprocessing French sentence :', fr_sent)\n",
        "print('After preprocessing French sentence :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJdUEWO39Sfq",
        "outputId": "2bc20993-2da4-48d4-e960-d5a38ae0b0e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing English sentence : Have you had dinner?\n",
            "After preprocessing English sentence : have you had dinner ?\n",
            "Before preprocessing French sentence : Avez-vous déjà diné?\n",
            "After preprocessing French sentence : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "    src_seqlen = 0\n",
        "    tar_seqlen = 0\n",
        "    with open(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", \"r\", encoding='UTF-8') as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "            # split source data and target data \n",
        "            src_line, tar_line, _ = line.strip().split('\\t')\n",
        "            \n",
        "            # preprocess source data\n",
        "            preprocessed_sen = preprocess_sentence(src_line)\n",
        "            src_line = [w for w in preprocessed_sen.split()]\n",
        "            for w in preprocessed_sen.split():\n",
        "              if len(w) > src_seqlen:\n",
        "                src_seqlen = len(w)\n",
        "            \n",
        "            # preprocess target data -> we'll use teacher forcing in training phase, so divide decoder's input sequence and original value (label)\n",
        "            tar_line = preprocess_sentence(tar_line)\n",
        "            for w in tar_line.split():\n",
        "              if len(w) > tar_seqlen:\n",
        "                tar_seqlen = len(w)\n",
        "            #  For target sequence (French) adding <sos> meaning start and <eos> meaning end.  \n",
        "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "            \n",
        "            encoder_input.append(src_line)\n",
        "            decoder_input.append(tar_line_in)\n",
        "            decoder_target.append(tar_line_out)\n",
        "            \n",
        "            if i == num_samples - 1:\n",
        "                break\n",
        "        \n",
        "    return encoder_input, decoder_input, decoder_target, src_seqlen, tar_seqlen"
      ],
      "metadata": {
        "id": "SU0FM-yw9T_K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 70000 # we use 70000 among about 190000 data"
      ],
      "metadata": {
        "id": "Kx4H1PvY9sdC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 samples\n",
        "sents_en_in, sents_fra_in, sents_fra_out, src_seqlen, tar_seqlen = load_preprocessed_data()\n",
        "print('Input of Encoder :',sents_en_in[:5])\n",
        "print('Input of Decoder :',sents_fra_in[:5])\n",
        "print('Label of Decoder :',sents_fra_out[:5])\n",
        "print(src_seqlen)\n",
        "print(tar_seqlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEOxVdso9S4m",
        "outputId": "a5c04dd7-c841-457e-ebf9-9c0458a3b76e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input of Encoder : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "Input of Decoder : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "Label of Decoder : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n",
            "16\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teacher Forcing\n",
        "\n",
        "* Technique where the target word(ground truth) is passed as the next input to the decoder \n",
        "\n",
        "\n",
        "### Why it is needed?\n",
        "\n",
        "* The problem with training recurrent neural networks that use output from prior time steps as input.\n",
        "\n",
        "-> There is a risk that every single part part fails even though only one mistake in the first one. (previous decoder cell's prediction is wrong, this affects entire prediction wrong.) \n",
        "\n",
        "-> Analogy : a teacher records the score for each individual part and then tells the student the correct answre, to be used in the next part. (giving real value instead of predicted previous value.)"
      ],
      "metadata": {
        "id": "Sgmo_DqhnECX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate set of word with keras tokenizer and do integer encoding and padding \n",
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "vR3hqwy79lSe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of each data\n",
        "print('Input of Encoder Shape :',encoder_input.shape)\n",
        "print('Input of Decoder Shape :',decoder_input.shape)\n",
        "print('Label of Decoder Shape :',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lErUV8Rn9xo1",
        "outputId": "3a2c6390-82b9-4b2f-e13d-5629e578f56d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input of Encoder Shape : (70000, 9)\n",
            "Input of Decoder Shape : (70000, 17)\n",
            "Label of Decoder Shape : (70000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of word set \n",
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"Size of English word set : {:d}, Size of French word set : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg6RgZNr9z3W",
        "outputId": "489f685d-e807-49ee-a198-f3a53ee3cd0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of English word set : 7241, Size of French word set : 12336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "484gZQKC903s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before spliting dataset, randomly mixing integer sequence list. \n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('Random Sequence :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT6KNypQ91_H",
        "outputId": "da60a118-95ba-41d6-a733-f5537274878f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Sequence : [29848 11081 63137 ... 22425  4389 22922]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set it to dataset's order -> samples are mixed unlike original order \n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "Tx2Ty3Td93G6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print randomly selected sample. \n",
        "# Here, decoder_input and decoder_target should have same integer sequence except <sos> and <eos> token.\n",
        "print(encoder_input[40111])\n",
        "print(decoder_input[40111])\n",
        "print(decoder_target[40111])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLFLtof494In",
        "outputId": "99b3be8a-343f-41eb-e2e5-996da85a31a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  6  84  11 290   5 389   1   0   0]\n",
            "[  2  10  22   7 122 310   1   0   0   0   0   0   0   0   0   0   0]\n",
            "[ 10  22   7 122 310   1   3   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of validation dataset (10%) \n",
        "n_of_val = int(num_samples*0.1)\n",
        "print('Number of validation data :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQnAwAoP94_z",
        "outputId": "88ba41f8-04e8-4fd8-d646-8bf9e53bd7d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation data : 7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide train (90%) and test data (10%)\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "CEdPf9RJ96Bk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of traning and test data\n",
        "print('Shape of training source data :',encoder_input_train.shape)\n",
        "print('Shape of training target data :',decoder_input_train.shape)\n",
        "print('Shape of trainig target label data :',decoder_target_train.shape)\n",
        "print('Shape of test source data :',encoder_input_test.shape)\n",
        "print('Shape of test target data :',decoder_input_test.shape)\n",
        "print('Shape of test target label data :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfYr08J397LQ",
        "outputId": "850ff9d9-0b7e-4170-b37a-b14985a014a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training source data : (63000, 9)\n",
            "Shape of training target data : (63000, 17)\n",
            "Shape of trainig target label data : (63000, 17)\n",
            "Shape of test source data : (7000, 9)\n",
            "Shape of test target data : (7000, 17)\n",
            "Shape of test target label data : (7000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM model**"
      ],
      "metadata": {
        "id": "SCoN6g7Wkphi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "_Fuxi-5R98K4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_Model(embedding_dim, hidden_units, lr, b1, b2, batchsize, encoder_dropout, decoder_dropout):\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # Embedding layer\n",
        "  enc_masking = Masking(mask_value=0.0)(enc_emb) # Exclude padding 0 in operation\n",
        "  encoder_lstm = LSTM(hidden_units, return_state=True, dropout=encoder_dropout) # To return state value\n",
        "  encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # Return hidden state and cell state\n",
        "  encoder_states = [state_h, state_c] # Save encoder's hidden state and cell state -> to decoder!\n",
        "\n",
        "  # Decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # Embedding layer\n",
        "  dec_emb = dec_emb_layer(decoder_inputs) # exclude padding 0 in operation\n",
        "  dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "  # To return state value, return_state = True\n",
        "  # To predict word for every time step, return_sequences = True\n",
        "  decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=decoder_dropout) \n",
        "\n",
        "  # Use encoder's hidden state as initial hidden state \n",
        "  decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                      initial_state=encoder_states)\n",
        "\n",
        "  # predict word bsaed on softmax activation function for all results from every time step\n",
        "  decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # Model's input and output \n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 3)], epochs=10) # for testing, we use epochs = 10 \n",
        "\n",
        "  return history.history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "IRoJKkCp99Mj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "ureNg0IerAW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyper_param(n_iteration):\n",
        "  hyper_param = []  # learning_rate, n_hidden, timestep, epochs\n",
        "  for i in range(n_iteration):\n",
        "    current_params = []\n",
        "    # We use adam optimizer, so we tune learning rate, b1, b2\n",
        "    current_params.append(np.random.uniform(0,0.1)) # learning rate \n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b1 (from Momentum)\n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b2 (from RMSProp)\n",
        "    current_params.append(np.random.randint(1,513)) # hidden units \n",
        "    current_params.append(np.random.randint(32,513)) # batch_size\n",
        "    current_params.append(np.random.randint(50,300)) # embedding dimension\n",
        "    current_params.append(np.random.uniform(0,0.5)) # encoder dropout\n",
        "    current_params.append(np.random.uniform(0,0.5)) # decoder dropout\n",
        "    hyper_param.append(current_params)\n",
        "  return hyper_param\n",
        "\n",
        "hyper_parameter = get_hyper_param(30)\n",
        "train_loss, val_loss = list(), list()\n",
        "best_params = []\n",
        "\n",
        "min_val_loss = 999\n",
        "for alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout in hyper_parameter:\n",
        "  print('lr',alpha, 'b1', b1, 'b2', b2, 'n_hiddens', hidden_units,'batch_size', batch_size, 'embedding_dim', embedding_dim, \n",
        "        'en_dropout',  en_dropout, 'de_dropout', de_dropout)\n",
        "\n",
        "  current_val_loss = LSTM_Model(embedding_dim, hidden_units, alpha, b1, b2, batch_size, en_dropout, de_dropout)\n",
        "\n",
        "  if current_val_loss < min_val_loss:\n",
        "    min_val_loss = current_val_loss\n",
        "    best_params = [alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout]\n",
        "    print('best_params',best_params)\n",
        "  \n",
        "print('final best params',best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-ic7p5kxBA4",
        "outputId": "219ae8b9-b226-4b63-ce29-979e14b44ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 0.06270461188149924 b1 0.843482643498749 b2 0.8925307649479214 n_hiddens 376 batch_size 455 embedding_dim 225 en_dropout 0.25149046826372107 de_dropout 0.28754374782314557\n",
            "Epoch 1/10\n",
            "139/139 [==============================] - 30s 161ms/step - loss: 1.9090 - acc: 0.7286 - val_loss: 1.3475 - val_acc: 0.7799\n",
            "Epoch 2/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 1.2717 - acc: 0.7855 - val_loss: 1.2778 - val_acc: 0.7887\n",
            "Epoch 3/10\n",
            "139/139 [==============================] - 20s 141ms/step - loss: 1.2140 - acc: 0.7914 - val_loss: 1.2685 - val_acc: 0.7928\n",
            "Epoch 4/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 1.1871 - acc: 0.7942 - val_loss: 1.2826 - val_acc: 0.7924\n",
            "Epoch 5/10\n",
            "139/139 [==============================] - 19s 139ms/step - loss: 1.1731 - acc: 0.7954 - val_loss: 1.2741 - val_acc: 0.7956\n",
            "Epoch 6/10\n",
            "139/139 [==============================] - 19s 139ms/step - loss: 1.1693 - acc: 0.7960 - val_loss: 1.2842 - val_acc: 0.7953\n",
            "best_params [0.06270461188149924, 0.843482643498749, 0.8925307649479214, 376, 455, 225, 0.25149046826372107, 0.28754374782314557]\n",
            "lr 0.02118024944924988 b1 0.9389073612586014 b2 0.6943851192685523 n_hiddens 180 batch_size 483 embedding_dim 267 en_dropout 0.26315532566808136 de_dropout 0.45960397092538496\n",
            "Epoch 1/10\n",
            "131/131 [==============================] - 23s 126ms/step - loss: 4.7951 - acc: 0.6921 - val_loss: 5.7771 - val_acc: 0.7042\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 11.5435 - acc: 0.6752 - val_loss: 10.9803 - val_acc: 0.6951\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 15s 112ms/step - loss: 27.6251 - acc: 0.6610 - val_loss: 19.6772 - val_acc: 0.6848\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 14s 107ms/step - loss: 27.9606 - acc: 0.6590 - val_loss: 15.0826 - val_acc: 0.6867\n",
            "lr 0.03532356910620672 b1 0.6991163352213629 b2 0.5106045388120525 n_hiddens 31 batch_size 428 embedding_dim 189 en_dropout 0.025871393417969957 de_dropout 0.11966258058417795\n",
            "Epoch 1/10\n",
            "148/148 [==============================] - 21s 85ms/step - loss: 2.1273 - acc: 0.6990 - val_loss: 1.5400 - val_acc: 0.7493\n",
            "Epoch 2/10\n",
            "148/148 [==============================] - 10s 71ms/step - loss: 1.4382 - acc: 0.7627 - val_loss: 1.3728 - val_acc: 0.7709\n",
            "Epoch 3/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.2985 - acc: 0.7808 - val_loss: 1.2627 - val_acc: 0.7897\n",
            "Epoch 4/10\n",
            "148/148 [==============================] - 10s 69ms/step - loss: 1.2128 - acc: 0.7962 - val_loss: 1.2211 - val_acc: 0.7977\n",
            "Epoch 5/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1712 - acc: 0.8026 - val_loss: 1.1976 - val_acc: 0.8020\n",
            "Epoch 6/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1501 - acc: 0.8057 - val_loss: 1.1824 - val_acc: 0.8044\n",
            "Epoch 7/10\n",
            "148/148 [==============================] - 11s 72ms/step - loss: 1.1391 - acc: 0.8079 - val_loss: 1.1858 - val_acc: 0.8046\n",
            "Epoch 8/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1317 - acc: 0.8094 - val_loss: 1.1775 - val_acc: 0.8071\n",
            "Epoch 9/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1240 - acc: 0.8106 - val_loss: 1.1830 - val_acc: 0.8075\n",
            "Epoch 10/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1185 - acc: 0.8112 - val_loss: 1.1884 - val_acc: 0.8047\n",
            "best_params [0.03532356910620672, 0.6991163352213629, 0.5106045388120525, 31, 428, 189, 0.025871393417969957, 0.11966258058417795]\n",
            "lr 0.004802586925210162 b1 0.9034282275741946 b2 0.5912046730992582 n_hiddens 347 batch_size 75 embedding_dim 197 en_dropout 0.09069297020865585 de_dropout 0.3571965711563737\n",
            "Epoch 1/10\n",
            "840/840 [==============================] - 41s 42ms/step - loss: 47.3274 - acc: 0.5939 - val_loss: 40.8621 - val_acc: 0.6827\n",
            "Epoch 2/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 28.2203 - acc: 0.6869 - val_loss: 20.1261 - val_acc: 0.7027\n",
            "Epoch 3/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 17.9412 - acc: 0.6983 - val_loss: 14.8258 - val_acc: 0.7074\n",
            "Epoch 4/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 14.3908 - acc: 0.6995 - val_loss: 12.5988 - val_acc: 0.7119\n",
            "Epoch 5/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 12.5363 - acc: 0.7025 - val_loss: 10.9532 - val_acc: 0.7096\n",
            "Epoch 6/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 11.0241 - acc: 0.7048 - val_loss: 9.7266 - val_acc: 0.7124\n",
            "Epoch 7/10\n",
            "840/840 [==============================] - 34s 40ms/step - loss: 9.6425 - acc: 0.7064 - val_loss: 8.6794 - val_acc: 0.7161\n",
            "Epoch 8/10\n",
            "840/840 [==============================] - 34s 40ms/step - loss: 8.8936 - acc: 0.7076 - val_loss: 8.1400 - val_acc: 0.7189\n",
            "Epoch 9/10\n",
            "840/840 [==============================] - 35s 41ms/step - loss: 7.9552 - acc: 0.7092 - val_loss: 7.3166 - val_acc: 0.7167\n",
            "Epoch 10/10\n",
            "840/840 [==============================] - 36s 43ms/step - loss: 7.5176 - acc: 0.7102 - val_loss: 6.7090 - val_acc: 0.7203\n",
            "lr 0.007884927954053279 b1 0.9902412478808713 b2 0.9118407790180642 n_hiddens 386 batch_size 63 embedding_dim 167 en_dropout 0.19009677049365437 de_dropout 0.45053422893100076\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 54s 48ms/step - loss: 24.4141 - acc: 0.6873 - val_loss: 93.1671 - val_acc: 0.5327\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 43s 43ms/step - loss: 238.8297 - acc: 0.5407 - val_loss: 200.5516 - val_acc: 0.5119\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 43s 43ms/step - loss: 427.3808 - acc: 0.5085 - val_loss: 337.5587 - val_acc: 0.6407\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 42s 42ms/step - loss: 537.8705 - acc: 0.4848 - val_loss: 327.3697 - val_acc: 0.6251\n",
            "lr 0.09523313337605821 b1 0.6918414557475971 b2 0.870946217502218 n_hiddens 400 batch_size 205 embedding_dim 191 en_dropout 0.2421933926534539 de_dropout 0.048646705756616504\n",
            "Epoch 1/10\n",
            "308/308 [==============================] - 40s 96ms/step - loss: 2.1461 - acc: 0.7245 - val_loss: 1.6466 - val_acc: 0.7559\n",
            "Epoch 2/10\n",
            "308/308 [==============================] - 27s 87ms/step - loss: 1.6281 - acc: 0.7584 - val_loss: 1.6918 - val_acc: 0.7595\n",
            "Epoch 3/10\n",
            "308/308 [==============================] - 27s 86ms/step - loss: 1.6613 - acc: 0.7608 - val_loss: 1.7405 - val_acc: 0.7580\n",
            "Epoch 4/10\n",
            "308/308 [==============================] - 26s 85ms/step - loss: 1.6841 - acc: 0.7651 - val_loss: 1.8078 - val_acc: 0.7674\n",
            "lr 0.08857063677568827 b1 0.7265080871745688 b2 0.6628457134279763 n_hiddens 372 batch_size 334 embedding_dim 189 en_dropout 0.4249982088943396 de_dropout 0.3398478068941738\n",
            "Epoch 1/10\n",
            "189/189 [==============================] - 32s 123ms/step - loss: 3.7367 - acc: 0.7050 - val_loss: 1.5900 - val_acc: 0.7657\n",
            "Epoch 2/10\n",
            "189/189 [==============================] - 20s 107ms/step - loss: 1.5847 - acc: 0.7706 - val_loss: 1.6171 - val_acc: 0.7728\n",
            "Epoch 3/10\n",
            "189/189 [==============================] - 20s 107ms/step - loss: 1.6355 - acc: 0.7723 - val_loss: 1.6590 - val_acc: 0.7752\n",
            "Epoch 4/10\n",
            "189/189 [==============================] - 20s 108ms/step - loss: 1.6976 - acc: 0.7729 - val_loss: 1.7277 - val_acc: 0.7767\n",
            "lr 0.0028760899584410193 b1 0.7334312865588856 b2 0.9341215051090186 n_hiddens 48 batch_size 192 embedding_dim 96 en_dropout 0.3086579782727112 de_dropout 0.49606742902546846\n",
            "Epoch 1/10\n",
            "329/329 [==============================] - 25s 54ms/step - loss: 2.3701 - acc: 0.6782 - val_loss: 1.5742 - val_acc: 0.7502\n",
            "Epoch 2/10\n",
            "329/329 [==============================] - 16s 48ms/step - loss: 1.4242 - acc: 0.7722 - val_loss: 1.3342 - val_acc: 0.7857\n",
            "Epoch 3/10\n",
            "329/329 [==============================] - 15s 46ms/step - loss: 1.2487 - acc: 0.7955 - val_loss: 1.2044 - val_acc: 0.8043\n",
            "Epoch 4/10\n",
            "329/329 [==============================] - 14s 44ms/step - loss: 1.1464 - acc: 0.8087 - val_loss: 1.1237 - val_acc: 0.8141\n",
            "Epoch 5/10\n",
            "329/329 [==============================] - 16s 50ms/step - loss: 1.0751 - acc: 0.8171 - val_loss: 1.0709 - val_acc: 0.8197\n",
            "Epoch 6/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 1.0205 - acc: 0.8232 - val_loss: 1.0251 - val_acc: 0.8253\n",
            "Epoch 7/10\n",
            "329/329 [==============================] - 14s 44ms/step - loss: 0.9763 - acc: 0.8284 - val_loss: 0.9852 - val_acc: 0.8310\n",
            "Epoch 8/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 0.9381 - acc: 0.8332 - val_loss: 0.9549 - val_acc: 0.8356\n",
            "Epoch 9/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 0.9056 - acc: 0.8373 - val_loss: 0.9281 - val_acc: 0.8395\n",
            "Epoch 10/10\n",
            "329/329 [==============================] - 15s 46ms/step - loss: 0.8772 - acc: 0.8410 - val_loss: 0.9062 - val_acc: 0.8423\n",
            "best_params [0.0028760899584410193, 0.7334312865588856, 0.9341215051090186, 48, 192, 96, 0.3086579782727112, 0.49606742902546846]\n",
            "lr 0.04238064913974372 b1 0.829922237961483 b2 0.8271039498988201 n_hiddens 185 batch_size 391 embedding_dim 170 en_dropout 0.2392713682353102 de_dropout 0.2268610592144788\n",
            "Epoch 1/10\n",
            "162/162 [==============================] - 26s 116ms/step - loss: 1.7773 - acc: 0.7336 - val_loss: 1.2973 - val_acc: 0.7786\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 16s 98ms/step - loss: 1.2114 - acc: 0.7892 - val_loss: 1.1852 - val_acc: 0.7972\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 95ms/step - loss: 1.1253 - acc: 0.8020 - val_loss: 1.1646 - val_acc: 0.8036\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 16s 96ms/step - loss: 1.0940 - acc: 0.8063 - val_loss: 1.1693 - val_acc: 0.8052\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 16s 96ms/step - loss: 1.0787 - acc: 0.8086 - val_loss: 1.1743 - val_acc: 0.8056\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 15s 94ms/step - loss: 1.0697 - acc: 0.8096 - val_loss: 1.1852 - val_acc: 0.8066\n",
            "lr 0.04862684276952181 b1 0.6396875348678882 b2 0.9641528204389789 n_hiddens 336 batch_size 154 embedding_dim 297 en_dropout 0.3454672501989144 de_dropout 0.011447366627501177\n",
            "Epoch 1/10\n",
            "410/410 [==============================] - 36s 68ms/step - loss: 2.0250 - acc: 0.7346 - val_loss: 1.4690 - val_acc: 0.7614\n",
            "Epoch 2/10\n",
            "410/410 [==============================] - 25s 61ms/step - loss: 1.3711 - acc: 0.7789 - val_loss: 1.4103 - val_acc: 0.7802\n",
            "Epoch 3/10\n",
            "410/410 [==============================] - 26s 63ms/step - loss: 1.3285 - acc: 0.7851 - val_loss: 1.4063 - val_acc: 0.7870\n",
            "Epoch 4/10\n",
            "410/410 [==============================] - 27s 66ms/step - loss: 1.3032 - acc: 0.7876 - val_loss: 1.4261 - val_acc: 0.7844\n",
            "Epoch 5/10\n",
            "410/410 [==============================] - 26s 64ms/step - loss: 1.3137 - acc: 0.7878 - val_loss: 1.4847 - val_acc: 0.7856\n",
            "Epoch 6/10\n",
            "410/410 [==============================] - 25s 61ms/step - loss: 1.3207 - acc: 0.7884 - val_loss: 1.4932 - val_acc: 0.7845\n",
            "lr 0.009356619761583062 b1 0.5332853760659995 b2 0.7985972254756912 n_hiddens 421 batch_size 60 embedding_dim 157 en_dropout 0.42917553109835965 de_dropout 0.360774491217183\n",
            "Epoch 1/10\n",
            "1050/1050 [==============================] - 52s 44ms/step - loss: 1.2394 - acc: 0.8083 - val_loss: 1.0283 - val_acc: 0.8401\n",
            "Epoch 2/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 1.0212 - acc: 0.8424 - val_loss: 0.9933 - val_acc: 0.8477\n",
            "Epoch 3/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9743 - acc: 0.8471 - val_loss: 0.9738 - val_acc: 0.8507\n",
            "Epoch 4/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 0.9512 - acc: 0.8493 - val_loss: 0.9908 - val_acc: 0.8514\n",
            "Epoch 5/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 0.9812 - acc: 0.8477 - val_loss: 0.9939 - val_acc: 0.8522\n",
            "Epoch 6/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9266 - acc: 0.8508 - val_loss: 0.9483 - val_acc: 0.8567\n",
            "Epoch 7/10\n",
            "1050/1050 [==============================] - 45s 42ms/step - loss: 0.8982 - acc: 0.8552 - val_loss: 0.9917 - val_acc: 0.8564\n",
            "Epoch 8/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9093 - acc: 0.8561 - val_loss: 1.0104 - val_acc: 0.8599\n",
            "Epoch 9/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.8996 - acc: 0.8576 - val_loss: 1.0145 - val_acc: 0.8573\n",
            "lr 0.0457281561797704 b1 0.5076710156515364 b2 0.5199289398437946 n_hiddens 165 batch_size 225 embedding_dim 129 en_dropout 0.1406144712333035 de_dropout 0.45555442338770247\n",
            "Epoch 1/10\n",
            "280/280 [==============================] - 25s 68ms/step - loss: 1.4679 - acc: 0.7771 - val_loss: 1.2018 - val_acc: 0.8130\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 16s 58ms/step - loss: 1.2358 - acc: 0.8104 - val_loss: 1.2081 - val_acc: 0.8154\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 16s 59ms/step - loss: 1.2749 - acc: 0.8106 - val_loss: 1.2691 - val_acc: 0.8141\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 16s 58ms/step - loss: 1.3564 - acc: 0.8092 - val_loss: 1.3323 - val_acc: 0.8130\n",
            "lr 0.01679811924217257 b1 0.9959429750827256 b2 0.7594863749500774 n_hiddens 231 batch_size 502 embedding_dim 267 en_dropout 0.3497943935602377 de_dropout 0.1712040816938234\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 24s 133ms/step - loss: 12.2860 - acc: 0.6638 - val_loss: 65.2469 - val_acc: 0.5983\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 14s 112ms/step - loss: 734.9335 - acc: 0.3042 - val_loss: 2500.8958 - val_acc: 0.1876\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 14s 114ms/step - loss: 7291.9438 - acc: 0.3178 - val_loss: 14923.3516 - val_acc: 0.2686\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 14s 113ms/step - loss: 20190.1211 - acc: 0.1384 - val_loss: 14675.0283 - val_acc: 0.1471\n",
            "lr 0.09265086317330892 b1 0.8800318284057217 b2 0.9425648260243342 n_hiddens 8 batch_size 292 embedding_dim 202 en_dropout 0.00891600351209293 de_dropout 0.2948899342456032\n",
            "Epoch 1/10\n",
            "216/216 [==============================] - 24s 71ms/step - loss: 2.0993 - acc: 0.6928 - val_loss: 1.6373 - val_acc: 0.7355\n",
            "Epoch 2/10\n",
            "216/216 [==============================] - 12s 54ms/step - loss: 1.6070 - acc: 0.7368 - val_loss: 1.5688 - val_acc: 0.7426\n",
            "Epoch 3/10\n",
            "216/216 [==============================] - 12s 54ms/step - loss: 1.5582 - acc: 0.7400 - val_loss: 1.5364 - val_acc: 0.7451\n",
            "Epoch 4/10\n",
            "216/216 [==============================] - 12s 55ms/step - loss: 1.5304 - acc: 0.7425 - val_loss: 1.5176 - val_acc: 0.7467\n",
            "Epoch 5/10\n",
            "216/216 [==============================] - 12s 55ms/step - loss: 1.5162 - acc: 0.7437 - val_loss: 1.5163 - val_acc: 0.7467\n",
            "Epoch 6/10\n",
            "216/216 [==============================] - 13s 60ms/step - loss: 1.5076 - acc: 0.7443 - val_loss: 1.5129 - val_acc: 0.7472\n",
            "Epoch 7/10\n",
            "216/216 [==============================] - 13s 58ms/step - loss: 1.5031 - acc: 0.7446 - val_loss: 1.5112 - val_acc: 0.7485\n",
            "Epoch 8/10\n",
            "216/216 [==============================] - 12s 56ms/step - loss: 1.5001 - acc: 0.7445 - val_loss: 1.5187 - val_acc: 0.7474\n",
            "Epoch 9/10\n",
            "216/216 [==============================] - 12s 56ms/step - loss: 1.4978 - acc: 0.7449 - val_loss: 1.5133 - val_acc: 0.7478\n",
            "Epoch 10/10\n",
            "216/216 [==============================] - 12s 58ms/step - loss: 1.4959 - acc: 0.7449 - val_loss: 1.5237 - val_acc: 0.7474\n",
            "lr 0.08068888142242694 b1 0.6897667126065324 b2 0.6593224222592495 n_hiddens 495 batch_size 387 embedding_dim 157 en_dropout 0.09281511616558086 de_dropout 0.09893348002157276\n",
            "Epoch 1/10\n",
            "163/163 [==============================] - 35s 174ms/step - loss: 4.2851 - acc: 0.6987 - val_loss: 1.6232 - val_acc: 0.7527\n",
            "Epoch 2/10\n",
            "163/163 [==============================] - 26s 162ms/step - loss: 1.5616 - acc: 0.7601 - val_loss: 1.5860 - val_acc: 0.7614\n",
            "Epoch 3/10\n",
            "163/163 [==============================] - 26s 158ms/step - loss: 1.5456 - acc: 0.7668 - val_loss: 1.5965 - val_acc: 0.7720\n",
            "Epoch 4/10\n",
            "163/163 [==============================] - 26s 158ms/step - loss: 1.5534 - acc: 0.7738 - val_loss: 1.6337 - val_acc: 0.7736\n",
            "Epoch 5/10\n",
            "163/163 [==============================] - 26s 159ms/step - loss: 1.5835 - acc: 0.7752 - val_loss: 1.6768 - val_acc: 0.7743\n",
            "lr 0.09368401832374376 b1 0.6441522779323736 b2 0.7202581827799663 n_hiddens 147 batch_size 304 embedding_dim 154 en_dropout 0.1984830083082012 de_dropout 0.09426206797341496\n",
            "Epoch 1/10\n",
            "208/208 [==============================] - 31s 97ms/step - loss: 1.6601 - acc: 0.7458 - val_loss: 1.4539 - val_acc: 0.7706\n",
            "Epoch 2/10\n",
            "208/208 [==============================] - 16s 76ms/step - loss: 1.4491 - acc: 0.7717 - val_loss: 1.5041 - val_acc: 0.7694\n",
            "Epoch 3/10\n",
            "208/208 [==============================] - 16s 76ms/step - loss: 1.4806 - acc: 0.7734 - val_loss: 1.5496 - val_acc: 0.7717\n",
            "Epoch 4/10\n",
            "208/208 [==============================] - 15s 73ms/step - loss: 1.5147 - acc: 0.7747 - val_loss: 1.5977 - val_acc: 0.7703\n",
            "lr 0.042692283278584466 b1 0.6253716592985747 b2 0.5118043348713013 n_hiddens 55 batch_size 232 embedding_dim 221 en_dropout 0.2098795539821759 de_dropout 0.15286758968866554\n",
            "Epoch 1/10\n",
            "272/272 [==============================] - 26s 60ms/step - loss: 1.5628 - acc: 0.7640 - val_loss: 1.2489 - val_acc: 0.8026\n",
            "Epoch 2/10\n",
            "272/272 [==============================] - 14s 51ms/step - loss: 1.2165 - acc: 0.8077 - val_loss: 1.1931 - val_acc: 0.8108\n",
            "Epoch 3/10\n",
            "272/272 [==============================] - 13s 49ms/step - loss: 1.1813 - acc: 0.8128 - val_loss: 1.1857 - val_acc: 0.8127\n",
            "Epoch 4/10\n",
            "272/272 [==============================] - 14s 52ms/step - loss: 1.1783 - acc: 0.8132 - val_loss: 1.2055 - val_acc: 0.8120\n",
            "Epoch 5/10\n",
            "272/272 [==============================] - 14s 52ms/step - loss: 1.1949 - acc: 0.8126 - val_loss: 1.2465 - val_acc: 0.8093\n",
            "Epoch 6/10\n",
            "272/272 [==============================] - 15s 55ms/step - loss: 1.2098 - acc: 0.8117 - val_loss: 1.2694 - val_acc: 0.8091\n",
            "lr 0.024942993924761403 b1 0.816646713153006 b2 0.5758239227333691 n_hiddens 299 batch_size 345 embedding_dim 258 en_dropout 0.28320602438051407 de_dropout 0.006364671290011947\n",
            "Epoch 1/10\n",
            "183/183 [==============================] - 32s 118ms/step - loss: 1.8320 - acc: 0.7312 - val_loss: 1.4160 - val_acc: 0.7705\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 19s 106ms/step - loss: 1.3323 - acc: 0.7758 - val_loss: 1.3254 - val_acc: 0.7756\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2669 - acc: 0.7813 - val_loss: 1.3012 - val_acc: 0.7809\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2382 - acc: 0.7847 - val_loss: 1.2951 - val_acc: 0.7823\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2307 - acc: 0.7867 - val_loss: 1.3297 - val_acc: 0.7815\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 19s 105ms/step - loss: 1.2393 - acc: 0.7880 - val_loss: 1.3414 - val_acc: 0.7842\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 19s 107ms/step - loss: 1.2456 - acc: 0.7891 - val_loss: 1.3687 - val_acc: 0.7856\n",
            "lr 0.029361976229069386 b1 0.8722124105253817 b2 0.7990420122442797 n_hiddens 471 batch_size 127 embedding_dim 80 en_dropout 0.46692086319982246 de_dropout 0.424282790315655\n",
            "Epoch 1/10\n",
            "497/497 [==============================] - 44s 70ms/step - loss: 2.1539 - acc: 0.7357 - val_loss: 1.5056 - val_acc: 0.7663\n",
            "Epoch 2/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.4992 - acc: 0.7694 - val_loss: 1.5014 - val_acc: 0.7733\n",
            "Epoch 3/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.5303 - acc: 0.7730 - val_loss: 1.5535 - val_acc: 0.7763\n",
            "Epoch 4/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.5718 - acc: 0.7737 - val_loss: 1.5835 - val_acc: 0.7772\n",
            "Epoch 5/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.6382 - acc: 0.7732 - val_loss: 1.6622 - val_acc: 0.7770\n",
            "lr 0.015886169959430787 b1 0.5930491086706786 b2 0.9364600597831465 n_hiddens 312 batch_size 273 embedding_dim 166 en_dropout 0.1266038983112921 de_dropout 0.47534027805897144\n",
            "Epoch 1/10\n",
            "231/231 [==============================] - 30s 97ms/step - loss: 1.4377 - acc: 0.7726 - val_loss: 0.9330 - val_acc: 0.8339\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.7914 - acc: 0.8508 - val_loss: 0.7378 - val_acc: 0.8631\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 20s 88ms/step - loss: 0.6159 - acc: 0.8744 - val_loss: 0.6561 - val_acc: 0.8763\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.5213 - acc: 0.8871 - val_loss: 0.6221 - val_acc: 0.8813\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.4640 - acc: 0.8944 - val_loss: 0.6002 - val_acc: 0.8846\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.4263 - acc: 0.8991 - val_loss: 0.6023 - val_acc: 0.8859\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.4004 - acc: 0.9024 - val_loss: 0.6051 - val_acc: 0.8852\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.3837 - acc: 0.9042 - val_loss: 0.6089 - val_acc: 0.8865\n",
            "best_params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n",
            "lr 0.05273769712037634 b1 0.771045744824429 b2 0.7738235870602771 n_hiddens 496 batch_size 72 embedding_dim 248 en_dropout 0.20062518730634193 de_dropout 0.09248141898419032\n",
            "Epoch 1/10\n",
            "875/875 [==============================] - 51s 51ms/step - loss: 1.8287 - acc: 0.7521 - val_loss: 1.8660 - val_acc: 0.7652\n",
            "Epoch 2/10\n",
            "875/875 [==============================] - 42s 47ms/step - loss: 1.9630 - acc: 0.7664 - val_loss: 2.0742 - val_acc: 0.7653\n",
            "Epoch 3/10\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 2.1886 - acc: 0.7659 - val_loss: 2.3230 - val_acc: 0.7642\n",
            "Epoch 4/10\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 2.4191 - acc: 0.7661 - val_loss: 2.5864 - val_acc: 0.7688\n",
            "lr 0.055601445446007314 b1 0.5916481064642332 b2 0.8536670972253568 n_hiddens 357 batch_size 176 embedding_dim 249 en_dropout 0.01406991540158703 de_dropout 0.39708082363877617\n",
            "Epoch 1/10\n",
            "358/358 [==============================] - 34s 78ms/step - loss: 1.5491 - acc: 0.7695 - val_loss: 1.3245 - val_acc: 0.7953\n",
            "Epoch 2/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.3409 - acc: 0.7935 - val_loss: 1.3572 - val_acc: 0.7964\n",
            "Epoch 3/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.3710 - acc: 0.7945 - val_loss: 1.3979 - val_acc: 0.7968\n",
            "Epoch 4/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.4169 - acc: 0.7941 - val_loss: 1.4399 - val_acc: 0.7970\n",
            "lr 0.044215098079003395 b1 0.8786494412321751 b2 0.5257896988483647 n_hiddens 384 batch_size 454 embedding_dim 209 en_dropout 0.12885558872455527 de_dropout 0.020384087026159292\n",
            "Epoch 1/10\n",
            "139/139 [==============================] - 28s 156ms/step - loss: 4.2794 - acc: 0.6652 - val_loss: 4.4400 - val_acc: 0.7061\n",
            "Epoch 2/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 4.0948 - acc: 0.7077 - val_loss: 3.9730 - val_acc: 0.7096\n",
            "Epoch 3/10\n",
            "139/139 [==============================] - 19s 138ms/step - loss: 4.0021 - acc: 0.7112 - val_loss: 4.0801 - val_acc: 0.7095\n",
            "Epoch 4/10\n",
            "139/139 [==============================] - 19s 135ms/step - loss: 4.6564 - acc: 0.6864 - val_loss: 4.6833 - val_acc: 0.6761\n",
            "Epoch 5/10\n",
            "139/139 [==============================] - 19s 134ms/step - loss: 5.1929 - acc: 0.7021 - val_loss: 5.6499 - val_acc: 0.6954\n",
            "lr 0.06399230927272871 b1 0.8022081836309816 b2 0.6193060601324336 n_hiddens 9 batch_size 429 embedding_dim 143 en_dropout 0.32337078773127215 de_dropout 0.19331651587228493\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 21s 85ms/step - loss: 2.7600 - acc: 0.5891 - val_loss: 2.1981 - val_acc: 0.7047\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 10s 68ms/step - loss: 1.9318 - acc: 0.7138 - val_loss: 1.7526 - val_acc: 0.7213\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.6939 - acc: 0.7297 - val_loss: 1.6521 - val_acc: 0.7357\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.6173 - acc: 0.7393 - val_loss: 1.5937 - val_acc: 0.7441\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5707 - acc: 0.7475 - val_loss: 1.5548 - val_acc: 0.7524\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.5391 - acc: 0.7523 - val_loss: 1.5328 - val_acc: 0.7555\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5202 - acc: 0.7545 - val_loss: 1.5238 - val_acc: 0.7573\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5058 - acc: 0.7559 - val_loss: 1.5089 - val_acc: 0.7576\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.4938 - acc: 0.7574 - val_loss: 1.5000 - val_acc: 0.7589\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 10s 68ms/step - loss: 1.4863 - acc: 0.7581 - val_loss: 1.5007 - val_acc: 0.7603\n",
            "lr 0.06915002475804598 b1 0.9260851442606486 b2 0.6656940653868313 n_hiddens 212 batch_size 507 embedding_dim 148 en_dropout 0.06237765532307099 de_dropout 0.20756265346428165\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 24s 128ms/step - loss: 3.0013 - acc: 0.6962 - val_loss: 3.7629 - val_acc: 0.7154\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 13s 108ms/step - loss: 3.5388 - acc: 0.7148 - val_loss: 3.2693 - val_acc: 0.7200\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.5486 - acc: 0.7154 - val_loss: 3.3086 - val_acc: 0.7173\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.8897 - acc: 0.7131 - val_loss: 3.5053 - val_acc: 0.7194\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 4.2286 - acc: 0.7094 - val_loss: 4.1605 - val_acc: 0.7138\n",
            "lr 0.05438078409563382 b1 0.797069514311043 b2 0.8301929923331611 n_hiddens 191 batch_size 437 embedding_dim 112 en_dropout 0.02218607384415172 de_dropout 0.2345667169178683\n",
            "Epoch 1/10\n",
            "145/145 [==============================] - 25s 118ms/step - loss: 1.8315 - acc: 0.7221 - val_loss: 1.3374 - val_acc: 0.7713\n",
            "Epoch 2/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.2338 - acc: 0.7848 - val_loss: 1.2122 - val_acc: 0.7924\n",
            "Epoch 3/10\n",
            "145/145 [==============================] - 15s 103ms/step - loss: 1.1584 - acc: 0.7955 - val_loss: 1.2064 - val_acc: 0.7968\n",
            "Epoch 4/10\n",
            "145/145 [==============================] - 15s 106ms/step - loss: 1.1330 - acc: 0.8000 - val_loss: 1.2036 - val_acc: 0.7991\n",
            "Epoch 5/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.1174 - acc: 0.8031 - val_loss: 1.2183 - val_acc: 0.8012\n",
            "Epoch 6/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.1152 - acc: 0.8047 - val_loss: 1.2330 - val_acc: 0.8017\n",
            "Epoch 7/10\n",
            "145/145 [==============================] - 15s 103ms/step - loss: 1.1137 - acc: 0.8055 - val_loss: 1.2429 - val_acc: 0.8024\n",
            "lr 0.033229711654843945 b1 0.6758362206652616 b2 0.5421071153171824 n_hiddens 474 batch_size 124 embedding_dim 175 en_dropout 0.2203264138955085 de_dropout 0.10858212483705315\n",
            "Epoch 1/10\n",
            "509/509 [==============================] - 43s 71ms/step - loss: 1.5089 - acc: 0.7781 - val_loss: 1.4467 - val_acc: 0.7902\n",
            "Epoch 2/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.4814 - acc: 0.7914 - val_loss: 1.5569 - val_acc: 0.7892\n",
            "Epoch 3/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.5894 - acc: 0.7877 - val_loss: 1.6942 - val_acc: 0.7818\n",
            "Epoch 4/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.6896 - acc: 0.7866 - val_loss: 1.8069 - val_acc: 0.7849\n",
            "lr 0.09371685820879909 b1 0.6131284379504724 b2 0.8654584831833052 n_hiddens 271 batch_size 235 embedding_dim 127 en_dropout 0.06330525067808912 de_dropout 0.4253666884019714\n",
            "Epoch 1/10\n",
            "269/269 [==============================] - 31s 83ms/step - loss: 1.8972 - acc: 0.7443 - val_loss: 1.4226 - val_acc: 0.7778\n",
            "Epoch 2/10\n",
            "269/269 [==============================] - 20s 76ms/step - loss: 1.4256 - acc: 0.7794 - val_loss: 1.4321 - val_acc: 0.7801\n",
            "Epoch 3/10\n",
            "269/269 [==============================] - 20s 74ms/step - loss: 1.4442 - acc: 0.7806 - val_loss: 1.4782 - val_acc: 0.7803\n",
            "Epoch 4/10\n",
            "269/269 [==============================] - 20s 73ms/step - loss: 1.4776 - acc: 0.7801 - val_loss: 1.5299 - val_acc: 0.7829\n",
            "lr 0.02534581039601982 b1 0.9644285184515715 b2 0.9541757548488676 n_hiddens 395 batch_size 408 embedding_dim 154 en_dropout 0.4695161901310664 de_dropout 0.020722358810335628\n",
            "Epoch 1/10\n",
            "155/155 [==============================] - 32s 159ms/step - loss: 2.8059 - acc: 0.6562 - val_loss: 1.9671 - val_acc: 0.7274\n",
            "Epoch 2/10\n",
            "155/155 [==============================] - 22s 144ms/step - loss: 1.5823 - acc: 0.7502 - val_loss: 1.4475 - val_acc: 0.7652\n",
            "Epoch 3/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.3072 - acc: 0.7746 - val_loss: 1.3300 - val_acc: 0.7802\n",
            "Epoch 4/10\n",
            "155/155 [==============================] - 22s 140ms/step - loss: 1.1962 - acc: 0.7825 - val_loss: 1.2911 - val_acc: 0.7821\n",
            "Epoch 5/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.1338 - acc: 0.7872 - val_loss: 1.2679 - val_acc: 0.7851\n",
            "Epoch 6/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0902 - acc: 0.7910 - val_loss: 1.2587 - val_acc: 0.7853\n",
            "Epoch 7/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0599 - acc: 0.7939 - val_loss: 1.2562 - val_acc: 0.7906\n",
            "Epoch 8/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.0367 - acc: 0.7959 - val_loss: 1.2608 - val_acc: 0.7905\n",
            "Epoch 9/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0128 - acc: 0.7990 - val_loss: 1.2467 - val_acc: 0.7916\n",
            "Epoch 10/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 0.9887 - acc: 0.8021 - val_loss: 1.2456 - val_acc: 0.7970\n",
            "lr 0.017128197680272673 b1 0.9758117650440161 b2 0.9319366006289951 n_hiddens 236 batch_size 512 embedding_dim 95 en_dropout 0.40416444616136715 de_dropout 0.09073304852829539\n",
            "Epoch 1/10\n",
            "124/124 [==============================] - 24s 138ms/step - loss: 1.9817 - acc: 0.7106 - val_loss: 1.5866 - val_acc: 0.7512\n",
            "Epoch 2/10\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.4751 - acc: 0.7630 - val_loss: 1.4215 - val_acc: 0.7699\n",
            "Epoch 3/10\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.3033 - acc: 0.7764 - val_loss: 1.3351 - val_acc: 0.7777\n",
            "Epoch 4/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.2057 - acc: 0.7836 - val_loss: 1.2881 - val_acc: 0.7835\n",
            "Epoch 5/10\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.1418 - acc: 0.7889 - val_loss: 1.2639 - val_acc: 0.7844\n",
            "Epoch 6/10\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.0979 - acc: 0.7924 - val_loss: 1.2459 - val_acc: 0.7876\n",
            "Epoch 7/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.0623 - acc: 0.7959 - val_loss: 1.2376 - val_acc: 0.7891\n",
            "Epoch 8/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.0305 - acc: 0.7984 - val_loss: 1.2266 - val_acc: 0.7891\n",
            "Epoch 9/10\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.0091 - acc: 0.8002 - val_loss: 1.2260 - val_acc: 0.7913\n",
            "Epoch 10/10\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 0.9892 - acc: 0.8027 - val_loss: 1.2158 - val_acc: 0.7926\n",
            "final best params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final LSTM model**\n",
        "\n",
        "Based on best hyperparameter from hyperparameter tuning process, we build our final LSTM model with them. "
      ],
      "metadata": {
        "id": "yJk8zfZUvwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate, b1, b2, hidden_units, batch_size, embedding_dim, encoder dropout, decoder dropout\n",
        "print('final best params',best_params)\n",
        "LSTM_final_lr = best_params[0]\n",
        "LSTM_final_b1 = best_params[1]\n",
        "LSTM_final_b2 = best_params[2]\n",
        "hidden_units = best_params[3]\n",
        "LSTM_final_batch = best_params[4]\n",
        "embedding_dim = best_params[5]\n",
        "LSTM_final_enD = best_params[6]\n",
        "LSTM_final_deD = best_params[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2OcnXDk2j-O",
        "outputId": "1f32688a-a144-4041-b685-0b46dd42fbe1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final best params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]"
      ],
      "metadata": {
        "id": "fmBRCRYMmOEN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # Embedding layer\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # Exclude padding 0 in operation\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # To return state value\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # Return hidden state and cell state\n",
        "encoder_states = [state_h, state_c] # Save encoder's hidden state and cell state\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # Embedding layer\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # exclude padding 0 in operation\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# To return state value, return_state = True\n",
        "# To predict word for every time step, return_sequences = True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# Use encoder's hidden state as initial hidden state \n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# predict word bsaed on softmax activation function for all results from every time step\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model's input and output \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=LSTM_final_lr, beta_1=LSTM_final_b1, beta_2=LSTM_final_b2), \n",
        "              loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model_weights_path = './gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt'\n",
        "\n",
        "save_best_weights = ModelCheckpoint(\n",
        "  model_weights_path, monitor='val_loss', mode='min',\n",
        "  save_weights_only=True, save_best_only=True, verbose=1, \n",
        ")\n",
        "\n",
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "        validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "        batch_size=LSTM_final_batch, callbacks=[save_best_weights,EarlyStopping(monitor='val_loss', patience = 10)], epochs=50) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "8QXSULWUUfEc",
        "outputId": "d01df972-38d4-45b0-c584-be4c09892dce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "  5/231 [..............................] - ETA: 14:58 - loss: 5.5656 - acc: 0.4773"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-22816a0f5a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         batch_size=LSTM_final_batch, callbacks=[save_best_weights,EarlyStopping(monitor='val_loss', patience = 10)], epochs=50) \n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM evaluation**\n",
        "\n",
        "In NMT, there are diverse evaluation metrics from human evaluation methods to automatic evaluation metrics [1]. In previous diverse neural machine traslation tasks [2] [3], the mostly used evelaution metrics are BLEU score. Therefore, among diverse evlauation metrics, we use BLEU score and ROUGE score for our model's evaluation. \n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1JuvHUO16HlSU7N6B5zSPDOM3BVAKsXuL\" height = 300 width = 400>\n",
        "\n",
        "\n",
        "**BLEU Score [4] (n-gram precision)**\n",
        "\n",
        "  BLEU is based on the degree of n-gram overlapping between the strings of words produced by the machine and the human translation references at the corpus level. BLEU computes the precision for n-gram of size 1-to-4 with\n",
        "the coefficient of brevity penalty (BP).\n",
        "\n",
        "\n",
        "**ROUGE Score [5] (n-gram recall)**\n",
        "\n",
        "  ROUGE compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE is another metric that focuses upon recall rather than precision like BLEU.\n",
        "\n",
        "* Diverse Variations, here we focus on ROUGE-1, ROUGE-2 and ROUGE-L\n",
        "\n",
        "  1) ROUGE-N: how many n-grams from the reference translation have been properly predicted in the machine generated translation\n",
        "\n",
        "  (ROUGE-1: unigrams, ROUGE-2: bigrams)\n",
        "\n",
        "  2) ROUGE-L: measure of recall for longest common subsequences. \n",
        "\n",
        "---------------\n",
        "BLEU focuses on precision: how much the words (and/or n-grams) in the candidate model outputs appear in the human reference.\n",
        "\n",
        "ROUGE focuses on recall: how much the words (and/or n-grams) in the human references appear in the candidate model outputs.\n",
        "\n",
        "These results are complementing, as is often the case in the precision-recall tradeoff. Therefore, we try to check both metrics for our model's evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "RYP9VndLr7QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpK7vJZpEmjA",
        "outputId": "5d959d3a-c63c-46b9-c768-63efeb18c952"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "xUgxdqwD93Ap"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다릅니다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. 특히 디코더를 수정해야 합니다. 이번에는 번역 단계를 위해 모델을 수정하고 동작시켜보겠습니다.\n",
        "In sequence-to-sequence, the \n",
        "\n",
        "We design another model for test process, especially the decoder. \n",
        "\n",
        "The overall translation process is as follows. \n",
        "\n",
        "1) Input source sentence to be translated into encoder and get the final step's hidden and cell state. \n",
        "\n",
        "2) Encoder's final hidden state, cell state and \\<sos> token is sent to decoder.\n",
        "\n",
        "3) Decoder iterate next word prediction until \\<eos> token.\n",
        "\n",
        "4) The input and output of encoder, encoder_inputs and encoder_states is reused that was defined in training phase. 이렇게 되면 훈련 단계에 encoder_inputs와 encoder_states 사이에 있는 모든 층까지 전부 불러오게 되므로 결과적으로 훈련 단계에서 사용한 인코더를 그대로 재사용하게 됩니다. \n",
        "\n",
        "5) We control decoder's every step in test phase, so we define decoder_state_input_h, decoder_state_input_c which store previous step's state. \n"
      ],
      "metadata": {
        "id": "No8ze4gUhXTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder \n",
        "# Tensor for previous step's state \n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Reuse Embedding layer that was used in training phase \n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict next word.. \n",
        "# Previous time step's state -> current time step's initial state  \n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# Predict word in every time step \n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Updated decoder \n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "lBppG7vpUsFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현합니다. 입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉 상태와 셀 상태를 리턴합니다. 이 두 개의 값을 states_value에 저장합니다. 그리고 디코더의 초기 입력으로 <SOS>를 준비합니다. 이를 target_seq에 저장합니다. 이 두 가지 입력을 가지고 while문 안으로 진입하여 이 두 가지를 디코더의 입력으로 사용합니다. 이제 디코더는 현재 시점에 대해서 예측을 하게 되는데, 현재 시점의 예측 벡터가 output_tokens, 현재 시점의 은닉 상태가 h, 현재 시점의 셀 상태가 c입니다. 예측 벡터로부터 현재 시점의 예측 단어인 target_seq를 얻고, h와 c 이 두 개의 값은 states_value에 저장합니다. 그리고 while문의 다음 루프. 즉, 두번째 시점의 디코더의 입력으로 다시 target_seq와 states_value를 사용합니다. 이를 현재 시점의 예측 단어로 <eos>를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복합니다. 각 시점마다 번역된 다어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴합니다."
      ],
      "metadata": {
        "id": "63sydMAinuEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # Iterate loop until stop_condition becomes True\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        \n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        \n",
        "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        \n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "oRu34aJ-WA_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 확인을 위한 함수를 만듭니다. seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환합니다. seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다."
      ],
      "metadata": {
        "id": "9E48BJi4olTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform original sentence's integer sequence to text sequence \n",
        "def seq_to_src(input_seq):\n",
        "    sentence = ''\n",
        "    for encoded_word in input_seq:\n",
        "        if(encoded_word != 0):\n",
        "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "    return sentence\n",
        "\n",
        "# transform translated sentence's integer sequence to text sequence\n",
        "def seq_to_tar(input_seq):\n",
        "    sentence = ''\n",
        "    for encoded_word in input_seq:\n",
        "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "            \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "Ap2hb1M9XNzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Result"
      ],
      "metadata": {
        "id": "btMGRg6ShaKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "    reference = [[]]\n",
        "    candidate = []\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "\n",
        "    input_sentence = seq_to_src(encoder_input_train[seq_index])\n",
        "    answer_sentence = seq_to_tar(decoder_input_train[seq_index])\n",
        "    translated_sentence = decoded_sentence[1:-5]\n",
        "\n",
        "    print(\"Input Sentence :\", input_sentence)\n",
        "    print(\"Answer Sentence: \", answer_sentence)\n",
        "    print(\"Translated Sentence :\", translated_sentence)\n",
        "\n",
        "    print(rouge.get_scores([translated_sentence], [answer_sentence], avg=True))\n",
        "    reference[0].extend(answer_sentence.split())\n",
        "    candidate.extend(translated_sentence.split())\n",
        "    print('BLEU score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(1.0, 0, 0, 0))))\n",
        "    \n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "-9YuQOqh43_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    print(input_seq)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnWII0PR_3Fd",
        "outputId": "b29b62bd-0d79-4bac-ce4c-032aa570b37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 21   3  33 136   4   0   0   0   0]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : are you all ready ? \n",
            "정답문장 : etes vous tous prets ? \n",
            "번역문장 : etes vous toutes dingues ? \n",
            "--------------------------------------------------\n",
            "[[  2 634 336 280 305   1   0   0   0]]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "입력문장 : i drank beer last night . \n",
            "정답문장 : j ai bu de la biere la nuit derniere . \n",
            "번역문장 : j ai la biere pour la biere . \n",
            "--------------------------------------------------\n",
            "[[   8    9    5 2103  157    1    0    0    0]]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "입력문장 : it s a sunny day . \n",
            "정답문장 : c est un jour ensoleille . \n",
            "번역문장 : c est une journee journee . \n",
            "--------------------------------------------------\n",
            "[[  2  17  92  12 544   1   0   0   0]]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "입력문장 : i m going to fight . \n",
            "정답문장 : je vais me battre . \n",
            "번역문장 : je vais etre prudente . \n",
            "--------------------------------------------------\n",
            "[[   3   13 1931    1    0    0    0    0    0]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "입력문장 : you re unambitious . \n",
            "정답문장 : tu es depourvu d ambition . \n",
            "번역문장 : vous etes depourvu d ambition . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRU model**"
      ],
      "metadata": {
        "id": "2PLmPlcOBZMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, GRU\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "uCMHzm945gNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_model(\n",
        "  enc_seqlen, dec_seqlen, enc_vocabsize, dec_vocabsize, hsize, \n",
        "  embdim, encoder_dropout, decoder_dropout):\n",
        "  ### Encoder\n",
        "  encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
        "  embedding = Embedding(\n",
        "      enc_vocabsize, embdim, name=\"encoder_embedding\"\n",
        "  )\n",
        "  encoder_gru = GRU(\n",
        "      hsize, return_state=True, name=\"encoder_gru\", dropout=encoder_dropout\n",
        "  )\n",
        "  encoder_emb = embedding(encoder_inputs)\n",
        "  encoder_outputs, encoder_state = encoder_gru(encoder_emb)\n",
        "\n",
        "  ### Decoder\n",
        "  decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
        "  embedding = Embedding(\n",
        "      dec_vocabsize, embdim, input_length=dec_seqlen-1, name=\"decoder_embedding\"\n",
        "   )\n",
        "  decoder_gru = GRU(\n",
        "      hsize, return_state=True, return_sequences=True, name=\"decoder_gru\", \n",
        "      dropout=decoder_dropout\n",
        "  )\n",
        "\n",
        "  decoder_emb = embedding(decoder_inputs)\n",
        "  decoder_outputs, _ = decoder_gru(decoder_emb, initial_state=encoder_state)\n",
        "  dense_layer = Dense(dec_vocabsize, activation=\"softmax\", name=\"dense_layer\")\n",
        "  decoder_outputs = dense_layer(decoder_outputs)\n",
        "\n",
        "  # Define the Model which accepts encoder/decoder inputs and outputs predictions\n",
        "  model = Model(\n",
        "      inputs=[encoder_inputs, decoder_inputs],\n",
        "      outputs=decoder_outputs,\n",
        "      name=\"encoder_decoder_model_gru\",\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZiTeUevlhxuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_Model(embedding_dim, hidden_units, lr, b1, b2, batchsize, encoder_dropout, decoder_dropout):\n",
        "  model = create_train_model(\n",
        "    src_seqlen, tar_seqlen, src_vocab_size, tar_vocab_size, \n",
        "    hidden_units, embedding_dim, \n",
        "    encoder_dropout, decoder_dropout\n",
        "  )\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 3)], epochs=10) # for testing, we use epochs = 10 \n",
        "\n",
        "\n",
        "  return history.history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "RPseZEYj5iDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Selection Process**\n",
        "\n",
        "Similar to LSTM, we try hyperparameter tuning to find best hyperparmeter for GRU."
      ],
      "metadata": {
        "id": "FLkbHUzWlx8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyper_param(n_iteration):\n",
        "  hyper_param = []  # learning_rate, n_hidden, timestep, epochs\n",
        "  for i in range(n_iteration):\n",
        "    current_params = []\n",
        "    # We use adam optimizer, so we tune learning rate\n",
        "    current_params.append(np.random.uniform(0,0.1)) # learning rate \n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b1 (from Momentum)\n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b2 (from RMSProp)\n",
        "    current_params.append(np.random.randint(1,513)) # hidden units \n",
        "    current_params.append(np.random.randint(32,513)) # batch_size\n",
        "    current_params.append(np.random.randint(50,300)) # embedding dimension\n",
        "    current_params.append(np.random.uniform(0,0.5)) # encoder dropout\n",
        "    current_params.append(np.random.uniform(0,0.5)) # decoder dropout\n",
        "    hyper_param.append(current_params)\n",
        "  return hyper_param\n",
        "\n",
        "hyper_parameter = get_hyper_param(30)\n",
        "train_loss, val_loss = list(), list()\n",
        "best_params = []\n",
        "\n",
        "min_val_loss = 999\n",
        "for alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout in hyper_parameter:\n",
        "  print('lr',alpha, 'b1', b1, 'b2', b2, 'n_hiddens', hidden_units,'batch_size', batch_size, 'embedding_dim', embedding_dim, \n",
        "        'en_dropout',  en_dropout, 'de_dropout', de_dropout)\n",
        "\n",
        "  current_val_loss = GRU_Model(embedding_dim, hidden_units, alpha, b1, b2, batch_size, en_dropout, de_dropout)\n",
        "\n",
        "  if current_val_loss < min_val_loss:\n",
        "    min_val_loss = current_val_loss\n",
        "    best_params = [alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHVi_IiC2050",
        "outputId": "df017535-1ad1-46a3-b3e6-d2dfde8b5cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 0.015976380574435466 b1 0.6780522443921471 b2 0.9702656093996469 n_hiddens 153 batch_size 204 embedding_dim 137 en_dropout 0.25427391287383155 de_dropout 0.2550421119727371\n",
            "Epoch 1/10\n",
            "309/309 [==============================] - 24s 51ms/step - loss: 1.3410 - acc: 0.7907 - val_loss: 0.9090 - val_acc: 0.8371\n",
            "Epoch 2/10\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.7851 - acc: 0.8480 - val_loss: 0.7665 - val_acc: 0.8563\n",
            "Epoch 3/10\n",
            "309/309 [==============================] - 15s 49ms/step - loss: 0.6456 - acc: 0.8643 - val_loss: 0.7159 - val_acc: 0.8652\n",
            "Epoch 4/10\n",
            "309/309 [==============================] - 15s 49ms/step - loss: 0.5727 - acc: 0.8731 - val_loss: 0.6909 - val_acc: 0.8682\n",
            "Epoch 5/10\n",
            "309/309 [==============================] - 15s 49ms/step - loss: 0.5285 - acc: 0.8786 - val_loss: 0.6812 - val_acc: 0.8710\n",
            "Epoch 6/10\n",
            "309/309 [==============================] - 15s 49ms/step - loss: 0.5010 - acc: 0.8820 - val_loss: 0.6705 - val_acc: 0.8732\n",
            "Epoch 7/10\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4811 - acc: 0.8850 - val_loss: 0.6728 - val_acc: 0.8739\n",
            "Epoch 8/10\n",
            "309/309 [==============================] - 16s 51ms/step - loss: 0.4684 - acc: 0.8864 - val_loss: 0.6760 - val_acc: 0.8740\n",
            "Epoch 9/10\n",
            "309/309 [==============================] - 16s 50ms/step - loss: 0.4569 - acc: 0.8883 - val_loss: 0.6785 - val_acc: 0.8730\n",
            "best_params []\n",
            "lr 0.04491102755594867 b1 0.7805041825318126 b2 0.6664284177293263 n_hiddens 256 batch_size 504 embedding_dim 163 en_dropout 0.039384420858482205 de_dropout 0.13412067736728178\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 17s 116ms/step - loss: 2.0671 - acc: 0.7350 - val_loss: 1.3532 - val_acc: 0.7971\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 1.2320 - acc: 0.8075 - val_loss: 1.1628 - val_acc: 0.8171\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 1.0936 - acc: 0.8217 - val_loss: 1.1094 - val_acc: 0.8250\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 1.0282 - acc: 0.8284 - val_loss: 1.0788 - val_acc: 0.8266\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 0.9820 - acc: 0.8329 - val_loss: 1.0794 - val_acc: 0.8306\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 14s 111ms/step - loss: 0.9534 - acc: 0.8357 - val_loss: 1.0791 - val_acc: 0.8298\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 0.9341 - acc: 0.8373 - val_loss: 1.0706 - val_acc: 0.8318\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.9218 - acc: 0.8387 - val_loss: 1.0732 - val_acc: 0.8342\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 0.9053 - acc: 0.8404 - val_loss: 1.0638 - val_acc: 0.8340\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 14s 108ms/step - loss: 0.8994 - acc: 0.8407 - val_loss: 1.0837 - val_acc: 0.8314\n",
            "lr 0.0884681434931694 b1 0.9048386013925377 b2 0.5891021495742974 n_hiddens 215 batch_size 348 embedding_dim 95 en_dropout 0.01550395909148361 de_dropout 0.31092965465473177\n",
            "Epoch 1/10\n",
            "182/182 [==============================] - 19s 89ms/step - loss: 23.4962 - acc: 0.6431 - val_loss: 60.7800 - val_acc: 0.6625\n",
            "Epoch 2/10\n",
            "182/182 [==============================] - 14s 77ms/step - loss: 89.5127 - acc: 0.6738 - val_loss: 83.7200 - val_acc: 0.6886\n",
            "Epoch 3/10\n",
            "182/182 [==============================] - 13s 74ms/step - loss: 95.7194 - acc: 0.6747 - val_loss: 80.1516 - val_acc: 0.6934\n",
            "Epoch 4/10\n",
            "182/182 [==============================] - 13s 74ms/step - loss: 94.6992 - acc: 0.6746 - val_loss: 79.8158 - val_acc: 0.6774\n",
            "lr 0.05011331486823975 b1 0.8251790155806231 b2 0.7497627375042516 n_hiddens 213 batch_size 64 embedding_dim 143 en_dropout 0.4227941663236037 de_dropout 0.350609993194086\n",
            "Epoch 1/10\n",
            "985/985 [==============================] - 33s 30ms/step - loss: 2.5484 - acc: 0.7428 - val_loss: 2.5118 - val_acc: 0.7593\n",
            "Epoch 2/10\n",
            "985/985 [==============================] - 29s 30ms/step - loss: 2.6909 - acc: 0.7508 - val_loss: 2.5941 - val_acc: 0.7608\n",
            "Epoch 3/10\n",
            "985/985 [==============================] - 29s 30ms/step - loss: 2.7870 - acc: 0.7518 - val_loss: 2.6766 - val_acc: 0.7645\n",
            "Epoch 4/10\n",
            "985/985 [==============================] - 29s 30ms/step - loss: 2.7960 - acc: 0.7548 - val_loss: 2.6214 - val_acc: 0.7732\n",
            "lr 0.05290053904773371 b1 0.8504018240662914 b2 0.5545947153636592 n_hiddens 216 batch_size 416 embedding_dim 217 en_dropout 0.2905372536054169 de_dropout 0.07305719568671992\n",
            "Epoch 1/10\n",
            "152/152 [==============================] - 17s 93ms/step - loss: 7.2310 - acc: 0.6786 - val_loss: 11.3121 - val_acc: 0.7034\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 13s 87ms/step - loss: 12.8149 - acc: 0.7218 - val_loss: 12.1453 - val_acc: 0.7320\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 13s 87ms/step - loss: 13.3280 - acc: 0.7320 - val_loss: 13.7485 - val_acc: 0.7450\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 13s 87ms/step - loss: 14.5372 - acc: 0.7346 - val_loss: 14.2735 - val_acc: 0.7396\n",
            "lr 0.07429428631519759 b1 0.6257330598291797 b2 0.5610351994623055 n_hiddens 57 batch_size 268 embedding_dim 133 en_dropout 0.36664047401271016 de_dropout 0.44963842075160376\n",
            "Epoch 1/10\n",
            "236/236 [==============================] - 15s 51ms/step - loss: 1.6930 - acc: 0.7622 - val_loss: 1.4735 - val_acc: 0.7896\n",
            "Epoch 2/10\n",
            "236/236 [==============================] - 11s 48ms/step - loss: 1.4620 - acc: 0.7914 - val_loss: 1.4663 - val_acc: 0.7961\n",
            "Epoch 3/10\n",
            "236/236 [==============================] - 11s 48ms/step - loss: 1.4635 - acc: 0.7936 - val_loss: 1.4725 - val_acc: 0.7887\n",
            "Epoch 4/10\n",
            "236/236 [==============================] - 12s 50ms/step - loss: 1.4558 - acc: 0.7937 - val_loss: 1.4797 - val_acc: 0.7941\n",
            "Epoch 5/10\n",
            "236/236 [==============================] - 11s 48ms/step - loss: 1.4657 - acc: 0.7943 - val_loss: 1.4915 - val_acc: 0.7927\n",
            "lr 0.04226469674655408 b1 0.6352413793280154 b2 0.7775220932495259 n_hiddens 84 batch_size 96 embedding_dim 144 en_dropout 0.4687169902583067 de_dropout 0.2535364441691332\n",
            "Epoch 1/10\n",
            "657/657 [==============================] - 24s 32ms/step - loss: 1.3856 - acc: 0.7964 - val_loss: 1.2625 - val_acc: 0.8136\n",
            "Epoch 2/10\n",
            "657/657 [==============================] - 20s 30ms/step - loss: 1.2918 - acc: 0.8123 - val_loss: 1.2811 - val_acc: 0.8175\n",
            "Epoch 3/10\n",
            "657/657 [==============================] - 20s 30ms/step - loss: 1.3125 - acc: 0.8132 - val_loss: 1.3402 - val_acc: 0.8164\n",
            "Epoch 4/10\n",
            "657/657 [==============================] - 20s 30ms/step - loss: 1.3470 - acc: 0.8129 - val_loss: 1.3785 - val_acc: 0.8142\n",
            "lr 0.09676552530560915 b1 0.669837719851135 b2 0.9678316830781978 n_hiddens 497 batch_size 268 embedding_dim 64 en_dropout 0.2257602117952286 de_dropout 0.07230108146186526\n",
            "Epoch 1/10\n",
            "236/236 [==============================] - 25s 94ms/step - loss: 13.0855 - acc: 0.6791 - val_loss: 16.7558 - val_acc: 0.6741\n",
            "Epoch 2/10\n",
            "236/236 [==============================] - 21s 91ms/step - loss: 13.4125 - acc: 0.7137 - val_loss: 15.9469 - val_acc: 0.6936\n",
            "Epoch 3/10\n",
            "236/236 [==============================] - 21s 90ms/step - loss: 12.6499 - acc: 0.7177 - val_loss: 16.7709 - val_acc: 0.6937\n",
            "Epoch 4/10\n",
            "236/236 [==============================] - 21s 89ms/step - loss: 12.8215 - acc: 0.7198 - val_loss: 16.2651 - val_acc: 0.6942\n",
            "Epoch 5/10\n",
            "236/236 [==============================] - 21s 89ms/step - loss: 12.5866 - acc: 0.7215 - val_loss: 17.0226 - val_acc: 0.6949\n",
            "lr 0.004901850898283544 b1 0.9090371123950443 b2 0.5840071508266729 n_hiddens 182 batch_size 305 embedding_dim 209 en_dropout 0.3517318167393525 de_dropout 0.08006630965056405\n",
            "Epoch 1/10\n",
            "207/207 [==============================] - 18s 69ms/step - loss: 4.8927 - acc: 0.6725 - val_loss: 5.2362 - val_acc: 0.6967\n",
            "Epoch 2/10\n",
            "207/207 [==============================] - 14s 65ms/step - loss: 5.7940 - acc: 0.7160 - val_loss: 6.7098 - val_acc: 0.7261\n",
            "Epoch 3/10\n",
            "207/207 [==============================] - 14s 65ms/step - loss: 7.5500 - acc: 0.7211 - val_loss: 7.9858 - val_acc: 0.7242\n",
            "Epoch 4/10\n",
            "207/207 [==============================] - 14s 65ms/step - loss: 8.1691 - acc: 0.7269 - val_loss: 8.7638 - val_acc: 0.7304\n",
            "lr 0.07349070605403135 b1 0.8936282612088107 b2 0.6198697548771606 n_hiddens 8 batch_size 442 embedding_dim 76 en_dropout 0.299908268479212 de_dropout 0.34318101255078465\n",
            "Epoch 1/10\n",
            "143/143 [==============================] - 13s 69ms/step - loss: 4.6362 - acc: 0.5870 - val_loss: 4.4087 - val_acc: 0.6000\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.7478 - acc: 0.5992 - val_loss: 3.9768 - val_acc: 0.6000\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5524 - acc: 0.5992 - val_loss: 3.9223 - val_acc: 0.6000\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5340 - acc: 0.5992 - val_loss: 3.8560 - val_acc: 0.6000\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5131 - acc: 0.5992 - val_loss: 3.7972 - val_acc: 0.6000\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5141 - acc: 0.5992 - val_loss: 3.7623 - val_acc: 0.6000\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5118 - acc: 0.5992 - val_loss: 3.7157 - val_acc: 0.6000\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5278 - acc: 0.5992 - val_loss: 3.6498 - val_acc: 0.6000\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5163 - acc: 0.5992 - val_loss: 3.6132 - val_acc: 0.6000\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 3.5190 - acc: 0.5992 - val_loss: 3.5904 - val_acc: 0.6000\n",
            "lr 0.03132470848533549 b1 0.5971550104811092 b2 0.5297979768308256 n_hiddens 127 batch_size 430 embedding_dim 263 en_dropout 0.49315533412456414 de_dropout 0.33536654468165256\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 15s 83ms/step - loss: 1.4885 - acc: 0.7812 - val_loss: 1.0974 - val_acc: 0.8222\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 12s 78ms/step - loss: 1.0482 - acc: 0.8271 - val_loss: 1.0129 - val_acc: 0.8327\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 12s 79ms/step - loss: 1.0033 - acc: 0.8346 - val_loss: 1.0038 - val_acc: 0.8381\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 12s 79ms/step - loss: 0.9785 - acc: 0.8386 - val_loss: 0.9849 - val_acc: 0.8422\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 11s 78ms/step - loss: 0.9586 - acc: 0.8406 - val_loss: 0.9870 - val_acc: 0.8398\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 11s 78ms/step - loss: 0.9489 - acc: 0.8416 - val_loss: 0.9709 - val_acc: 0.8420\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 11s 78ms/step - loss: 0.9409 - acc: 0.8423 - val_loss: 0.9683 - val_acc: 0.8427\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 11s 78ms/step - loss: 0.9385 - acc: 0.8425 - val_loss: 0.9726 - val_acc: 0.8420\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 11s 78ms/step - loss: 0.9338 - acc: 0.8432 - val_loss: 0.9654 - val_acc: 0.8445\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 12s 78ms/step - loss: 0.9345 - acc: 0.8429 - val_loss: 0.9719 - val_acc: 0.8411\n",
            "lr 0.09846687693153813 b1 0.6637383730208867 b2 0.7937722306502992 n_hiddens 207 batch_size 335 embedding_dim 268 en_dropout 0.41634141506470085 de_dropout 0.3409050187867168\n",
            "Epoch 1/10\n",
            "189/189 [==============================] - 19s 86ms/step - loss: 2.8075 - acc: 0.7315 - val_loss: 2.4238 - val_acc: 0.7605\n",
            "Epoch 2/10\n",
            "189/189 [==============================] - 15s 80ms/step - loss: 2.0644 - acc: 0.7781 - val_loss: 2.1520 - val_acc: 0.7776\n",
            "Epoch 3/10\n",
            "189/189 [==============================] - 15s 79ms/step - loss: 1.9069 - acc: 0.7863 - val_loss: 2.0376 - val_acc: 0.7832\n",
            "Epoch 4/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.8104 - acc: 0.7917 - val_loss: 2.0329 - val_acc: 0.7918\n",
            "Epoch 5/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.7702 - acc: 0.7945 - val_loss: 2.0267 - val_acc: 0.7858\n",
            "Epoch 6/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.7308 - acc: 0.7974 - val_loss: 2.0218 - val_acc: 0.7845\n",
            "Epoch 7/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.6994 - acc: 0.7996 - val_loss: 2.0506 - val_acc: 0.7804\n",
            "Epoch 8/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.6826 - acc: 0.8016 - val_loss: 2.0140 - val_acc: 0.8023\n",
            "Epoch 9/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.6603 - acc: 0.8033 - val_loss: 1.9962 - val_acc: 0.7990\n",
            "Epoch 10/10\n",
            "189/189 [==============================] - 15s 78ms/step - loss: 1.6361 - acc: 0.8052 - val_loss: 2.0588 - val_acc: 0.7917\n",
            "lr 0.039549624903919216 b1 0.9739939909608515 b2 0.7871854551372 n_hiddens 247 batch_size 469 embedding_dim 101 en_dropout 0.14277700147280525 de_dropout 0.24315565819276508\n",
            "Epoch 1/10\n",
            "135/135 [==============================] - 17s 105ms/step - loss: 4.9607 - acc: 0.6848 - val_loss: 10.3656 - val_acc: 0.6918\n",
            "Epoch 2/10\n",
            "135/135 [==============================] - 13s 97ms/step - loss: 26.1501 - acc: 0.6693 - val_loss: 39.1693 - val_acc: 0.6682\n",
            "Epoch 3/10\n",
            "135/135 [==============================] - 13s 97ms/step - loss: 53.3866 - acc: 0.6593 - val_loss: 56.6143 - val_acc: 0.6701\n",
            "Epoch 4/10\n",
            "135/135 [==============================] - 13s 97ms/step - loss: 64.9352 - acc: 0.6604 - val_loss: 56.7668 - val_acc: 0.6836\n",
            "lr 0.09683220585923386 b1 0.8953572086688428 b2 0.5529256011187094 n_hiddens 99 batch_size 400 embedding_dim 188 en_dropout 0.258354321184434 de_dropout 0.2078229949971523\n",
            "Epoch 1/10\n",
            "158/158 [==============================] - 14s 73ms/step - loss: 18.1243 - acc: 0.6613 - val_loss: 36.5120 - val_acc: 0.6477\n",
            "Epoch 2/10\n",
            "158/158 [==============================] - 11s 68ms/step - loss: 64.8154 - acc: 0.6125 - val_loss: 61.9641 - val_acc: 0.6274\n",
            "Epoch 3/10\n",
            "158/158 [==============================] - 11s 68ms/step - loss: 88.8494 - acc: 0.6238 - val_loss: 123.7296 - val_acc: 0.6416\n",
            "Epoch 4/10\n",
            "158/158 [==============================] - 11s 72ms/step - loss: 408.2194 - acc: 0.4973 - val_loss: 287.5294 - val_acc: 0.4799\n",
            "lr 0.03050011010605813 b1 0.619887647106252 b2 0.550453094938836 n_hiddens 343 batch_size 118 embedding_dim 249 en_dropout 0.046083980287361104 de_dropout 0.3107335742863073\n",
            "Epoch 1/10\n",
            "534/534 [==============================] - 29s 47ms/step - loss: 1.5282 - acc: 0.7874 - val_loss: 1.3978 - val_acc: 0.8075\n",
            "Epoch 2/10\n",
            "534/534 [==============================] - 25s 46ms/step - loss: 1.3855 - acc: 0.8102 - val_loss: 1.4263 - val_acc: 0.8117\n",
            "Epoch 3/10\n",
            "534/534 [==============================] - 25s 46ms/step - loss: 1.4658 - acc: 0.8110 - val_loss: 1.5940 - val_acc: 0.8060\n",
            "Epoch 4/10\n",
            "534/534 [==============================] - 25s 46ms/step - loss: 1.5301 - acc: 0.8107 - val_loss: 1.6039 - val_acc: 0.8162\n",
            "lr 0.037119093565685095 b1 0.5550929350898923 b2 0.83323024203605 n_hiddens 219 batch_size 264 embedding_dim 286 en_dropout 0.10538275202028774 de_dropout 0.26138875678094553\n",
            "Epoch 1/10\n",
            "239/239 [==============================] - 19s 69ms/step - loss: 1.3944 - acc: 0.7904 - val_loss: 1.0829 - val_acc: 0.8215\n",
            "Epoch 2/10\n",
            "239/239 [==============================] - 16s 67ms/step - loss: 1.0202 - acc: 0.8283 - val_loss: 1.0159 - val_acc: 0.8306\n",
            "Epoch 3/10\n",
            "239/239 [==============================] - 16s 67ms/step - loss: 0.9509 - acc: 0.8358 - val_loss: 0.9923 - val_acc: 0.8339\n",
            "Epoch 4/10\n",
            "239/239 [==============================] - 16s 67ms/step - loss: 0.9095 - acc: 0.8392 - val_loss: 0.9675 - val_acc: 0.8387\n",
            "Epoch 5/10\n",
            "239/239 [==============================] - 16s 67ms/step - loss: 0.8902 - acc: 0.8407 - val_loss: 0.9828 - val_acc: 0.8396\n",
            "Epoch 6/10\n",
            "239/239 [==============================] - 16s 66ms/step - loss: 0.8718 - acc: 0.8428 - val_loss: 0.9795 - val_acc: 0.8389\n",
            "Epoch 7/10\n",
            "239/239 [==============================] - 16s 67ms/step - loss: 0.8563 - acc: 0.8440 - val_loss: 0.9898 - val_acc: 0.8383\n",
            "lr 0.0693433398149614 b1 0.8037209524712705 b2 0.6650451391789752 n_hiddens 453 batch_size 432 embedding_dim 299 en_dropout 0.1504797288195472 de_dropout 0.11388289288785092\n",
            "Epoch 1/10\n",
            "146/146 [==============================] - 25s 153ms/step - loss: 7.5002 - acc: 0.6858 - val_loss: 7.0704 - val_acc: 0.7277\n",
            "Epoch 2/10\n",
            "146/146 [==============================] - 21s 144ms/step - loss: 6.8215 - acc: 0.7359 - val_loss: 6.9967 - val_acc: 0.7520\n",
            "Epoch 3/10\n",
            "146/146 [==============================] - 20s 140ms/step - loss: 6.9019 - acc: 0.7489 - val_loss: 6.9604 - val_acc: 0.7574\n",
            "Epoch 4/10\n",
            "146/146 [==============================] - 20s 139ms/step - loss: 6.6614 - acc: 0.7577 - val_loss: 6.7974 - val_acc: 0.7621\n",
            "Epoch 5/10\n",
            "146/146 [==============================] - 20s 139ms/step - loss: 6.4794 - acc: 0.7645 - val_loss: 7.0319 - val_acc: 0.7743\n",
            "Epoch 6/10\n",
            "146/146 [==============================] - 20s 139ms/step - loss: 6.3164 - acc: 0.7706 - val_loss: 6.9389 - val_acc: 0.7660\n",
            "Epoch 7/10\n",
            "146/146 [==============================] - 20s 139ms/step - loss: 6.1500 - acc: 0.7760 - val_loss: 7.0607 - val_acc: 0.7816\n",
            "lr 0.034887476186179524 b1 0.8499572523336213 b2 0.9265745537884973 n_hiddens 454 batch_size 50 embedding_dim 151 en_dropout 0.36775474890652565 de_dropout 0.16863362306027252\n",
            "Epoch 1/10\n",
            "1260/1260 [==============================] - 41s 31ms/step - loss: 3.4585 - acc: 0.7420 - val_loss: 4.1658 - val_acc: 0.7517\n",
            "Epoch 2/10\n",
            "1260/1260 [==============================] - 38s 30ms/step - loss: 4.4215 - acc: 0.7443 - val_loss: 4.4205 - val_acc: 0.7498\n",
            "Epoch 3/10\n",
            "1260/1260 [==============================] - 38s 31ms/step - loss: 4.6730 - acc: 0.7430 - val_loss: 4.6331 - val_acc: 0.7570\n",
            "Epoch 4/10\n",
            "1260/1260 [==============================] - 39s 31ms/step - loss: 4.9299 - acc: 0.7388 - val_loss: 5.0294 - val_acc: 0.7491\n",
            "lr 0.025878495785565992 b1 0.5573053939990477 b2 0.8599318199946977 n_hiddens 491 batch_size 282 embedding_dim 127 en_dropout 0.2502815176808676 de_dropout 0.021732888803815797\n",
            "Epoch 1/10\n",
            "224/224 [==============================] - 26s 106ms/step - loss: 1.5340 - acc: 0.7833 - val_loss: 0.9108 - val_acc: 0.8421\n",
            "Epoch 2/10\n",
            "224/224 [==============================] - 23s 103ms/step - loss: 0.8115 - acc: 0.8536 - val_loss: 0.8172 - val_acc: 0.8565\n",
            "Epoch 3/10\n",
            "224/224 [==============================] - 24s 105ms/step - loss: 0.7445 - acc: 0.8611 - val_loss: 0.7924 - val_acc: 0.8584\n",
            "Epoch 4/10\n",
            "224/224 [==============================] - 24s 107ms/step - loss: 0.6959 - acc: 0.8658 - val_loss: 0.8459 - val_acc: 0.8548\n",
            "Epoch 5/10\n",
            "224/224 [==============================] - 24s 107ms/step - loss: 0.6740 - acc: 0.8677 - val_loss: 0.8092 - val_acc: 0.8578\n",
            "Epoch 6/10\n",
            "224/224 [==============================] - 24s 108ms/step - loss: 0.6557 - acc: 0.8698 - val_loss: 0.8200 - val_acc: 0.8609\n",
            "lr 0.025147351631887294 b1 0.832287647463699 b2 0.7294788474301586 n_hiddens 371 batch_size 453 embedding_dim 172 en_dropout 0.27819869607734116 de_dropout 0.297570470648657\n",
            "Epoch 1/10\n",
            "140/140 [==============================] - 22s 134ms/step - loss: 1.8696 - acc: 0.7490 - val_loss: 1.3368 - val_acc: 0.8012\n",
            "Epoch 2/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 1.2013 - acc: 0.8146 - val_loss: 1.1454 - val_acc: 0.8207\n",
            "Epoch 3/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 1.0547 - acc: 0.8287 - val_loss: 1.0749 - val_acc: 0.8312\n",
            "Epoch 4/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 0.9721 - acc: 0.8358 - val_loss: 1.0238 - val_acc: 0.8343\n",
            "Epoch 5/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 0.9185 - acc: 0.8407 - val_loss: 1.0093 - val_acc: 0.8395\n",
            "Epoch 6/10\n",
            "140/140 [==============================] - 18s 125ms/step - loss: 0.8752 - acc: 0.8449 - val_loss: 0.9912 - val_acc: 0.8410\n",
            "Epoch 7/10\n",
            "140/140 [==============================] - 17s 125ms/step - loss: 0.8466 - acc: 0.8473 - val_loss: 0.9870 - val_acc: 0.8423\n",
            "Epoch 8/10\n",
            "140/140 [==============================] - 17s 125ms/step - loss: 0.8216 - acc: 0.8495 - val_loss: 0.9925 - val_acc: 0.8435\n",
            "Epoch 9/10\n",
            "140/140 [==============================] - 17s 124ms/step - loss: 0.8049 - acc: 0.8518 - val_loss: 0.9924 - val_acc: 0.8423\n",
            "Epoch 10/10\n",
            "140/140 [==============================] - 18s 127ms/step - loss: 0.7865 - acc: 0.8540 - val_loss: 1.0183 - val_acc: 0.8432\n",
            "lr 0.062442088182150246 b1 0.8260799924454298 b2 0.5876783176085574 n_hiddens 512 batch_size 458 embedding_dim 83 en_dropout 0.3386266262041194 de_dropout 0.4945738191922385\n",
            "Epoch 1/10\n",
            "138/138 [==============================] - 24s 153ms/step - loss: 6.9590 - acc: 0.6859 - val_loss: 9.3542 - val_acc: 0.7193\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 20s 144ms/step - loss: 11.6178 - acc: 0.7149 - val_loss: 12.5806 - val_acc: 0.7365\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 20s 142ms/step - loss: 14.3727 - acc: 0.7168 - val_loss: 14.5036 - val_acc: 0.7225\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 19s 141ms/step - loss: 15.4061 - acc: 0.7163 - val_loss: 15.4411 - val_acc: 0.7302\n",
            "lr 0.08139662440715888 b1 0.9220813659535774 b2 0.5661674844979955 n_hiddens 413 batch_size 346 embedding_dim 58 en_dropout 0.23782082709722763 de_dropout 0.49331699469110823\n",
            "Epoch 1/10\n",
            "183/183 [==============================] - 22s 108ms/step - loss: 431.8622 - acc: 0.1841 - val_loss: 490.3604 - val_acc: 0.4885\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 18s 101ms/step - loss: 482.1654 - acc: 0.5793 - val_loss: 304.5826 - val_acc: 0.6752\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 318.0076 - acc: 0.6588 - val_loss: 238.7361 - val_acc: 0.6962\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 260.2776 - acc: 0.6729 - val_loss: 215.2524 - val_acc: 0.7081\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 236.0615 - acc: 0.6768 - val_loss: 201.4750 - val_acc: 0.7047\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 223.1039 - acc: 0.6783 - val_loss: 194.1261 - val_acc: 0.7092\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 214.9514 - acc: 0.6795 - val_loss: 190.2137 - val_acc: 0.7105\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 208.2602 - acc: 0.6803 - val_loss: 185.2534 - val_acc: 0.7119\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 205.4124 - acc: 0.6805 - val_loss: 182.6447 - val_acc: 0.7067\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 18s 100ms/step - loss: 201.0093 - acc: 0.6808 - val_loss: 181.7946 - val_acc: 0.7079\n",
            "lr 0.06489428378773703 b1 0.8180072642764915 b2 0.5946182736575747 n_hiddens 183 batch_size 116 embedding_dim 179 en_dropout 0.2960524367290365 de_dropout 0.351067973548734\n",
            "Epoch 1/10\n",
            "544/544 [==============================] - 24s 39ms/step - loss: 5.4982 - acc: 0.7285 - val_loss: 6.5011 - val_acc: 0.7431\n",
            "Epoch 2/10\n",
            "544/544 [==============================] - 21s 38ms/step - loss: 7.2444 - acc: 0.7357 - val_loss: 7.5075 - val_acc: 0.7357\n",
            "Epoch 3/10\n",
            "544/544 [==============================] - 20s 38ms/step - loss: 8.2126 - acc: 0.7359 - val_loss: 8.0820 - val_acc: 0.7450\n",
            "Epoch 4/10\n",
            "544/544 [==============================] - 20s 37ms/step - loss: 8.8300 - acc: 0.7358 - val_loss: 8.8668 - val_acc: 0.7452\n",
            "lr 0.041646346845400944 b1 0.7714302872957428 b2 0.6878614609939232 n_hiddens 388 batch_size 163 embedding_dim 62 en_dropout 0.10075135058241935 de_dropout 0.2433291640208579\n",
            "Epoch 1/10\n",
            "387/387 [==============================] - 27s 64ms/step - loss: 1.9413 - acc: 0.7659 - val_loss: 1.2826 - val_acc: 0.8074\n",
            "Epoch 2/10\n",
            "387/387 [==============================] - 24s 62ms/step - loss: 1.2671 - acc: 0.8102 - val_loss: 1.2593 - val_acc: 0.8125\n",
            "Epoch 3/10\n",
            "387/387 [==============================] - 24s 62ms/step - loss: 1.2724 - acc: 0.8112 - val_loss: 1.3148 - val_acc: 0.8121\n",
            "Epoch 4/10\n",
            "387/387 [==============================] - 23s 61ms/step - loss: 1.3071 - acc: 0.8107 - val_loss: 1.3554 - val_acc: 0.8153\n",
            "Epoch 5/10\n",
            "387/387 [==============================] - 23s 60ms/step - loss: 1.3373 - acc: 0.8106 - val_loss: 1.4152 - val_acc: 0.8128\n",
            "lr 0.03325607146838409 b1 0.6516160840834683 b2 0.9407176384678588 n_hiddens 98 batch_size 199 embedding_dim 226 en_dropout 0.3645239401191509 de_dropout 0.4381150952994617\n",
            "Epoch 1/10\n",
            "317/317 [==============================] - 18s 47ms/step - loss: 1.3244 - acc: 0.7944 - val_loss: 1.0164 - val_acc: 0.8266\n",
            "Epoch 2/10\n",
            "317/317 [==============================] - 14s 45ms/step - loss: 0.9576 - acc: 0.8298 - val_loss: 0.9557 - val_acc: 0.8355\n",
            "Epoch 3/10\n",
            "317/317 [==============================] - 14s 45ms/step - loss: 0.8822 - acc: 0.8367 - val_loss: 0.9203 - val_acc: 0.8401\n",
            "Epoch 4/10\n",
            "317/317 [==============================] - 14s 44ms/step - loss: 0.8418 - acc: 0.8401 - val_loss: 0.9151 - val_acc: 0.8399\n",
            "Epoch 5/10\n",
            "317/317 [==============================] - 14s 44ms/step - loss: 0.8166 - acc: 0.8420 - val_loss: 0.9018 - val_acc: 0.8415\n",
            "Epoch 6/10\n",
            "317/317 [==============================] - 14s 44ms/step - loss: 0.7992 - acc: 0.8435 - val_loss: 0.9046 - val_acc: 0.8426\n",
            "Epoch 7/10\n",
            "317/317 [==============================] - 14s 44ms/step - loss: 0.7830 - acc: 0.8452 - val_loss: 0.9082 - val_acc: 0.8410\n",
            "Epoch 8/10\n",
            "317/317 [==============================] - 14s 44ms/step - loss: 0.7718 - acc: 0.8461 - val_loss: 0.9045 - val_acc: 0.8431\n",
            "lr 0.07089442500893914 b1 0.7415078921465774 b2 0.713051593855119 n_hiddens 466 batch_size 416 embedding_dim 182 en_dropout 0.4263131152514977 de_dropout 0.3676438176812881\n",
            "Epoch 1/10\n",
            "152/152 [==============================] - 25s 150ms/step - loss: 5.7765 - acc: 0.6875 - val_loss: 3.4863 - val_acc: 0.7493\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 22s 142ms/step - loss: 2.9947 - acc: 0.7589 - val_loss: 2.7324 - val_acc: 0.7698\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 21s 140ms/step - loss: 2.5571 - acc: 0.7750 - val_loss: 2.4862 - val_acc: 0.7871\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 21s 139ms/step - loss: 2.3421 - acc: 0.7845 - val_loss: 2.3406 - val_acc: 0.7958\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 21s 140ms/step - loss: 2.2201 - acc: 0.7904 - val_loss: 2.2451 - val_acc: 0.8016\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 21s 141ms/step - loss: 2.1123 - acc: 0.7955 - val_loss: 2.2379 - val_acc: 0.8013\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 21s 140ms/step - loss: 2.0449 - acc: 0.7997 - val_loss: 2.1714 - val_acc: 0.8105\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 21s 140ms/step - loss: 1.9670 - acc: 0.8035 - val_loss: 2.1669 - val_acc: 0.8107\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 21s 141ms/step - loss: 1.9354 - acc: 0.8062 - val_loss: 2.2211 - val_acc: 0.8077\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 21s 141ms/step - loss: 1.9180 - acc: 0.8082 - val_loss: 2.2123 - val_acc: 0.8113\n",
            "lr 0.09082062859565186 b1 0.8333757639958714 b2 0.6304759771613146 n_hiddens 125 batch_size 219 embedding_dim 174 en_dropout 0.3536552779351306 de_dropout 0.0938062604006697\n",
            "Epoch 1/10\n",
            "288/288 [==============================] - 18s 52ms/step - loss: 4.1246 - acc: 0.7263 - val_loss: 4.7627 - val_acc: 0.7455\n",
            "Epoch 2/10\n",
            "288/288 [==============================] - 14s 49ms/step - loss: 5.0721 - acc: 0.7472 - val_loss: 4.9235 - val_acc: 0.7536\n",
            "Epoch 3/10\n",
            "288/288 [==============================] - 14s 48ms/step - loss: 5.2190 - acc: 0.7529 - val_loss: 4.8394 - val_acc: 0.7619\n",
            "Epoch 4/10\n",
            "288/288 [==============================] - 14s 47ms/step - loss: 5.1778 - acc: 0.7553 - val_loss: 5.0352 - val_acc: 0.7600\n",
            "lr 0.07070573511818411 b1 0.655979867157936 b2 0.9340874483853273 n_hiddens 206 batch_size 220 embedding_dim 294 en_dropout 0.48382642885728033 de_dropout 0.35789310492049714\n",
            "Epoch 1/10\n",
            "287/287 [==============================] - 23s 66ms/step - loss: 2.0166 - acc: 0.7602 - val_loss: 1.5786 - val_acc: 0.7959\n",
            "Epoch 2/10\n",
            "287/287 [==============================] - 18s 62ms/step - loss: 1.5147 - acc: 0.7960 - val_loss: 1.5185 - val_acc: 0.8024\n",
            "Epoch 3/10\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 1.4217 - acc: 0.8037 - val_loss: 1.5197 - val_acc: 0.8053\n",
            "Epoch 4/10\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 1.3556 - acc: 0.8088 - val_loss: 1.5531 - val_acc: 0.8067\n",
            "Epoch 5/10\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 1.3229 - acc: 0.8122 - val_loss: 1.5596 - val_acc: 0.8117\n",
            "lr 0.08058572467196032 b1 0.5835942748544141 b2 0.5402689485081582 n_hiddens 267 batch_size 446 embedding_dim 245 en_dropout 0.23585421154618202 de_dropout 0.27245848008821116\n",
            "Epoch 1/10\n",
            "142/142 [==============================] - 22s 134ms/step - loss: 3.3759 - acc: 0.7133 - val_loss: 2.7415 - val_acc: 0.7612\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 18s 128ms/step - loss: 2.6069 - acc: 0.7670 - val_loss: 2.6083 - val_acc: 0.7712\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 18s 127ms/step - loss: 2.4893 - acc: 0.7751 - val_loss: 2.6217 - val_acc: 0.7764\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 18s 127ms/step - loss: 2.4054 - acc: 0.7806 - val_loss: 2.5352 - val_acc: 0.7822\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 18s 126ms/step - loss: 2.2964 - acc: 0.7868 - val_loss: 2.4744 - val_acc: 0.7906\n",
            "Epoch 6/10\n",
            "142/142 [==============================] - 18s 126ms/step - loss: 2.2298 - acc: 0.7906 - val_loss: 2.5127 - val_acc: 0.7889\n",
            "Epoch 7/10\n",
            "142/142 [==============================] - 18s 126ms/step - loss: 2.1754 - acc: 0.7938 - val_loss: 2.4371 - val_acc: 0.7959\n",
            "Epoch 8/10\n",
            "142/142 [==============================] - 18s 126ms/step - loss: 2.1242 - acc: 0.7964 - val_loss: 2.4273 - val_acc: 0.7981\n",
            "Epoch 9/10\n",
            "142/142 [==============================] - 19s 131ms/step - loss: 2.0770 - acc: 0.7984 - val_loss: 2.4296 - val_acc: 0.7962\n",
            "Epoch 10/10\n",
            "142/142 [==============================] - 18s 127ms/step - loss: 2.0357 - acc: 0.8002 - val_loss: 2.4337 - val_acc: 0.7921\n",
            "lr 0.095893107050256 b1 0.8041718117790025 b2 0.9644786340186802 n_hiddens 111 batch_size 104 embedding_dim 242 en_dropout 0.08397261375798709 de_dropout 0.40036426600016994\n",
            "Epoch 1/10\n",
            "606/606 [==============================] - 25s 35ms/step - loss: 2.5462 - acc: 0.7476 - val_loss: 2.4827 - val_acc: 0.7689\n",
            "Epoch 2/10\n",
            "606/606 [==============================] - 21s 34ms/step - loss: 2.5443 - acc: 0.7613 - val_loss: 2.5353 - val_acc: 0.7709\n",
            "Epoch 3/10\n",
            "606/606 [==============================] - 21s 34ms/step - loss: 2.5772 - acc: 0.7621 - val_loss: 2.6733 - val_acc: 0.7628\n",
            "Epoch 4/10\n",
            "606/606 [==============================] - 20s 34ms/step - loss: 2.6098 - acc: 0.7635 - val_loss: 2.7403 - val_acc: 0.7683\n",
            "final best params []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final GRU model**\n",
        "\n",
        "With final best hyperparameter for GRU model, we build our final GRU model based on those hyperparameter. "
      ],
      "metadata": {
        "id": "fb2WyGduD5CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate, b1, b2, hidden_units, batch_size, embedding_dim, encoder dropout, decoder dropout\n",
        "print('final best params',best_params)\n",
        "\n",
        "GRU_final_lr = best_params[0]\n",
        "GRU_final_b1 = best_params[1]\n",
        "GRU_final_b2 = best_params[2]\n",
        "GRU_hidden_units = best_params[3]\n",
        "GRU_final_batch = best_params[4]\n",
        "GRU_embedding_dim = best_params[5]\n",
        "GRU_final_enD = best_params[6]\n",
        "GRU_final_deD = best_params[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzXTx1XtJUED",
        "outputId": "c03f825f-446c-42f6-fc5b-50957f619f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final best params [0.015976380574435466, 0.6780522443921471, 0.9702656093996469, 153, 204, 137, 0.25427391287383155, 0.2550421119727371]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = [0.015976380574435466, 0.6780522443921471, 0.9702656093996469, 153, 204, 137, 0.25427391287383155, 0.2550421119727371]"
      ],
      "metadata": {
        "id": "HeGI3Lovl52K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Final_GRU_Model(embedding_dim, hidden_units, lr, b1, b2, batchsize, encoder_dropout, decoder_dropout):\n",
        "  model = create_train_model(\n",
        "    src_seqlen, tar_seqlen, src_vocab_size, tar_vocab_size, \n",
        "    hidden_units, embedding_dim, \n",
        "    encoder_dropout, decoder_dropout\n",
        "  )\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  model_weights_path_gru = './gdrive/MyDrive/AI/teamProject/GRU/weights/weights.ckpt'\n",
        "\n",
        "  save_best_weights = ModelCheckpoint(\n",
        "    model_weights_path_gru, monitor='val_loss', mode='min',\n",
        "    save_weights_only=True, save_best_only=True, verbose=1, \n",
        "  )\n",
        "\n",
        "  history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 10)], epochs=50)\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "xGu_BEA0Qr9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Final_GRU_Model(GRU_embedding_dim, GRU_hidden_units, GRU_final_lr, GRU_final_b1, GRU_final_b2, GRU_final_batch, GRU_final_enD, GRU_final_deD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH4mlYXZUjSG",
        "outputId": "9aff4af1-5257-40e5-9deb-b9dbcfab44c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "309/309 [==============================] - 19s 51ms/step - loss: 1.3089 - acc: 0.7944 - val_loss: 0.8632 - val_acc: 0.8441\n",
            "Epoch 2/50\n",
            "309/309 [==============================] - 15s 49ms/step - loss: 0.7337 - acc: 0.8552 - val_loss: 0.7159 - val_acc: 0.8647\n",
            "Epoch 3/50\n",
            "309/309 [==============================] - 16s 51ms/step - loss: 0.5901 - acc: 0.8728 - val_loss: 0.6697 - val_acc: 0.8718\n",
            "Epoch 4/50\n",
            "309/309 [==============================] - 16s 50ms/step - loss: 0.5242 - acc: 0.8809 - val_loss: 0.6556 - val_acc: 0.8755\n",
            "Epoch 5/50\n",
            "309/309 [==============================] - 16s 50ms/step - loss: 0.4875 - acc: 0.8854 - val_loss: 0.6548 - val_acc: 0.8755\n",
            "Epoch 6/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4631 - acc: 0.8886 - val_loss: 0.6538 - val_acc: 0.8764\n",
            "Epoch 7/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4479 - acc: 0.8906 - val_loss: 0.6508 - val_acc: 0.8769\n",
            "Epoch 8/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4369 - acc: 0.8916 - val_loss: 0.6495 - val_acc: 0.8774\n",
            "Epoch 9/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4278 - acc: 0.8934 - val_loss: 0.6539 - val_acc: 0.8789\n",
            "Epoch 10/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4192 - acc: 0.8949 - val_loss: 0.6600 - val_acc: 0.8787\n",
            "Epoch 11/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4166 - acc: 0.8953 - val_loss: 0.6633 - val_acc: 0.8783\n",
            "Epoch 12/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4120 - acc: 0.8959 - val_loss: 0.6694 - val_acc: 0.8782\n",
            "Epoch 13/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4096 - acc: 0.8964 - val_loss: 0.6708 - val_acc: 0.8784\n",
            "Epoch 14/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4057 - acc: 0.8968 - val_loss: 0.6757 - val_acc: 0.8791\n",
            "Epoch 15/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.4026 - acc: 0.8975 - val_loss: 0.6774 - val_acc: 0.8785\n",
            "Epoch 16/50\n",
            "309/309 [==============================] - 16s 51ms/step - loss: 0.4017 - acc: 0.8978 - val_loss: 0.6839 - val_acc: 0.8783\n",
            "Epoch 17/50\n",
            "309/309 [==============================] - 15s 50ms/step - loss: 0.3994 - acc: 0.8980 - val_loss: 0.6882 - val_acc: 0.8773\n",
            "Epoch 18/50\n",
            "309/309 [==============================] - 16s 50ms/step - loss: 0.3984 - acc: 0.8984 - val_loss: 0.6907 - val_acc: 0.8775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "dzRK2PifmBXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inference_encoder_from(input_layer, embedding_layer, gru_layer):\n",
        "  encoder_embedding = embedding_layer(input_layer)\n",
        "  encoder_outputs, encoder_state = gru_layer(encoder_embedding)\n",
        "  encoder = Model(input_layer, encoder_state)\n",
        "  return encoder\n",
        "\n",
        "def create_inference_decoder_from(input_layer, embedding_layer, gru_layer, dense_layer):\n",
        "  # decoder_input = Input(shape=(1,))\n",
        "  decoder_embedding = embedding_layer(input_layer)\n",
        "  input_shape = dense_layer.input.shape[-1]\n",
        "  decoder_inputs_state = Input(shape=(input_shape,))\n",
        "\n",
        "  decoder_output, decoder_output_state = gru_layer(\n",
        "    decoder_embedding, initial_state=decoder_inputs_state\n",
        "  )\n",
        "\n",
        "  decoder_prediction = dense_layer(decoder_output)\n",
        "  decoder = Model(\n",
        "    inputs=[input_layer, decoder_inputs_state], \n",
        "    outputs=[decoder_prediction, decoder_output_state]\n",
        "  )\n",
        "  return decoder"
      ],
      "metadata": {
        "id": "qO92pTyzD9IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = create_inference_encoder_from(\n",
        "  model.get_layer('encoder_input').input,\n",
        "  model.get_layer('encoder_embedding'),\n",
        "  model.get_layer('encoder_gru')\n",
        ")\n",
        "\n",
        "decoder = create_inference_decoder_from(\n",
        "  model.get_layer('decoder_input').input,\n",
        "  model.get_layer('decoder_embedding'),\n",
        "  model.get_layer('decoder_gru'),\n",
        "  model.get_layer('dense_layer'),\n",
        ")"
      ],
      "metadata": {
        "id": "dBROLhPAEIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "    state = encoder.predict(input_seq)\n",
        "    \n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, state = decoder.predict([target_seq, state])\n",
        "        \n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        \n",
        "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        \n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        state = [state]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "SYfeWyjdEMAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######     \n",
        "\n",
        "### We set random parameter for comparing LSTM and GRU model. (here, same parameter is used for both models.)\n",
        "\n",
        "# hidden_units = 64, embedding_dim = 64, batch_size = 128, 50_epoch "
      ],
      "metadata": {
        "id": "XeEt0LE6o3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicted Result & Evaluation**\n",
        "\n",
        "### Evaluation Metrics\n",
        "Same metrics as LSTM."
      ],
      "metadata": {
        "id": "APNeSEp6SlOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "1dvVYV_iVs5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "    reference = [[]]\n",
        "    candidate = []\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "\n",
        "    input_sentence = seq_to_src(encoder_input_train[seq_index])\n",
        "    answer_sentence = seq_to_tar(decoder_input_train[seq_index])\n",
        "    translated_sentence = decoded_sentence[1:-5]\n",
        "\n",
        "    print(\"Input Sentence :\", input_sentence)\n",
        "    print(\"Answer Sentence: \", answer_sentence)\n",
        "    print(\"Translated Sentence :\", translated_sentence)\n",
        "\n",
        "    print(rouge.get_scores([translated_sentence], [answer_sentence], avg=True))\n",
        "    reference[0].extend(answer_sentence.split())\n",
        "    candidate.extend(translated_sentence.split())\n",
        "    print('BLEU score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(1.0, 0, 0, 0))))\n",
        "    \n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n0KH_3AEOTQ",
        "outputId": "134a9c2f-a93a-4ee1-d7e6-92604695035a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Input Sentence : we had a little water . \n",
            "Answer Sentence:  nous disposions d un peu d eau . \n",
            "Translated Sentence : on a eu un peu d eau . \n",
            "{'rouge-1': {'r': 0.7142857142857143, 'p': 0.625, 'f': 0.6666666616888889}, 'rouge-2': {'r': 0.5714285714285714, 'p': 0.5714285714285714, 'f': 0.5714285664285715}, 'rouge-l': {'r': 0.7142857142857143, 'p': 0.625, 'f': 0.6666666616888889}}\n",
            "BLEU score: 0.62500\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Input Sentence : who wants to go ? \n",
            "Answer Sentence:  qui veut y aller ? \n",
            "Translated Sentence : qui veut aller ? \n",
            "{'rouge-1': {'r': 0.8, 'p': 1.0, 'f': 0.8888888839506174}, 'rouge-2': {'r': 0.5, 'p': 0.6666666666666666, 'f': 0.5714285665306124}, 'rouge-l': {'r': 0.8, 'p': 1.0, 'f': 0.8888888839506174}}\n",
            "BLEU score: 0.77880\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Input Sentence : who made this pie ? \n",
            "Answer Sentence:  qui a fait cette tourte ? \n",
            "Translated Sentence : qui a fait cette tarte ? \n",
            "{'rouge-1': {'r': 0.8333333333333334, 'p': 0.8333333333333334, 'f': 0.8333333283333335}, 'rouge-2': {'r': 0.6, 'p': 0.6, 'f': 0.5999999950000001}, 'rouge-l': {'r': 0.8333333333333334, 'p': 0.8333333333333334, 'f': 0.8333333283333335}}\n",
            "BLEU score: 0.83333\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Input Sentence : it feels like rain . \n",
            "Answer Sentence:  on dirait qu il va pleuvoir . \n",
            "Translated Sentence : il semble qu il pleuve . \n",
            "{'rouge-1': {'r': 0.42857142857142855, 'p': 0.6, 'f': 0.499999995138889}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.2, 'f': 0.18181817685950424}, 'rouge-l': {'r': 0.42857142857142855, 'p': 0.6, 'f': 0.499999995138889}}\n",
            "BLEU score: 0.42324\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Input Sentence : this annoys me . \n",
            "Answer Sentence:  ca m agace . \n",
            "Translated Sentence : ca me gave . \n",
            "{'rouge-1': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001}}\n",
            "BLEU score: 0.50000\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Result and Insights**\n",
        "\n",
        "Our final application is making voice with translated text."
      ],
      "metadata": {
        "id": "tp2GbEBYCjTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzhBgKWUcHex",
        "outputId": "f1f8ad20-2688-4849-92bf-13546fbecbb8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "muN38XTRZGto",
        "outputId": "87b28809-e9d2-46ec-f573-6dee51753a99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.3.0-py3-none-any.whl (26 kB)\n",
            "Collecting requests~=2.28.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 561 kB/s \n",
            "\u001b[?25hCollecting six~=1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting click~=8.1.3\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (1.24.3)\n",
            "Installing collected packages: six, requests, click, gTTS\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.1.3 gTTS-2.3.0 requests-2.28.1 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sErXhiB_ZHU5",
        "outputId": "b54011c7-ff0f-4700-8c8a-255d0f1ae0cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.8.16)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Hi, everybody. Playing with Python is fun!!!\" # put translated text here\n",
        "\n",
        "tts = gTTS(text=text, lang='en')\n",
        "tts.save(\"helloEN.mp3\")"
      ],
      "metadata": {
        "id": "B9-oWqrMZjYi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load('./helloEN.mp3')\n",
        "pygame.mixer.music.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "PbBH4_2calzZ",
        "outputId": "8507cd1c-afc5-4d0e-ced5-8d4cbcd5d0d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-01d12ed32b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./helloEN.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: ALSA: Couldn't open audio device: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "[1] Han, Lifeng. \"Machine translation evaluation resources and methods: A survey.\" arXiv preprint arXiv:1605.04515 (2016).\n",
        "\n",
        "[2] Chimalamarri, Santwana, and Dinkar Sitaram. \"Linguistically enhanced word segmentation for better neural machine translation of low resource agglutinative languages.\" International Journal of Speech Technology 24.4 (2021): 1047-1053.\n",
        "\n",
        "[3] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n",
        "\n",
        "[4] Papineni, Kishore, et al. \"Bleu: a method for automatic evaluation of machine translation.\" Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.\n",
        "\n",
        "[5] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "pe68r1Q1EYcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Member's contribution statement**"
      ],
      "metadata": {
        "id": "A3apgZJC7PCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Debugging experience worth sharing**"
      ],
      "metadata": {
        "id": "WMhSVCTg7S6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Github repository with the commit history**"
      ],
      "metadata": {
        "id": "NPnwleLT7W5_"
      }
    }
  ]
}