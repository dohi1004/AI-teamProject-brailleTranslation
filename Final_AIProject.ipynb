{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohi1004/AI-teamProject-brailleTranslation/blob/main/Final_AIProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# **AI Team Project Final Presentation**\n",
        "\n",
        "### **Team 6**\n",
        "\n",
        "16102269 Kim Jong Gyu\n",
        "\n",
        "19102095 Lee Do Hui\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "VpRu5GlOLhbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Motivation For The Proeject**\n",
        "* Similar to general language, braille is different according to the world.\n",
        "* T\n",
        "\n",
        "시각 장애인이 외국어로 되어 있는 점자를 읽어야 하는 경우 해당 나라 언어의 점자를 배운 적이 없다면 읽을 수 없습니다. 따라서, 외국어 점자를 모국어 음성으로 변환해주는 application을 만들고자 합니다."
      ],
      "metadata": {
        "id": "xz2a97bVLqBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Problem Description**\n",
        "Here, we suppose the use case as follows. Here, we use similar word order languages (English, French) because of using basic NMT model.\n",
        "\n",
        "1) English blind \n",
        "\n",
        "2) French\n",
        "\n",
        "1. English blind write braille \n",
        "2. Translate English braille to English (with Braille Translation)\n",
        "3. Translate English to French (with Neural Machine Translation)\n",
        "\n",
        "\n",
        "**Braille Translation and Language Translation**\n",
        "\n",
        "### 1) Braille Translation\n",
        "\n",
        "Translate English braille to English.\n",
        "\n",
        "### 2) Language Translation\n",
        "\n",
        "Translate English to French.\n",
        "\n",
        "**Finally, our application can translate other language's braille to target person's language by connecting those two models.** \n",
        "\n"
      ],
      "metadata": {
        "id": "sXhdwLhc6aJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Description**\n",
        "\n",
        "### **Braille Translation**\n",
        "\n",
        "* Braille Characteristics\n",
        "  * Braille should basically have six points placed at equal intervals.\n",
        "  * Braille should not be inverted too much (less than 90 degree).\n",
        "  * Spaces should be written at regular intervals.\n",
        "* Kaggle Data\n",
        "  * Problems\n",
        "  \n",
        "     <img src = \"https://drive.google.com/uc?id=1VeUkM0AQPdw3uxnFWUu8bq5vyGjoeKef\" height = 300 width = 500>\n",
        "    \n",
        "    * There are pictures with a rotation of more than 90 degrees and indistinguishable. \n",
        "    \n",
        "    <img src = \"https://drive.google.com/uc?id=1UQ8jsegwxsHsrih4eBFWJR3wS4a66JgO\" height = 300 width = 500>\n",
        "\n",
        "    * Only alphabets are used except for special characters.\n",
        "\n",
        "* **Actual data used**\n",
        "  * We make dataset by drawing it ourselves using PPT.\n",
        "\n",
        "    <img src = \"https://drive.google.com/uc?id=1ZunHu0KumEkiVuFcUvJGYG2cIHza_Vw3\" height = 300 width = 500>\n",
        "  \n",
        "  * **Actual data**\n",
        "  \n",
        "    <img src = \"https://drive.google.com/uc?id=1Kwm0NxQTKBNKQ4Z-Gau56bMnLlWFF948\" height = 300 width = 500>\n",
        "  \n",
        "\n",
        "### **Neural Machine Translation**\n",
        "\n",
        "*   For neural machine translation model, we used dataset comprised of French phrases and their English counterparts. The dataset is available from the http://www.manythings.org/anki/, with examples drawn from the Tatoeba Project.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1zgHVKVLXsjyD4re6bWE5baV0q71J4yQo\" height = 250 width = 500>\n",
        "\n",
        "* The fra.txt file contains pairs of English to French phrases, one pair per line with a tab separating the language. Total pairs of sentences are 197,463.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1-WO41TvUqFickYyjN298293hUtBSZRUs\" height = 150 width = 600>\n",
        "\n",
        "------------------\n",
        "Preprocessing process of each data is explained in **5. Implementation Details**\n",
        "\n",
        "------------------"
      ],
      "metadata": {
        "id": "8U0pwhGVLr70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.Model Architecture**\n",
        "\n",
        "### 1) Model 1: Convolutional Neural Network (2D-CNN) for Braille Translation\n",
        "a model consisting of three SeperableConv2D layers and max pooling layers.\n",
        "\n",
        "\n",
        "### 2) Model 2: Sequence-to-Sequence model - LSTM & GRU for Neural Machine Translation\n",
        "**Encoder-Decoder based model**\n",
        "다양한 길이의 input/output 을 동일한 길이의 memory에 할당한다.\n",
        "-> 그래서 input 의 길이는 다양해도 상관 없음 (Maps variable-length sequence to fixed-length memory)\n",
        "--------------------\n",
        "* Input Language: English\n",
        "\n",
        "* Output Language: French\n",
        "---------------\n",
        "\n",
        "1. Encoder\n",
        "\n",
        "  It reads the input sequence and summarizes the information in one vector, called context vector (in LSTM, hidden state and cell state vectors). This context vector aims to contain all information for all input elements to help the decoder make accurate predictions. The hidden and cell state of the network is passed along to the decoder as input. \n",
        "\n",
        "2. Decoder\n",
        "\n",
        "  It interprets the context vector obtained from the encoder. The context vecotr of the encoder's final cell is input to the first cell of the decoder network. Using these initial states, the decoder starts generating the output sequence, and these outputs are also considered for futrue predictions. \n",
        "\n",
        "  \n",
        "Our problem is multiclass classification, so we use softmax activation function and cross entropy for the output layer. \n",
        "\n",
        "Our decoder_outputs aren't one-hot encoded, so they are integer labels. Therefore, we use sparse_categorical_crossentropy for loss function to solve the multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1XChI2rJDoa6r8nL7hcBEELICLT-bDK8N\" height = 300 width = 800>\n",
        "\n",
        "----------------------\n",
        "\n",
        "\n",
        "3) Final Architecture\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1PRGd8u2JUUeMloKnRxjW9zLl3WaDxJlu\" height = 400 width = 850>"
      ],
      "metadata": {
        "id": "s9FaYZLN_vex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Implementation Details**"
      ],
      "metadata": {
        "id": "PzhphPvU7WkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part1. Braille Translation Model** \n",
        "* Firstly, we detected simple convnet model using Kaggle Data\n",
        "* Model consisting of three convolutional layers and max pooling layers.\n",
        "* \n"
      ],
      "metadata": {
        "id": "xOVbGzraM_qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "MGwwXGqYKcLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8da3d5c-8b72-4dcf-ac7a-225d068ec1de"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def createFolder(directory):\n",
        "      if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n"
      ],
      "metadata": {
        "id": "aRLQ3k5WKa0I"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from shutil import copyfile\n",
        "\n",
        "dir = '/content/gdrive/MyDrive/AI/teamProject/Data/'\n",
        "createFolder(dir + 'existing_dataset/')\n",
        "alpha = 'a'\n",
        "for i in range(0, 26): \n",
        "    createFolder(dir+ 'existing_dataset/' + alpha)\n",
        "    alpha = chr(ord(alpha) + 1)\n",
        "\n",
        "rootdir = '/content/gdrive/MyDrive/AI/teamProject/Data/brailleset/'\n",
        "\n",
        "for file in os.listdir(rootdir):\n",
        "    letter = file[0]\n",
        "    copyfile(rootdir+file, dir + 'existing_dataset/' + letter + '/' + file) "
      ],
      "metadata": {
        "id": "-GN80Va7KefD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "b23fe6f2-49ba-4885-b689-a07026fc8700"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-aecd73d67fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mletter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'existing_dataset/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                     \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "prev_img_dir = \"/content/gdrive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/existing_dataset/\"\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=20,\n",
        "                             shear_range=10,\n",
        "                             validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(prev_img_dir,\n",
        "                                              target_size=(28,28),\n",
        "                                              subset='training')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(prev_img_dir,\n",
        "                                            target_size=(28,28),\n",
        "                                            subset='validation')"
      ],
      "metadata": {
        "id": "FMN1F5SXKfrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple convnet\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "\n",
        "def Make_simple_model(train,val):\n",
        "  model = models.Sequential()\n",
        "  path = \"/content/gdrive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/\"\n",
        "  model_ckpt = ModelCheckpoint(path+'BrailleNet_simple.h5',save_best_only=True)\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28,28, 3))) \n",
        "  model.add(layers.MaxPooling2D((2, 2))) \n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
        "  model.add(layers.MaxPooling2D((2, 2))) \n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(26, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  history = model.fit_generator(train,\n",
        "                              epochs=30,\n",
        "                              callbacks=[model_ckpt],\n",
        "                              validation_data=val)\n",
        "  return history"
      ],
      "metadata": {
        "id": "U93gTcbuKhfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_simple = Make_simple_model(train_generator,val_generator)"
      ],
      "metadata": {
        "id": "5capelb6Kiy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **First model visualize assessment results**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ra8l_jntKlY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acc_loss(history):\n",
        "    # 평가 결과 도식화\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, loss_ax = plt.subplots(figsize=(10, 5))\n",
        "    acc_ax = loss_ax.twinx()\n",
        "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "    acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
        "    acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    acc_ax.set_ylabel('accuray')\n",
        "    loss_ax.legend(loc='upper left')\n",
        "    acc_ax.legend(loc='lower left')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9MK1WfmVKl48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_loss(history_simple)"
      ],
      "metadata": {
        "id": "OCxFCvpSKnME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **First model accuracy**"
      ],
      "metadata": {
        "id": "BaVeKd_bKqfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    from keras.models import load_model\n",
        "    model = load_model('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/' + model_name)\n",
        "    return model\n",
        "\n",
        "def acc_chk(model, val):\n",
        "    acc = model.evaluate_generator(val)[1]\n",
        "    print('model accuracy: {}'.format(round(acc,4)))"
      ],
      "metadata": {
        "id": "hP9wc_VBKoll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first model accuray\n",
        "from keras.models import Model,load_model\n",
        "model_simple = load_model('BrailleNet_simple.h5')\n",
        "acc = model_simple.evaluate_generator(val_generator)[1]\n",
        "print('model accuracy: {}'.format(round(acc,4)))"
      ],
      "metadata": {
        "id": "o4rcdw5iKs8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.2 Second Model**\n",
        "* Secondly, we detected CNN model with SeperableConv2D using Kaggle Data\n",
        "* Add Features:\n",
        "  * ReduceLROnPlateau : This is the part to improve the learning rate during model learning. If you fall into a local pit before reaching your final goal, adjust the learning rate to get out.\n",
        "  * EarlyStopping : If there is no significant change during learning, get out. Overfitting and underfitting can be properly determined.\n",
        "  * kernel_regularizer: Regularizer to apply a penalty on the layer's kernel using L2\n",
        "  * SperableConv2D\n",
        "\n",
        "\n",
        "###**SeperableConv2D**\n",
        "* Depthwise Conv + Pointwise Conv\n",
        "  \n",
        "  <img src = \"https://drive.google.com/uc?id=159W63eJVfpUz_x98OIJWw75gWdOD-yNj\" height = 150 width = 500>\n",
        "\n",
        "* Comparing MobileNet results above consisting of general convs only with MobileNet below, which adds Depthwise Separable Convolution Layer, the accuracy is reduced, but Multi-Adds and parameters are reduced by about 88.3% and 85.7%.\n"
      ],
      "metadata": {
        "id": "y5z4PbtiKvBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Seperable con \n",
        "\n",
        "from keras import backend as K\n",
        "from keras import layers as L\n",
        "from keras.models import Model,load_model\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "def Make_second_model(train,val):\n",
        "  K.clear_session()\n",
        "  reduce_lr = ReduceLROnPlateau(patience=8,verbose=0)\n",
        "  path = \"/content/gdrive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/\"\n",
        "  model_ckpt = ModelCheckpoint(path+'BrailleNet_second.h5',save_best_only=True)\n",
        "  early_stop = EarlyStopping(patience=15,verbose=1)\n",
        "  entry = L.Input(shape=(28, 28 ,3))\n",
        "  x = L.SeparableConv2D(64,(3,3),activation='relu')(entry)\n",
        "  x = L.MaxPooling2D((2,2))(x)\n",
        "  x = L.SeparableConv2D(128,(3,3),activation='relu')(x)\n",
        "  x = L.MaxPooling2D((2,2))(x)\n",
        "  x = L.SeparableConv2D(256,(2,2),activation='relu')(x)\n",
        "  x = L.GlobalMaxPooling2D()(x)\n",
        "  x = L.Dense(256)(x)\n",
        "  x = L.LeakyReLU()(x)\n",
        "  x = L.Dense(64,kernel_regularizer=l2(2e-4))(x)\n",
        "  x = L.LeakyReLU()(x)\n",
        "  x = L.Dense(26,activation='softmax')(x)\n",
        "  \n",
        "  model = Model(entry,x)\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  history = model.fit_generator(train_generator,validation_data=val_generator,epochs=30,\n",
        "                              callbacks=[model_ckpt,reduce_lr,early_stop],verbose=0)\n",
        "  return history"
      ],
      "metadata": {
        "id": "nZZK4jdbKtOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_second = Make_second_model(train_generator,val_generator)"
      ],
      "metadata": {
        "id": "PlU0un_yK38N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Second model visualize assessment results**"
      ],
      "metadata": {
        "id": "X_Qyo-WVK5bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_loss(history_second)"
      ],
      "metadata": {
        "id": "QmY9jYmlK5_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.3 Third Model**\n",
        "* Thirdly, we detected CNN model by referring to the contents of the paper.\n",
        "<img src = \"https://drive.google.com/uc?id=1TCEppgk86gHO_0N7sgcfUQpKxmsQyw9i\" height = 300 width = 500>\n",
        "* There are three ways to improve the performance of the CNN model:\n",
        "  * Increase channels\n",
        "  * Increase the layer\n",
        "  * Increase original resolution\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1yCQONsNp3YvSMbEPiKnZdmEKqrUYrlWg\" height = 300 width = 500>\n",
        "\n",
        "* In the same paper, show a higher performance growth rate when the resolution of the input image is high even in the same model.\n",
        "\n",
        "* ### **In CNN, the size of the filter is very important. The accuracy depends on how you use the filter.**\n",
        "\n",
        "```\n",
        " entry = L.Input(shape=(36,36,3))\n",
        "    x = L.SeparableConv2D(64,(3,3),activation='relu',padding ='same')(entry)\n",
        "    x = L.MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = L.SeparableConv2D(128,(3,3),activation='relu',padding ='same')(x)\n",
        "    x = L.MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = L.SeparableConv2D(256,(2,2),activation='relu',padding ='same')(x)\n",
        "    x = L.GlobalMaxPooling2D()(x)\n",
        "```\n",
        "* Using Conv2D, we find it using 64 kernels and its strid is 3*3.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1k92Ls5MWK3lOei96U2It9GLGE4OlxZ98\" height = 300 width = 200>\n",
        "\n",
        "* When increasing the resolution, it is better to increase the stride value as well.\n",
        "\n",
        "* ### **Changed the resolution of the image to 50 * 50 and the filter size to 10 * 10**\n",
        "\n",
        "```\n",
        "    entry = L.Input(shape=(50,50,3))\n",
        "    x = L.SeparableConv2D(128, (10,10), activation='relu', padding='same')(entry)\n",
        "    x = L.MaxPooling2D((2, 2))(x)\n",
        "    x = L.SeparableConv2D(256,(10,10),activation='relu',padding ='same')(x)\n",
        "    x = L.MaxPooling2D((2,2))(x)\n",
        "```\n",
        "\n",
        "* This is the approximate size of the image and the size of the filter\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1RHLRBWx7ULvnQh7fCiQmPvxsj2c17Y4K\" height = 200 width = 150>"
      ],
      "metadata": {
        "id": "CQ7QWgT2K-hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "def Make_model(train, val):\n",
        "    K.clear_session()\n",
        "\n",
        "    model_ckpt = ModelCheckpoint('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/BrailleNet_third.h5',save_best_only=True)\n",
        "    reduce_lr = ReduceLROnPlateau(patience=8,verbose=1)\n",
        "    early_stop = EarlyStopping(patience=5,verbose=2,monitor='accuracy')\n",
        "\n",
        "    entry = L.Input(shape=(50,50,3))\n",
        "    x = L.SeparableConv2D(128, (10,10), activation='relu', padding='same')(entry)\n",
        "    x = L.MaxPooling2D((2, 2))(x)\n",
        "    x = L.SeparableConv2D(256,(10,10),activation='relu',padding ='same')(x)\n",
        "    x = L.MaxPooling2D((2,2))(x)\n",
        "    x = L.SeparableConv2D(512,(10,10),activation='relu',padding ='same')(x)\n",
        "    x = L.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    x = L.Dense(512)(x)\n",
        "    x = L.LeakyReLU()(x)\n",
        "    x = L.Dense(256)(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.Dense(128,kernel_regularizer=l2(2e-4))(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.Dense(32,activation='softmax')(x)\n",
        "\n",
        "    model = Model(entry,x)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit_generator(train,validation_data=val,epochs=60,\n",
        "                                  callbacks=[model_ckpt,reduce_lr,early_stop],verbose=1)\n",
        "    \n",
        "    return history\n"
      ],
      "metadata": {
        "id": "YlyWXn1GK_jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_third = Make_model(train_generator,val_generator)"
      ],
      "metadata": {
        "id": "pznpIp1dLBvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Third model visualize assessment results**"
      ],
      "metadata": {
        "id": "-Kvo9qjULDyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_loss(history_third)"
      ],
      "metadata": {
        "id": "tMyB720LLEHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Third model accuracy**"
      ],
      "metadata": {
        "id": "vxchCYjeLGQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model,load_model\n",
        "model_second = load_model('BrailleNet_third.h5')\n",
        "acc = model_second.evaluate_generator(val_generator)[1]\n",
        "print('model accuracy: {}'.format(round(acc,4)))"
      ],
      "metadata": {
        "id": "jkJXa57WLGAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.4 Final braille Model**"
      ],
      "metadata": {
        "id": "GumQ7SdwLJTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Data Augmentation"
      ],
      "metadata": {
        "id": "MfdoHLnyLNX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation'\n",
        "\n",
        "def createFolder(directory):\n",
        "      if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "createFolder(file_path+'/new_dataset')"
      ],
      "metadata": {
        "id": "9gaAGLgsLI0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.05,\n",
        "        zoom_range=0.01,\n",
        "        fill_mode='constant',\n",
        "        cval=255\n",
        "        )\n",
        "\n",
        "def newGenerateData():\n",
        "  files = os.listdir(file_path + '/dataset')\n",
        "  for file in files:\n",
        "    if(os.path.splitext(file)[1] == '.png'):\n",
        "      filename = os.path.splitext(file)[0]\n",
        "      createFolder(file_path + '/new_dataset/'+ filename)\n",
        "      img = load_img(file_path+'/dataset/'+file)  # PIL 이미지\n",
        "      x = img_to_array(img)\n",
        "      x = x.reshape((1,) + x.shape)\n",
        "      if file == 'space.png':\n",
        "        import shutil\n",
        "        for _ in range(10):\n",
        "          print(_,file, file_path +'/dataset/'+file)\n",
        "          shutil.copy(file_path+'/dataset/'+file, file_path + '/new_dataset/'+ filename + '/' + filename + str(_) + '.jpg')\n",
        "      else:\n",
        "        i = 0\n",
        "        for batch in datagen.flow(x, batch_size=1, save_to_dir= file_path + '/new_dataset/'+ filename, save_prefix=filename, save_format='jpg'):\n",
        "          i += 1\n",
        "          if i > 20:\n",
        "            break  # 이미지 20장을 생성하고 마칩니다\n",
        "\n",
        "newGenerateData()\n"
      ],
      "metadata": {
        "id": "WD53ExaPLP4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Generator에 넣어 학습 준비"
      ],
      "metadata": {
        "id": "zA_wjhG_LR7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def data_ready(input_size):\n",
        "    images_dir = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/new_dataset'\n",
        "\n",
        "    datagen = ImageDataGenerator(rotation_range=5,\n",
        "                                 shear_range=5,\n",
        "                                 validation_split=0.2,\n",
        "                                 ) #20%를 검증모델로 사용.\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(images_dir,\n",
        "                                                  target_size=(input_size,input_size),\n",
        "                                                  subset='training')\n",
        "\n",
        "    val_generator = datagen.flow_from_directory(images_dir,\n",
        "                                                target_size=(input_size,input_size),\n",
        "                                                subset='validation')\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "def load_image(img_path, input_size):\n",
        "    images_dir = img_path\n",
        "    datagen = ImageDataGenerator()\n",
        "    real_generator = datagen.flow_from_directory(images_dir,\n",
        "                                                 target_size=(input_size, input_size))\n",
        "\n",
        "    return real_generator"
      ],
      "metadata": {
        "id": "yBlz_HSWLSON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) 모델 생성 "
      ],
      "metadata": {
        "id": "Opb3E9igLY_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "def Make_model(train, val, input_size):\n",
        "    K.clear_session()\n",
        "\n",
        "    model_ckpt = ModelCheckpoint('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/BrailleNet.h5',save_best_only=True)\n",
        "    reduce_lr = ReduceLROnPlateau(patience=8,verbose=1)\n",
        "    early_stop = EarlyStopping(patience=5,verbose=2,monitor='accuracy')\n",
        "\n",
        "    entry = L.Input(shape=(input_size,input_size,3))\n",
        "    x = L.SeparableConv2D(128, (10,10), activation='relu', padding='same')(entry)\n",
        "    x = L.MaxPooling2D((2, 2))(x)\n",
        "    x = L.SeparableConv2D(256,(10,10),activation='relu',padding ='same')(x)\n",
        "    x = L.MaxPooling2D((2,2))(x)\n",
        "    x = L.SeparableConv2D(512,(10,10),activation='relu',padding ='same')(x)\n",
        "    x = L.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    x = L.Dense(512)(x)\n",
        "    x = L.LeakyReLU()(x)\n",
        "    x = L.Dense(256)(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.Dense(128,kernel_regularizer=l2(2e-4))(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.Dense(32,activation='softmax')(x)\n",
        "\n",
        "    model = Model(entry,x)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit_generator(train,validation_data=val,epochs=60,\n",
        "                                  callbacks=[model_ckpt,reduce_lr,early_stop],verbose=1)\n",
        "    \n",
        "    return history\n"
      ],
      "metadata": {
        "id": "LOMRWsJVLZV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) 모델 결과 도식화"
      ],
      "metadata": {
        "id": "VHDm-l_7Lbjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acc_loss(history):\n",
        "    # 평가 결과 도식화\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, loss_ax = plt.subplots(figsize=(10, 5))\n",
        "    acc_ax = loss_ax.twinx()\n",
        "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "    acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
        "    acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    acc_ax.set_ylabel('accuray')\n",
        "    loss_ax.legend(loc='upper left')\n",
        "    acc_ax.legend(loc='lower left')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X0yS1cHGLb4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(5) 모델 불러오기 , 정확도 확인"
      ],
      "metadata": {
        "id": "hxo8JdV4LdnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    from keras.models import load_model\n",
        "    model = load_model('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/' + model_name)\n",
        "    return model\n",
        "\n",
        "def acc_chk(model, val):\n",
        "    acc = model.evaluate_generator(val)[1]\n",
        "    print('model accuracy: {}'.format(round(acc,4)))"
      ],
      "metadata": {
        "id": "S1-F_DAFLfdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(6) 학습 시작"
      ],
      "metadata": {
        "id": "oKyh882ZLhIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 36\n",
        "input_size_prev = 28\n",
        "train_generator_prev, val_genrator = data_ready(input_size_prev)\n",
        "train_generator, val_generator = data_ready(input_size)\n"
      ],
      "metadata": {
        "id": "0xL12KLELjJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = Make_model(train_generator,val_generator)"
      ],
      "metadata": {
        "id": "OuzUVnvhLmh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_loss(hist)"
      ],
      "metadata": {
        "id": "FJfa69TsLnxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('BrailleNet.h5')\n",
        "acc_chk(model, val_generator)"
      ],
      "metadata": {
        "id": "qBECH421LpXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.5 Test using final braille model**"
      ],
      "metadata": {
        "id": "pDyfh9XHLr2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as Img\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_test_img(path):\n",
        "  img = Img.open(path).convert('RGB')\n",
        "  img = img.resize((36,36))\n",
        "  x = img_to_array(img)\n",
        "  x = x.reshape((1,) + x.shape)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "VQvSECBbLpy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import update_wrapper\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "def decode_predict(result,labels):\n",
        "  max = np.max(result)\n",
        "  index = np.where(result == max)\n",
        "  return labels[index[1][0]]\n",
        "\n",
        "def translate(test_image):\n",
        "  dir = os.listdir('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/dataset')\n",
        "  tmp = []\n",
        "  for file in dir:\n",
        "    if(os.path.splitext(file)[1] == '.png'):\n",
        "      tmp.append(file.split('.')[0])\n",
        "  dir = tmp\n",
        "  dir.sort()\n",
        "  print(dir)\n",
        "  model = load_model('BrailleNet.h5')\n",
        "  result = model.predict(test_image)\n",
        "  decoded = decode_predict(result,dir)\n",
        "  return(decoded)\n",
        "\n",
        "def convert_test_img(input_img):\n",
        "  img = input_img.convert('RGB')\n",
        "  img = img.resize((36,36))\n",
        "  x = img_to_array(img)\n",
        "  x = x.reshape((1,) + x.shape)\n",
        "  return x"
      ],
      "metadata": {
        "id": "PmFti85ELsWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Translate Character"
      ],
      "metadata": {
        "id": "uo4PKNuqLu_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as Img\n",
        "test_file = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/testset/test_q.png'\n",
        "input = get_test_img(test_file)\n",
        "print(translate(input))"
      ],
      "metadata": {
        "id": "2xd1GgB7LtyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(2) Making test case for sentence"
      ],
      "metadata": {
        "id": "LFMZat0kLyd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make test datset\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def make_test(word):\n",
        "  path = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/dataset'\n",
        "  store_path = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/testset'\n",
        "  result_img = np.zeros((36,36,3), np.uint8)\n",
        "  for ch in word:\n",
        "    if ch == ' ':\n",
        "      img = cv2.imread(path+'/space.png')\n",
        "      img = cv2.resize(img,(36,36))\n",
        "      result_img = cv2.hconcat([result_img,img])\n",
        "    elif ch == '!':\n",
        "      img = cv2.imread(path+'/exclamation_point.png')\n",
        "      img = cv2.resize(img,(36,36))\n",
        "      result_img = cv2.hconcat([result_img,img])\n",
        "    elif ch == ',':\n",
        "      img = cv2.imread(path+'/comma.png')\n",
        "      img = cv2.resize(img,(36,36))\n",
        "      result_img = cv2.hconcat([result_img,img])\n",
        "    elif ch == '.':\n",
        "      img = cv2.imread(path+'/period.png')\n",
        "      img = cv2.resize(img,(36,36))\n",
        "      result_img = cv2.hconcat([result_img,img])\n",
        "    elif ch == '?':\n",
        "      img = cv2.imread(path+'/question_mark.png')\n",
        "      img = cv2.resize(img,(36,36))\n",
        "      result_img = cv2.hconcat([result_img,img])\n",
        "    else:\n",
        "      if word.find(ch) == 0:\n",
        "        if ch.isupper():\n",
        "          result_img = cv2.imread(path+'/upper.png')\n",
        "          result_img = cv2.resize(result_img,(36,36))\n",
        "          ch = ch.lower()\n",
        "          img = cv2.imread(path+'/'+ ch +'.png')\n",
        "          img = cv2.resize(img,(36,36))\n",
        "          result_img = cv2.hconcat([result_img,img])\n",
        "        else:\n",
        "          result_img = cv2.imread(path+'/'+ ch +'.png')\n",
        "          result_img = cv2.resize(result_img,(36,36))\n",
        "      else:\n",
        "        if ch.isupper():\n",
        "          print(ch)\n",
        "          img = cv2.imread(path+'/upper.png')\n",
        "          img = cv2.resize(img,(36,36))\n",
        "          result_img = cv2.hconcat([result_img,img])\n",
        "          ch = ch.lower()\n",
        "          img = cv2.imread(path+'/'+ ch +'.png')\n",
        "          img = cv2.resize(img,(36,36))\n",
        "          result_img = cv2.hconcat([result_img,img])\n",
        "        else:\n",
        "          img = cv2.imread(path+'/'+ ch +'.png')\n",
        "          img = cv2.resize(img,(36,36))\n",
        "          result_img = cv2.hconcat([result_img,img])\n",
        "  cv2.imwrite(store_path+'/'+word+'.png', result_img)\n",
        "make_test('I am a student')"
      ],
      "metadata": {
        "id": "NTBZ8-iuLySu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Translate Sentence"
      ],
      "metadata": {
        "id": "zKEnAxcCL3B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def devide_img(fileName):\n",
        "  path = '/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/testset/'\n",
        "  dir = os.listdir('/content/drive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/dataset')\n",
        "  tmp = []\n",
        "  for file in dir:\n",
        "    if(os.path.splitext(file)[1] == '.png'):\n",
        "      tmp.append(file.split('.')[0])\n",
        "      dir = tmp\n",
        "      dir.sort()\n",
        "  img = Image.open(path+fileName)\n",
        "  (width,height) = img.size\n",
        "  print(width, height)\n",
        "  iterate_num = width // height\n",
        "  iterate_num = int(iterate_num)\n",
        "  start_point = 0\n",
        "  size = height\n",
        "  end_point = height\n",
        "  text = ''\n",
        "\n",
        "  special_character_dir = {'space': ' ', 'exclamation_point': '!', 'comma': ',', 'period':'.', \n",
        "                           'question_mark': '?'}\n",
        "  keyList = special_character_dir.keys()\n",
        "  translated_sentence = ''\n",
        "  isUpper = False\n",
        "  for i in range(0, iterate_num):\n",
        "    area = (start_point, 0, end_point, height)\n",
        "    cropped_img = img.crop(area)\n",
        "    result = translate(convert_test_img(cropped_img))\n",
        "    start_point = end_point\n",
        "    end_point += size\n",
        "    if result == 'upper':\n",
        "      isUpper = True\n",
        "      continue\n",
        "    if isUpper == True:\n",
        "      result = result.upper()\n",
        "      isUpper = False\n",
        "    if result in keyList:\n",
        "      result = special_character_dir[result]\n",
        "    print(result)\n",
        "    translated_sentence += result\n",
        "\n",
        "  return translated_sentence\n",
        "result = devide_img('Welcome to AI class!.png')"
      ],
      "metadata": {
        "id": "v_itjl3nL3S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "6mMYTGqXL6Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Second model accuracy**"
      ],
      "metadata": {
        "id": "Hwjdgx0kK7f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model,load_model\n",
        "path = \"/content/gdrive/MyDrive/AI_TeamProject/AI-teamProject-brailleTranslation/\"\n",
        "model_second = load_model(path + 'BrailleNet_second.h5')\n",
        "acc = model_second.evaluate_generator(val_generator)[1]\n",
        "print('model accuracy: {}'.format(round(acc,4)))"
      ],
      "metadata": {
        "id": "l5ynFrK2K8Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Part2. NMT model (English -> French)**\n"
      ],
      "metadata": {
        "id": "sqeK8VzxOlmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZ0kYVb8ioM",
        "outputId": "ac5dbb1b-0526-4ae4-d1d5-9d6d93a22c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezoPv2cgKYz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "gPZO7fF-huhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**\n",
        "\n",
        "* Parallel corpus data \n",
        "\n",
        "- source: English\n",
        "\n",
        "- target: French\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-5Aa2bshpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 0\n",
        "with open(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", \"r\", encoding='UTF-8') as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "            length+=1\n",
        "print(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Y3A2UKmsSu",
        "outputId": "676668bb-ff77-4a33-98f4-41de38606f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", delimiter = \"\\t\")\n",
        "data.columns = [\"en\", \"fr\", \"cc\"]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LLk6wspjpG2Q",
        "outputId": "cb78d772-ff65-4fb6-997a-37780545673a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       en  \\\n",
              "0                                                     Go.   \n",
              "1                                                     Go.   \n",
              "2                                                     Go.   \n",
              "3                                                     Hi.   \n",
              "4                                                     Hi.   \n",
              "...                                                   ...   \n",
              "197457  A carbon footprint is the amount of carbon dio...   \n",
              "197458  Death is something that we're often discourage...   \n",
              "197459  Since there are usually multiple websites on a...   \n",
              "197460  If someone who doesn't know your background sa...   \n",
              "197461  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                       fr  \\\n",
              "0                                                 Marche.   \n",
              "1                                              En route !   \n",
              "2                                                 Bouge !   \n",
              "3                                                 Salut !   \n",
              "4                                                  Salut.   \n",
              "...                                                   ...   \n",
              "197457  Une empreinte carbone est la somme de pollutio...   \n",
              "197458  La mort est une chose qu'on nous décourage sou...   \n",
              "197459  Puisqu'il y a de multiples sites web sur chaqu...   \n",
              "197460  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
              "197461  Il est peut-être impossible d'obtenir un Corpu...   \n",
              "\n",
              "                                                       cc  \n",
              "0       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "1       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "2       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "3       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "4       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "...                                                   ...  \n",
              "197457  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "197458  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "197459  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "197460  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "197461  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "\n",
              "[197462 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9547374-f44d-41a3-9d13-04db66a3a039\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>cc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>En route !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197457</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197458</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197459</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197460</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197461</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197462 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9547374-f44d-41a3-9d13-04db66a3a039')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d9547374-f44d-41a3-9d13-04db66a3a039 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d9547374-f44d-41a3-9d13-04db66a3a039');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "JE0m29pUlf4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "    # delete accent in French\n",
        "    # EX) : 'déjà diné' -> deja dine\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "    # Call accent deletion function\n",
        "    sent = to_ascii(sent.lower())\n",
        "    \n",
        "    # Add blank between word and punctuation \n",
        "    # ex) \"I am a student.\" => \"I am a student .\"\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "    \n",
        "    # Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), transform others to blank\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    \n",
        "    # Transform multiple blanks to one blank\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    \n",
        "    return sent"
      ],
      "metadata": {
        "id": "x0WkR49X9Qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Test\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('Before preprocessing English sentence :', en_sent)\n",
        "print('After preprocessing English sentence :',preprocess_sentence(en_sent))\n",
        "print('Before preprocessing French sentence :', fr_sent)\n",
        "print('After preprocessing French sentence :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJdUEWO39Sfq",
        "outputId": "37eb8207-098f-4074-f519-4ab69d13f91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing English sentence : Have you had dinner?\n",
            "After preprocessing English sentence : have you had dinner ?\n",
            "Before preprocessing French sentence : Avez-vous déjà diné?\n",
            "After preprocessing French sentence : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "    src_seqlen = 0\n",
        "    tar_seqlen = 0\n",
        "    with open(\"./gdrive/MyDrive/AI/teamProject/fra.txt\", \"r\", encoding='UTF-8') as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "            # split source data and target data \n",
        "            src_line, tar_line, _ = line.strip().split('\\t')\n",
        "            \n",
        "            # preprocess source data\n",
        "            preprocessed_sen = preprocess_sentence(src_line)\n",
        "            src_line = [w for w in preprocessed_sen.split()]\n",
        "            for w in preprocessed_sen.split():\n",
        "              if len(w) > src_seqlen:\n",
        "                src_seqlen = len(w)\n",
        "            \n",
        "            # preprocess target data -> we'll use teacher forcing in training phase, so divide decoder's input sequence and original value (label)\n",
        "            tar_line = preprocess_sentence(tar_line)\n",
        "            for w in tar_line.split():\n",
        "              if len(w) > tar_seqlen:\n",
        "                tar_seqlen = len(w)\n",
        "            #  For target sequence (French) adding <sos> meaning start and <eos> meaning end.  \n",
        "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "            \n",
        "            encoder_input.append(src_line)\n",
        "            decoder_input.append(tar_line_in)\n",
        "            decoder_target.append(tar_line_out)\n",
        "            \n",
        "            if i == num_samples - 1:\n",
        "                break\n",
        "        \n",
        "    return encoder_input, decoder_input, decoder_target, src_seqlen, tar_seqlen"
      ],
      "metadata": {
        "id": "SU0FM-yw9T_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 70000 # we use 70000 among about 190000 data"
      ],
      "metadata": {
        "id": "Kx4H1PvY9sdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 samples\n",
        "sents_en_in, sents_fra_in, sents_fra_out, src_seqlen, tar_seqlen = load_preprocessed_data()\n",
        "print('Input of Encoder :',sents_en_in[:5])\n",
        "print('Input of Decoder :',sents_fra_in[:5])\n",
        "print('Label of Decoder :',sents_fra_out[:5])\n",
        "print(src_seqlen)\n",
        "print(tar_seqlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEOxVdso9S4m",
        "outputId": "1cf82dc8-405c-4e87-e418-a2a7e4cb8386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input of Encoder : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "Input of Decoder : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "Label of Decoder : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n",
            "16\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teacher Forcing\n",
        "\n",
        "* Technique where the target word(ground truth) is passed as the next input to the decoder \n",
        "\n",
        "\n",
        "### Why it is needed?\n",
        "\n",
        "* The problem with training recurrent neural networks that use output from prior time steps as input.\n",
        "\n",
        "-> There is a risk that every single part part fails even though only one mistake in the first one. (previous decoder cell's prediction is wrong, this affects entire prediction wrong.) \n",
        "\n",
        "-> Analogy : a teacher records the score for each individual part and then tells the student the correct answre, to be used in the next part. (giving real value instead of predicted previous value.)"
      ],
      "metadata": {
        "id": "Sgmo_DqhnECX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate set of word with keras tokenizer and do integer encoding and padding \n",
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "vR3hqwy79lSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of each data\n",
        "print('Input of Encoder Shape :',encoder_input.shape)\n",
        "print('Input of Decoder Shape :',decoder_input.shape)\n",
        "print('Label of Decoder Shape :',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lErUV8Rn9xo1",
        "outputId": "92f84187-3235-438f-d987-0213bfc512a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input of Encoder Shape : (70000, 9)\n",
            "Input of Decoder Shape : (70000, 17)\n",
            "Label of Decoder Shape : (70000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of word set \n",
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"Size of English word set : {:d}, Size of French word set : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg6RgZNr9z3W",
        "outputId": "7b5c3e83-06a7-4233-b162-603d2d338039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of English word set : 7241, Size of French word set : 12336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "484gZQKC903s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before spliting dataset, randomly mixing integer sequence list. \n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('Random Sequence :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT6KNypQ91_H",
        "outputId": "532f976e-2ef6-4867-b868-72bd85bcda6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Sequence : [63791 47890 40039 ... 64159 29344   844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set it to dataset's order -> samples are mixed unlike original order \n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "Tx2Ty3Td93G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print randomly selected sample. \n",
        "# Here, decoder_input and decoder_target should have same integer sequence except <sos> and <eos> token.\n",
        "print(encoder_input[40111])\n",
        "print(decoder_input[40111])\n",
        "print(decoder_target[40111])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLFLtof494In",
        "outputId": "c3aafc06-6f59-4303-bb12-d4307fef161c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[129  16 185  44   4   0   0   0   0]\n",
            "[  2 528  12  68   5   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[528  12  68   5   3   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of validation dataset (10%) \n",
        "n_of_val = int(num_samples*0.1)\n",
        "print('Number of validation data :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQnAwAoP94_z",
        "outputId": "e36dddea-53f5-4226-ccd4-817d144cbb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation data : 7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide train (90%) and test data (10%)\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "CEdPf9RJ96Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of traning and test data\n",
        "print('Shape of training source data :',encoder_input_train.shape)\n",
        "print('Shape of training target data :',decoder_input_train.shape)\n",
        "print('Shape of trainig target label data :',decoder_target_train.shape)\n",
        "print('Shape of test source data :',encoder_input_test.shape)\n",
        "print('Shape of test target data :',decoder_input_test.shape)\n",
        "print('Shape of test target label data :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfYr08J397LQ",
        "outputId": "0e499338-d4af-4b69-e6f2-04c8c80ba066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training source data : (63000, 9)\n",
            "Shape of training target data : (63000, 17)\n",
            "Shape of trainig target label data : (63000, 17)\n",
            "Shape of test source data : (7000, 9)\n",
            "Shape of test target data : (7000, 17)\n",
            "Shape of test target label data : (7000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing LSTM and GRU Model**\n",
        "\n",
        "We randomly select hyperparamter, and this same hyperparamter is applied to comparing LSTM and GRU model."
      ],
      "metadata": {
        "id": "IEyuRpVpvO9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking, GRU, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aZaDG5UMvbDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same hyperparameter for LSTM and GRU model\n",
        "hidden_units = 64\n",
        "embedding_dim = 64\n",
        "batchsize = 128\n",
        "encoder_dropout = 0.2\n",
        "decoder_dropout = 0.2"
      ],
      "metadata": {
        "id": "K6fdRm96vqZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # Embedding layer\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # Exclude padding 0 in operation\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True, dropout=encoder_dropout) # To return state value\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # Return hidden state and cell state\n",
        "encoder_states = [state_h, state_c] # Save encoder's hidden state and cell state -> to decoder!\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # Embedding layer\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # exclude padding 0 in operation\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# To return state value, return_state = True\n",
        "# To predict word for every time step, return_sequences = True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=decoder_dropout) \n",
        "\n",
        "# Use encoder's hidden state as initial hidden state \n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                    initial_state=encoder_states)\n",
        "\n",
        "# predict word bsaed on softmax activation function for all results from every time step\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model's input and output \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "        validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "        batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 10)], epochs=50) # for testing, we use epochs = 10 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkjDAHDbvkbp",
        "outputId": "cddb1156-d82f-438d-aea8-48252f4d1cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "493/493 [==============================] - 28s 42ms/step - loss: 2.8225 - acc: 0.6187 - val_loss: 1.9384 - val_acc: 0.7034\n",
            "Epoch 2/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.7906 - acc: 0.7225 - val_loss: 1.6807 - val_acc: 0.7352\n",
            "Epoch 3/50\n",
            "493/493 [==============================] - 18s 37ms/step - loss: 1.5871 - acc: 0.7432 - val_loss: 1.5165 - val_acc: 0.7566\n",
            "Epoch 4/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.4284 - acc: 0.7692 - val_loss: 1.3811 - val_acc: 0.7784\n",
            "Epoch 5/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.3087 - acc: 0.7878 - val_loss: 1.2893 - val_acc: 0.7924\n",
            "Epoch 6/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.2202 - acc: 0.7993 - val_loss: 1.2204 - val_acc: 0.8014\n",
            "Epoch 7/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.1526 - acc: 0.8067 - val_loss: 1.1673 - val_acc: 0.8073\n",
            "Epoch 8/50\n",
            "493/493 [==============================] - 20s 40ms/step - loss: 1.0946 - acc: 0.8125 - val_loss: 1.1201 - val_acc: 0.8117\n",
            "Epoch 9/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 1.0420 - acc: 0.8179 - val_loss: 1.0770 - val_acc: 0.8167\n",
            "Epoch 10/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.9943 - acc: 0.8233 - val_loss: 1.0381 - val_acc: 0.8222\n",
            "Epoch 11/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.9503 - acc: 0.8279 - val_loss: 1.0060 - val_acc: 0.8246\n",
            "Epoch 12/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.9109 - acc: 0.8321 - val_loss: 0.9723 - val_acc: 0.8296\n",
            "Epoch 13/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.8745 - acc: 0.8360 - val_loss: 0.9448 - val_acc: 0.8320\n",
            "Epoch 14/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.8411 - acc: 0.8396 - val_loss: 0.9202 - val_acc: 0.8353\n",
            "Epoch 15/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.8099 - acc: 0.8428 - val_loss: 0.8983 - val_acc: 0.8373\n",
            "Epoch 16/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.7815 - acc: 0.8459 - val_loss: 0.8784 - val_acc: 0.8395\n",
            "Epoch 17/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.7556 - acc: 0.8488 - val_loss: 0.8593 - val_acc: 0.8427\n",
            "Epoch 18/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.7309 - acc: 0.8514 - val_loss: 0.8425 - val_acc: 0.8447\n",
            "Epoch 19/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.7085 - acc: 0.8539 - val_loss: 0.8276 - val_acc: 0.8468\n",
            "Epoch 20/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.6871 - acc: 0.8564 - val_loss: 0.8142 - val_acc: 0.8487\n",
            "Epoch 21/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.6670 - acc: 0.8587 - val_loss: 0.8013 - val_acc: 0.8505\n",
            "Epoch 22/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.6484 - acc: 0.8609 - val_loss: 0.7890 - val_acc: 0.8520\n",
            "Epoch 23/50\n",
            "493/493 [==============================] - 20s 40ms/step - loss: 0.6312 - acc: 0.8632 - val_loss: 0.7785 - val_acc: 0.8544\n",
            "Epoch 24/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.6146 - acc: 0.8653 - val_loss: 0.7684 - val_acc: 0.8552\n",
            "Epoch 25/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5992 - acc: 0.8674 - val_loss: 0.7589 - val_acc: 0.8573\n",
            "Epoch 26/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5838 - acc: 0.8695 - val_loss: 0.7504 - val_acc: 0.8577\n",
            "Epoch 27/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5701 - acc: 0.8713 - val_loss: 0.7425 - val_acc: 0.8596\n",
            "Epoch 28/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5572 - acc: 0.8730 - val_loss: 0.7348 - val_acc: 0.8603\n",
            "Epoch 29/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5445 - acc: 0.8748 - val_loss: 0.7293 - val_acc: 0.8615\n",
            "Epoch 30/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5332 - acc: 0.8768 - val_loss: 0.7220 - val_acc: 0.8624\n",
            "Epoch 31/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5222 - acc: 0.8785 - val_loss: 0.7148 - val_acc: 0.8645\n",
            "Epoch 32/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5112 - acc: 0.8800 - val_loss: 0.7102 - val_acc: 0.8648\n",
            "Epoch 33/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.5015 - acc: 0.8819 - val_loss: 0.7046 - val_acc: 0.8659\n",
            "Epoch 34/50\n",
            "493/493 [==============================] - 21s 43ms/step - loss: 0.4919 - acc: 0.8832 - val_loss: 0.6989 - val_acc: 0.8666\n",
            "Epoch 35/50\n",
            "493/493 [==============================] - 20s 41ms/step - loss: 0.4828 - acc: 0.8848 - val_loss: 0.6946 - val_acc: 0.8671\n",
            "Epoch 36/50\n",
            "493/493 [==============================] - 20s 41ms/step - loss: 0.4738 - acc: 0.8863 - val_loss: 0.6889 - val_acc: 0.8682\n",
            "Epoch 37/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4657 - acc: 0.8875 - val_loss: 0.6865 - val_acc: 0.8682\n",
            "Epoch 38/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4578 - acc: 0.8889 - val_loss: 0.6807 - val_acc: 0.8696\n",
            "Epoch 39/50\n",
            "493/493 [==============================] - 20s 40ms/step - loss: 0.4505 - acc: 0.8900 - val_loss: 0.6769 - val_acc: 0.8699\n",
            "Epoch 40/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4441 - acc: 0.8912 - val_loss: 0.6743 - val_acc: 0.8711\n",
            "Epoch 41/50\n",
            "493/493 [==============================] - 19s 39ms/step - loss: 0.4370 - acc: 0.8924 - val_loss: 0.6709 - val_acc: 0.8712\n",
            "Epoch 42/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4302 - acc: 0.8938 - val_loss: 0.6678 - val_acc: 0.8725\n",
            "Epoch 43/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4242 - acc: 0.8948 - val_loss: 0.6655 - val_acc: 0.8729\n",
            "Epoch 44/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4179 - acc: 0.8960 - val_loss: 0.6629 - val_acc: 0.8733\n",
            "Epoch 45/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4128 - acc: 0.8969 - val_loss: 0.6600 - val_acc: 0.8738\n",
            "Epoch 46/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.4073 - acc: 0.8979 - val_loss: 0.6586 - val_acc: 0.8742\n",
            "Epoch 47/50\n",
            "493/493 [==============================] - 20s 40ms/step - loss: 0.4020 - acc: 0.8988 - val_loss: 0.6558 - val_acc: 0.8749\n",
            "Epoch 48/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.3968 - acc: 0.8998 - val_loss: 0.6546 - val_acc: 0.8748\n",
            "Epoch 49/50\n",
            "493/493 [==============================] - 19s 39ms/step - loss: 0.3927 - acc: 0.9002 - val_loss: 0.6520 - val_acc: 0.8754\n",
            "Epoch 50/50\n",
            "493/493 [==============================] - 19s 38ms/step - loss: 0.3880 - acc: 0.9011 - val_loss: 0.6516 - val_acc: 0.8758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.legend(labels=[\"loss\", \"val_loss\"])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "3PBy3bDhwnB5",
        "outputId": "ae6b190e-b34a-4efa-88b9-a04ed601c445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation Metrics**\n",
        "\n",
        "In NMT, there are diverse evaluation metrics from human evaluation methods to automatic evaluation metrics [1]. In previous diverse neural machine traslation tasks [2] [3], the mostly used evelaution metrics are BLEU score. Therefore, among diverse evlauation metrics, we use BLEU score and ROUGE score for our model's evaluation. \n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1JuvHUO16HlSU7N6B5zSPDOM3BVAKsXuL\" height = 300 width = 400>\n",
        "\n",
        "\n",
        "**BLEU Score [4] (n-gram precision)**\n",
        "\n",
        "  BLEU is based on the degree of n-gram overlapping between the strings of words produced by the machine and the human translation references at the corpus level. BLEU computes the precision for n-gram of size 1-to-4 with\n",
        "the coefficient of brevity penalty (BP).\n",
        "\n",
        "\n",
        "**ROUGE Score [5] (n-gram recall)**\n",
        "\n",
        "  ROUGE compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE is another metric that focuses upon recall rather than precision like BLEU.\n",
        "\n",
        "* Diverse Variations, here we focus on ROUGE-1, ROUGE-2 and ROUGE-L\n",
        "\n",
        "  1) ROUGE-N: how many n-grams from the reference translation have been properly predicted in the machine generated translation\n",
        "\n",
        "  (ROUGE-1: unigrams, ROUGE-2: bigrams)\n",
        "\n",
        "  2) ROUGE-L: measure of recall for longest common subsequences. \n",
        "\n",
        "---------------\n",
        "BLEU focuses on precision: how much the words (and/or n-grams) in the candidate model outputs appear in the human reference.\n",
        "\n",
        "ROUGE focuses on recall: how much the words (and/or n-grams) in the human references appear in the candidate model outputs.\n",
        "\n",
        "These results are complementing, as is often the case in the precision-recall tradeoff. Therefore, we try to check both metrics for our model's evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ayt8EvmtAIwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "id": "6B3HBE6AA8nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "Dv6nGsI1A5AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM Inference \n",
        "In sequence-to-sequence, how it works in training phase and test phase is different. Therefore, we have to design model for test phase (especially, we need to change decoder). \n",
        "\n",
        "\n",
        "* Overall translation process is as follows. \n",
        "\n",
        "  1) Input source sentence to be translated into encoder and get the final step's hidden and cell state. \n",
        "\n",
        "  2) Encoder's final hidden state, cell state and \\<sos> token is sent to decoder.\n",
        "\n",
        "  3) Decoder iterate next word prediction until \\<eos> token.\n",
        "\n",
        "  4) The input and output of encoder, encoder_inputs and encoder_states is reused that was defined in training phase. We get all layers from encoder_inputs to encoder_states in training phase. That is, we reuse encoder used in training phase.\n",
        "\n",
        "  5) We control decoder's every step in test phase, so we define decoder_state_input_h, decoder_state_input_c which store previous step's state. "
      ],
      "metadata": {
        "id": "275JdZdj8CBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder \n",
        "# Tensor for previous step's state \n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Reuse Embedding layer that was used in training phase \n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict next word.. \n",
        "# Previous time step's state -> current time step's initial state  \n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# Predict word in every time step \n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Updated decoder \n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "EMSTDTFEx4BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # Iterate loop until stop_condition becomes True (we use batch size 1 for 구현의  간소화를 위해)\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # use previous step's states_value as current step's initial state \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        \n",
        "        # transform predicted result to word \n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        \n",
        "        # add current step's predicted word to predicted sentence \n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        \n",
        "        # reach <eos> or exceed range -> stop\n",
        "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # to use current step's predicted result to next step's input\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # to use current step's state in next step's state \n",
        "        states_value = [h, c]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "jTGfysgJx4xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform original sentence's integer sequence to text sequence \n",
        "# English integer sequence -> English text sequence\n",
        "def seq_to_src(input_seq):\n",
        "    sentence = ''\n",
        "    for encoded_word in input_seq:\n",
        "        if(encoded_word != 0):\n",
        "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "    return sentence\n",
        "\n",
        "# transform translated sentence's integer sequence to text sequence\n",
        "# French integer sequence -> French text sequence\n",
        "def seq_to_tar(input_seq):\n",
        "    sentence = ''\n",
        "    for encoded_word in input_seq:\n",
        "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "            \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "5wjIsGaFyFzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Evaluation"
      ],
      "metadata": {
        "id": "NpJghazNO6gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "for seq_index in [3, 55, 1020, 300, 1001]:\n",
        "    reference = [[]]\n",
        "    candidate = []\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "\n",
        "    input_sentence = seq_to_src(encoder_input_train[seq_index])\n",
        "    answer_sentence = seq_to_tar(decoder_input_train[seq_index])\n",
        "    translated_sentence = decoded_sentence[1:-5]\n",
        "\n",
        "    print(\"Input Sentence :\", input_sentence)\n",
        "    print(\"Answer Sentence: \", answer_sentence)\n",
        "    print(\"Translated Sentence :\", translated_sentence)\n",
        "\n",
        "    print(rouge.get_scores([translated_sentence], [answer_sentence], avg=True))\n",
        "    reference[0].extend(answer_sentence.split())\n",
        "    candidate.extend(translated_sentence.split())\n",
        "    print('BLEU-1 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))))\n",
        "    print('BLEU-2 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))))\n",
        "    print('BLEU-3 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))))\n",
        "    print('BLEU-4 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))))\n",
        "\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RB4CBPqyLeK",
        "outputId": "e4ca7817-0031-464f-9c43-196c29cf3c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Input Sentence : yeah you re right . \n",
            "Answer Sentence:  ouais vous avez raison . \n",
            "Translated Sentence : ouais tu as raison . \n",
            "{'rouge-1': {'r': 0.6, 'p': 0.6, 'f': 0.5999999950000001}, 'rouge-2': {'r': 0.25, 'p': 0.25, 'f': 0.24999999500000009}, 'rouge-l': {'r': 0.6, 'p': 0.6, 'f': 0.5999999950000001}}\n",
            "BLEU-1 score: 0.60000\n",
            "BLEU-2 score: 0.25000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Input Sentence : i know tom is sensible . \n",
            "Answer Sentence:  je sais que tom est raisonnable . \n",
            "Translated Sentence : je sais que tom est raisonnable . \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Input Sentence : he gave the dog a bone . \n",
            "Answer Sentence:  il donna un os au chien . \n",
            "Translated Sentence : il a donne un chien de gateau . \n",
            "{'rouge-1': {'r': 0.5714285714285714, 'p': 0.5, 'f': 0.5333333283555556}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.5714285714285714, 'p': 0.5, 'f': 0.5333333283555556}}\n",
            "BLEU-1 score: 0.50000\n",
            "BLEU-2 score: 0.00000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Input Sentence : i m sincere . \n",
            "Answer Sentence:  je suis sincere . \n",
            "Translated Sentence : je suis sincere . \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Input Sentence : who do you live with ? \n",
            "Answer Sentence:  avec qui vis tu ? \n",
            "Translated Sentence : avec qui es tu en train de voir ? \n",
            "{'rouge-1': {'r': 0.8, 'p': 0.4444444444444444, 'f': 0.5714285668367348}, 'rouge-2': {'r': 0.25, 'p': 0.125, 'f': 0.16666666222222234}, 'rouge-l': {'r': 0.8, 'p': 0.4444444444444444, 'f': 0.5714285668367348}}\n",
            "BLEU-1 score: 0.44444\n",
            "BLEU-2 score: 0.12500\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GRU model with same hyperparameter"
      ],
      "metadata": {
        "id": "YTbIt7K18F28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## GRU\n",
        "### Encoder\n",
        "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
        "embedding = Embedding(\n",
        "    src_vocab_size, embedding_dim, name=\"encoder_embedding\"\n",
        ")\n",
        "encoder_gru = GRU(\n",
        "    hidden_units, return_state=True, name=\"encoder_gru\", dropout=encoder_dropout\n",
        ")\n",
        "encoder_emb = embedding(encoder_inputs)\n",
        "encoder_outputs, encoder_state = encoder_gru(encoder_emb)\n",
        "\n",
        "  ### Decoder\n",
        "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
        "embedding = Embedding(\n",
        "    tar_vocab_size, embedding_dim, input_length=tar_seqlen-1, name=\"decoder_embedding\"\n",
        "  )\n",
        "decoder_gru = GRU(\n",
        "    hidden_units, return_state=True, return_sequences=True, name=\"decoder_gru\", \n",
        "    dropout=decoder_dropout\n",
        ")\n",
        "\n",
        "decoder_emb = embedding(decoder_inputs)\n",
        "decoder_outputs, _ = decoder_gru(decoder_emb, initial_state=encoder_state)\n",
        "dense_layer = Dense(tar_vocab_size, activation=\"softmax\", name=\"dense_layer\")\n",
        "decoder_outputs = dense_layer(decoder_outputs)\n",
        "\n",
        "  # Define the Model which accepts encoder/decoder inputs and outputs predictions\n",
        "model = Model(\n",
        "    inputs=[encoder_inputs, decoder_inputs],\n",
        "    outputs=decoder_outputs,\n",
        "    name=\"encoder_decoder_model_gru\",\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "        validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "        batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 10)], epochs=50) # for testing, we use epochs = 10 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My5jVwpiwEto",
        "outputId": "e3701571-163c-4db8-8c7f-5eccc69f81e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "493/493 [==============================] - 19s 32ms/step - loss: 2.7961 - acc: 0.6422 - val_loss: 1.8180 - val_acc: 0.7192\n",
            "Epoch 2/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.6975 - acc: 0.7283 - val_loss: 1.6294 - val_acc: 0.7350\n",
            "Epoch 3/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.5571 - acc: 0.7422 - val_loss: 1.5171 - val_acc: 0.7488\n",
            "Epoch 4/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.4334 - acc: 0.7562 - val_loss: 1.3820 - val_acc: 0.7662\n",
            "Epoch 5/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.2983 - acc: 0.7772 - val_loss: 1.2724 - val_acc: 0.7833\n",
            "Epoch 6/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.1920 - acc: 0.7915 - val_loss: 1.1878 - val_acc: 0.7956\n",
            "Epoch 7/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.1080 - acc: 0.8032 - val_loss: 1.1253 - val_acc: 0.8058\n",
            "Epoch 8/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 1.0442 - acc: 0.8113 - val_loss: 1.0771 - val_acc: 0.8126\n",
            "Epoch 9/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.9898 - acc: 0.8178 - val_loss: 1.0357 - val_acc: 0.8179\n",
            "Epoch 10/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.9432 - acc: 0.8233 - val_loss: 1.0053 - val_acc: 0.8213\n",
            "Epoch 11/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.9062 - acc: 0.8272 - val_loss: 0.9800 - val_acc: 0.8248\n",
            "Epoch 12/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.8739 - acc: 0.8307 - val_loss: 0.9591 - val_acc: 0.8274\n",
            "Epoch 13/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.8452 - acc: 0.8337 - val_loss: 0.9410 - val_acc: 0.8298\n",
            "Epoch 14/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.8195 - acc: 0.8365 - val_loss: 0.9239 - val_acc: 0.8312\n",
            "Epoch 15/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.7957 - acc: 0.8393 - val_loss: 0.9087 - val_acc: 0.8339\n",
            "Epoch 16/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.7733 - acc: 0.8421 - val_loss: 0.8950 - val_acc: 0.8355\n",
            "Epoch 17/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.7527 - acc: 0.8442 - val_loss: 0.8818 - val_acc: 0.8377\n",
            "Epoch 18/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.7340 - acc: 0.8466 - val_loss: 0.8696 - val_acc: 0.8392\n",
            "Epoch 19/50\n",
            "493/493 [==============================] - 16s 33ms/step - loss: 0.7152 - acc: 0.8488 - val_loss: 0.8585 - val_acc: 0.8405\n",
            "Epoch 20/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.6981 - acc: 0.8509 - val_loss: 0.8477 - val_acc: 0.8422\n",
            "Epoch 21/50\n",
            "493/493 [==============================] - 16s 33ms/step - loss: 0.6821 - acc: 0.8532 - val_loss: 0.8380 - val_acc: 0.8437\n",
            "Epoch 22/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.6666 - acc: 0.8554 - val_loss: 0.8283 - val_acc: 0.8452\n",
            "Epoch 23/50\n",
            "493/493 [==============================] - 18s 36ms/step - loss: 0.6515 - acc: 0.8573 - val_loss: 0.8189 - val_acc: 0.8464\n",
            "Epoch 24/50\n",
            "493/493 [==============================] - 17s 35ms/step - loss: 0.6378 - acc: 0.8590 - val_loss: 0.8106 - val_acc: 0.8472\n",
            "Epoch 25/50\n",
            "493/493 [==============================] - 17s 34ms/step - loss: 0.6242 - acc: 0.8608 - val_loss: 0.8029 - val_acc: 0.8491\n",
            "Epoch 26/50\n",
            "493/493 [==============================] - 16s 33ms/step - loss: 0.6111 - acc: 0.8631 - val_loss: 0.7953 - val_acc: 0.8497\n",
            "Epoch 27/50\n",
            "493/493 [==============================] - 17s 33ms/step - loss: 0.5991 - acc: 0.8647 - val_loss: 0.7873 - val_acc: 0.8508\n",
            "Epoch 28/50\n",
            "493/493 [==============================] - 17s 34ms/step - loss: 0.5872 - acc: 0.8665 - val_loss: 0.7803 - val_acc: 0.8518\n",
            "Epoch 29/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5757 - acc: 0.8679 - val_loss: 0.7741 - val_acc: 0.8526\n",
            "Epoch 30/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5642 - acc: 0.8700 - val_loss: 0.7667 - val_acc: 0.8538\n",
            "Epoch 31/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.5543 - acc: 0.8713 - val_loss: 0.7611 - val_acc: 0.8547\n",
            "Epoch 32/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5438 - acc: 0.8729 - val_loss: 0.7551 - val_acc: 0.8562\n",
            "Epoch 33/50\n",
            "493/493 [==============================] - 17s 35ms/step - loss: 0.5346 - acc: 0.8746 - val_loss: 0.7490 - val_acc: 0.8568\n",
            "Epoch 34/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5245 - acc: 0.8762 - val_loss: 0.7437 - val_acc: 0.8577\n",
            "Epoch 35/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5158 - acc: 0.8777 - val_loss: 0.7385 - val_acc: 0.8588\n",
            "Epoch 36/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.5067 - acc: 0.8793 - val_loss: 0.7336 - val_acc: 0.8599\n",
            "Epoch 37/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4988 - acc: 0.8805 - val_loss: 0.7288 - val_acc: 0.8603\n",
            "Epoch 38/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4907 - acc: 0.8821 - val_loss: 0.7246 - val_acc: 0.8613\n",
            "Epoch 39/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4822 - acc: 0.8834 - val_loss: 0.7196 - val_acc: 0.8624\n",
            "Epoch 40/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4753 - acc: 0.8845 - val_loss: 0.7150 - val_acc: 0.8627\n",
            "Epoch 41/50\n",
            "493/493 [==============================] - 15s 31ms/step - loss: 0.4678 - acc: 0.8859 - val_loss: 0.7114 - val_acc: 0.8632\n",
            "Epoch 42/50\n",
            "493/493 [==============================] - 16s 31ms/step - loss: 0.4603 - acc: 0.8872 - val_loss: 0.7075 - val_acc: 0.8644\n",
            "Epoch 43/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4537 - acc: 0.8884 - val_loss: 0.7031 - val_acc: 0.8653\n",
            "Epoch 44/50\n",
            "493/493 [==============================] - 16s 31ms/step - loss: 0.4475 - acc: 0.8896 - val_loss: 0.6998 - val_acc: 0.8653\n",
            "Epoch 45/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4408 - acc: 0.8907 - val_loss: 0.6962 - val_acc: 0.8667\n",
            "Epoch 46/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4348 - acc: 0.8918 - val_loss: 0.6942 - val_acc: 0.8668\n",
            "Epoch 47/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4288 - acc: 0.8931 - val_loss: 0.6908 - val_acc: 0.8673\n",
            "Epoch 48/50\n",
            "493/493 [==============================] - 16s 32ms/step - loss: 0.4237 - acc: 0.8941 - val_loss: 0.6874 - val_acc: 0.8684\n",
            "Epoch 49/50\n",
            "493/493 [==============================] - 16s 31ms/step - loss: 0.4184 - acc: 0.8949 - val_loss: 0.6848 - val_acc: 0.8685\n",
            "Epoch 50/50\n",
            "493/493 [==============================] - 16s 31ms/step - loss: 0.4135 - acc: 0.8957 - val_loss: 0.6821 - val_acc: 0.8695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.legend(labels=[\"loss\", \"val_loss\"])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "NyXjak0rzbTY",
        "outputId": "ab016816-8bfb-4b4c-d488-9835cf208571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GRU inference & evalulation"
      ],
      "metadata": {
        "id": "0cV5vt-f8Ler"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inference_encoder_from(input_layer, embedding_layer, gru_layer):\n",
        "  encoder_embedding = embedding_layer(input_layer)\n",
        "  encoder_outputs, encoder_state = gru_layer(encoder_embedding)\n",
        "  encoder = Model(input_layer, encoder_state)\n",
        "  return encoder\n",
        "\n",
        "def create_inference_decoder_from(input_layer, embedding_layer, gru_layer, dense_layer):\n",
        "  decoder_embedding = embedding_layer(input_layer)\n",
        "  input_shape = dense_layer.input.shape[-1]\n",
        "  decoder_inputs_state = Input(shape=(input_shape,))\n",
        "\n",
        "  decoder_output, decoder_output_state = gru_layer(\n",
        "    decoder_embedding, initial_state=decoder_inputs_state\n",
        "  )\n",
        "\n",
        "  decoder_prediction = dense_layer(decoder_output)\n",
        "  decoder = Model(\n",
        "    inputs=[input_layer, decoder_inputs_state], \n",
        "    outputs=[decoder_prediction, decoder_output_state]\n",
        "  )\n",
        "  return decoder"
      ],
      "metadata": {
        "id": "9PKGW6zqzA0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = create_inference_encoder_from(\n",
        "  model.get_layer('encoder_input').input,\n",
        "  model.get_layer('encoder_embedding'),\n",
        "  model.get_layer('encoder_gru')\n",
        ")\n",
        "\n",
        "decoder = create_inference_decoder_from(\n",
        "  model.get_layer('decoder_input').input,\n",
        "  model.get_layer('decoder_embedding'),\n",
        "  model.get_layer('decoder_gru'),\n",
        "  model.get_layer('dense_layer'),\n",
        ")"
      ],
      "metadata": {
        "id": "TerIKRHpzhwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "    state = encoder.predict(input_seq)\n",
        "    \n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, state = decoder.predict([target_seq, state])\n",
        "        \n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        \n",
        "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        \n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        state = [state]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "hhZOcfnL0Ibk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "for seq_index in [3, 55, 1020, 300, 1001]:\n",
        "    reference = [[]]\n",
        "    candidate = []\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "\n",
        "    input_sentence = seq_to_src(encoder_input_train[seq_index])\n",
        "    answer_sentence = seq_to_tar(decoder_input_train[seq_index])\n",
        "    translated_sentence = decoded_sentence[1:-5]\n",
        "\n",
        "    print(\"Input Sentence :\", input_sentence)\n",
        "    print(\"Answer Sentence: \", answer_sentence)\n",
        "    print(\"Translated Sentence :\", translated_sentence)\n",
        "\n",
        "    print(rouge.get_scores([translated_sentence], [answer_sentence], avg=True))\n",
        "    reference[0].extend(answer_sentence.split())\n",
        "    candidate.extend(translated_sentence.split())\n",
        "\n",
        "    print('BLEU-1 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))))\n",
        "    print('BLEU-2 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))))\n",
        "    print('BLEU-3 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))))\n",
        "    print('BLEU-4 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))))\n",
        "    \n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBMzXzwd0Naj",
        "outputId": "c9b99d66-aca7-4cf7-de5b-f48762fee16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Input Sentence : yeah you re right . \n",
            "Answer Sentence:  ouais vous avez raison . \n",
            "Translated Sentence : ouais tu as raison . \n",
            "{'rouge-1': {'r': 0.6, 'p': 0.6, 'f': 0.5999999950000001}, 'rouge-2': {'r': 0.25, 'p': 0.25, 'f': 0.24999999500000009}, 'rouge-l': {'r': 0.6, 'p': 0.6, 'f': 0.5999999950000001}}\n",
            "BLEU-1 score: 0.60000\n",
            "BLEU-2 score: 0.25000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Input Sentence : i know tom is sensible . \n",
            "Answer Sentence:  je sais que tom est raisonnable . \n",
            "Translated Sentence : je sais que tom est agite . \n",
            "{'rouge-1': {'r': 0.8571428571428571, 'p': 0.8571428571428571, 'f': 0.8571428521428571}, 'rouge-2': {'r': 0.6666666666666666, 'p': 0.6666666666666666, 'f': 0.6666666616666668}, 'rouge-l': {'r': 0.8571428571428571, 'p': 0.8571428571428571, 'f': 0.8571428521428571}}\n",
            "BLEU-1 score: 0.85714\n",
            "BLEU-2 score: 0.66667\n",
            "BLEU-3 score: 0.60000\n",
            "BLEU-4 score: 0.50000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Input Sentence : he gave the dog a bone . \n",
            "Answer Sentence:  il donna un os au chien . \n",
            "Translated Sentence : il a tenu sa propre chambre . \n",
            "{'rouge-1': {'r': 0.2857142857142857, 'p': 0.2857142857142857, 'f': 0.2857142807142858}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.2857142857142857, 'f': 0.2857142807142858}}\n",
            "BLEU-1 score: 0.28571\n",
            "BLEU-2 score: 0.00000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Input Sentence : i m sincere . \n",
            "Answer Sentence:  je suis sincere . \n",
            "Translated Sentence : je suis sincere . \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Input Sentence : who do you live with ? \n",
            "Answer Sentence:  avec qui vis tu ? \n",
            "Translated Sentence : qui vous voulez vous ? \n",
            "{'rouge-1': {'r': 0.4, 'p': 0.5, 'f': 0.4444444395061729}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.4, 'p': 0.5, 'f': 0.4444444395061729}}\n",
            "BLEU-1 score: 0.40000\n",
            "BLEU-2 score: 0.00000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Model - LSTM**\n",
        "Comparing BLEU-1, BLEU-2, BLEU-3, BLEU-4, the LSTM overally shows better performance compared to GRU. \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1KLOz1mVR7XJvgPxAejy23eTxtR3q9Zab\" height = 350 width = 500>\n",
        "\n",
        "Also, comparing ROUGE-1, ROUGE-2, ROUGE-L, the LSTM overally shows better performance compared to GRU. \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1EsAAGH37dFSZB4aAJXmFWpvfPg0OsN2o\" height = 400 width = 500>\n",
        "\n",
        "\n",
        "Therefore, we try to use LSTM for our neural machine translation model.\n",
        "\n"
      ],
      "metadata": {
        "id": "jvr43hxVC7r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM model**"
      ],
      "metadata": {
        "id": "SCoN6g7Wkphi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_Model(embedding_dim, hidden_units, lr, b1, b2, batchsize, encoder_dropout, decoder_dropout):\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # Embedding layer\n",
        "  enc_masking = Masking(mask_value=0.0)(enc_emb) # Exclude padding 0 in operation\n",
        "  encoder_lstm = LSTM(hidden_units, return_state=True, dropout=encoder_dropout) # To return state value\n",
        "  encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # Return hidden state and cell state\n",
        "  encoder_states = [state_h, state_c] # Save encoder's hidden state and cell state -> to decoder!\n",
        "\n",
        "  # Decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # Embedding layer\n",
        "  dec_emb = dec_emb_layer(decoder_inputs) # exclude padding 0 in operation\n",
        "  dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "  # To return state value, return_state = True\n",
        "  # To predict word for every time step, return_sequences = True\n",
        "  decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=decoder_dropout) \n",
        "\n",
        "  # Use encoder's hidden state as initial hidden state \n",
        "  decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                      initial_state=encoder_states)\n",
        "\n",
        "  # predict word bsaed on softmax activation function for all results from every time step\n",
        "  decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # Model's input and output \n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=batchsize, callbacks=[EarlyStopping(monitor='val_loss', patience = 3)], epochs=10) # for testing, we use epochs = 10 \n",
        "\n",
        "  return history.history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "IRoJKkCp99Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "ureNg0IerAW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyper_param(n_iteration):\n",
        "  hyper_param = []  # learning_rate, n_hidden, timestep, epochs\n",
        "  for i in range(n_iteration):\n",
        "    current_params = []\n",
        "    # We use adam optimizer, so we tune learning rate, b1, b2\n",
        "    current_params.append(np.random.uniform(0,0.1)) # learning rate \n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b1 (from Momentum)\n",
        "    current_params.append(np.random.uniform(0.5,0.999)) # b2 (from RMSProp)\n",
        "    current_params.append(np.random.randint(1,513)) # hidden units \n",
        "    current_params.append(np.random.randint(32,513)) # batch_size\n",
        "    current_params.append(np.random.randint(50,300)) # embedding dimension\n",
        "    current_params.append(np.random.uniform(0,0.5)) # encoder dropout\n",
        "    current_params.append(np.random.uniform(0,0.5)) # decoder dropout\n",
        "    hyper_param.append(current_params)\n",
        "  return hyper_param\n",
        "\n",
        "hyper_parameter = get_hyper_param(30)\n",
        "train_loss, val_loss = list(), list()\n",
        "best_params = []\n",
        "\n",
        "min_val_loss = 999\n",
        "for alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout in hyper_parameter:\n",
        "  print('lr',alpha, 'b1', b1, 'b2', b2, 'n_hiddens', hidden_units,'batch_size', batch_size, 'embedding_dim', embedding_dim, \n",
        "        'en_dropout',  en_dropout, 'de_dropout', de_dropout)\n",
        "\n",
        "  current_val_loss = LSTM_Model(embedding_dim, hidden_units, alpha, b1, b2, batch_size, en_dropout, de_dropout)\n",
        "\n",
        "  if current_val_loss < min_val_loss:\n",
        "    min_val_loss = current_val_loss\n",
        "    best_params = [alpha, b1, b2, hidden_units, batch_size, embedding_dim, en_dropout, de_dropout]\n",
        "    print('best_params',best_params)\n",
        "  \n",
        "print('final best params',best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-ic7p5kxBA4",
        "outputId": "219ae8b9-b226-4b63-ce29-979e14b44ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 0.06270461188149924 b1 0.843482643498749 b2 0.8925307649479214 n_hiddens 376 batch_size 455 embedding_dim 225 en_dropout 0.25149046826372107 de_dropout 0.28754374782314557\n",
            "Epoch 1/10\n",
            "139/139 [==============================] - 30s 161ms/step - loss: 1.9090 - acc: 0.7286 - val_loss: 1.3475 - val_acc: 0.7799\n",
            "Epoch 2/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 1.2717 - acc: 0.7855 - val_loss: 1.2778 - val_acc: 0.7887\n",
            "Epoch 3/10\n",
            "139/139 [==============================] - 20s 141ms/step - loss: 1.2140 - acc: 0.7914 - val_loss: 1.2685 - val_acc: 0.7928\n",
            "Epoch 4/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 1.1871 - acc: 0.7942 - val_loss: 1.2826 - val_acc: 0.7924\n",
            "Epoch 5/10\n",
            "139/139 [==============================] - 19s 139ms/step - loss: 1.1731 - acc: 0.7954 - val_loss: 1.2741 - val_acc: 0.7956\n",
            "Epoch 6/10\n",
            "139/139 [==============================] - 19s 139ms/step - loss: 1.1693 - acc: 0.7960 - val_loss: 1.2842 - val_acc: 0.7953\n",
            "best_params [0.06270461188149924, 0.843482643498749, 0.8925307649479214, 376, 455, 225, 0.25149046826372107, 0.28754374782314557]\n",
            "lr 0.02118024944924988 b1 0.9389073612586014 b2 0.6943851192685523 n_hiddens 180 batch_size 483 embedding_dim 267 en_dropout 0.26315532566808136 de_dropout 0.45960397092538496\n",
            "Epoch 1/10\n",
            "131/131 [==============================] - 23s 126ms/step - loss: 4.7951 - acc: 0.6921 - val_loss: 5.7771 - val_acc: 0.7042\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 11.5435 - acc: 0.6752 - val_loss: 10.9803 - val_acc: 0.6951\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 15s 112ms/step - loss: 27.6251 - acc: 0.6610 - val_loss: 19.6772 - val_acc: 0.6848\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 14s 107ms/step - loss: 27.9606 - acc: 0.6590 - val_loss: 15.0826 - val_acc: 0.6867\n",
            "lr 0.03532356910620672 b1 0.6991163352213629 b2 0.5106045388120525 n_hiddens 31 batch_size 428 embedding_dim 189 en_dropout 0.025871393417969957 de_dropout 0.11966258058417795\n",
            "Epoch 1/10\n",
            "148/148 [==============================] - 21s 85ms/step - loss: 2.1273 - acc: 0.6990 - val_loss: 1.5400 - val_acc: 0.7493\n",
            "Epoch 2/10\n",
            "148/148 [==============================] - 10s 71ms/step - loss: 1.4382 - acc: 0.7627 - val_loss: 1.3728 - val_acc: 0.7709\n",
            "Epoch 3/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.2985 - acc: 0.7808 - val_loss: 1.2627 - val_acc: 0.7897\n",
            "Epoch 4/10\n",
            "148/148 [==============================] - 10s 69ms/step - loss: 1.2128 - acc: 0.7962 - val_loss: 1.2211 - val_acc: 0.7977\n",
            "Epoch 5/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1712 - acc: 0.8026 - val_loss: 1.1976 - val_acc: 0.8020\n",
            "Epoch 6/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1501 - acc: 0.8057 - val_loss: 1.1824 - val_acc: 0.8044\n",
            "Epoch 7/10\n",
            "148/148 [==============================] - 11s 72ms/step - loss: 1.1391 - acc: 0.8079 - val_loss: 1.1858 - val_acc: 0.8046\n",
            "Epoch 8/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1317 - acc: 0.8094 - val_loss: 1.1775 - val_acc: 0.8071\n",
            "Epoch 9/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1240 - acc: 0.8106 - val_loss: 1.1830 - val_acc: 0.8075\n",
            "Epoch 10/10\n",
            "148/148 [==============================] - 10s 68ms/step - loss: 1.1185 - acc: 0.8112 - val_loss: 1.1884 - val_acc: 0.8047\n",
            "best_params [0.03532356910620672, 0.6991163352213629, 0.5106045388120525, 31, 428, 189, 0.025871393417969957, 0.11966258058417795]\n",
            "lr 0.004802586925210162 b1 0.9034282275741946 b2 0.5912046730992582 n_hiddens 347 batch_size 75 embedding_dim 197 en_dropout 0.09069297020865585 de_dropout 0.3571965711563737\n",
            "Epoch 1/10\n",
            "840/840 [==============================] - 41s 42ms/step - loss: 47.3274 - acc: 0.5939 - val_loss: 40.8621 - val_acc: 0.6827\n",
            "Epoch 2/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 28.2203 - acc: 0.6869 - val_loss: 20.1261 - val_acc: 0.7027\n",
            "Epoch 3/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 17.9412 - acc: 0.6983 - val_loss: 14.8258 - val_acc: 0.7074\n",
            "Epoch 4/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 14.3908 - acc: 0.6995 - val_loss: 12.5988 - val_acc: 0.7119\n",
            "Epoch 5/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 12.5363 - acc: 0.7025 - val_loss: 10.9532 - val_acc: 0.7096\n",
            "Epoch 6/10\n",
            "840/840 [==============================] - 33s 39ms/step - loss: 11.0241 - acc: 0.7048 - val_loss: 9.7266 - val_acc: 0.7124\n",
            "Epoch 7/10\n",
            "840/840 [==============================] - 34s 40ms/step - loss: 9.6425 - acc: 0.7064 - val_loss: 8.6794 - val_acc: 0.7161\n",
            "Epoch 8/10\n",
            "840/840 [==============================] - 34s 40ms/step - loss: 8.8936 - acc: 0.7076 - val_loss: 8.1400 - val_acc: 0.7189\n",
            "Epoch 9/10\n",
            "840/840 [==============================] - 35s 41ms/step - loss: 7.9552 - acc: 0.7092 - val_loss: 7.3166 - val_acc: 0.7167\n",
            "Epoch 10/10\n",
            "840/840 [==============================] - 36s 43ms/step - loss: 7.5176 - acc: 0.7102 - val_loss: 6.7090 - val_acc: 0.7203\n",
            "lr 0.007884927954053279 b1 0.9902412478808713 b2 0.9118407790180642 n_hiddens 386 batch_size 63 embedding_dim 167 en_dropout 0.19009677049365437 de_dropout 0.45053422893100076\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 54s 48ms/step - loss: 24.4141 - acc: 0.6873 - val_loss: 93.1671 - val_acc: 0.5327\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 43s 43ms/step - loss: 238.8297 - acc: 0.5407 - val_loss: 200.5516 - val_acc: 0.5119\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 43s 43ms/step - loss: 427.3808 - acc: 0.5085 - val_loss: 337.5587 - val_acc: 0.6407\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 42s 42ms/step - loss: 537.8705 - acc: 0.4848 - val_loss: 327.3697 - val_acc: 0.6251\n",
            "lr 0.09523313337605821 b1 0.6918414557475971 b2 0.870946217502218 n_hiddens 400 batch_size 205 embedding_dim 191 en_dropout 0.2421933926534539 de_dropout 0.048646705756616504\n",
            "Epoch 1/10\n",
            "308/308 [==============================] - 40s 96ms/step - loss: 2.1461 - acc: 0.7245 - val_loss: 1.6466 - val_acc: 0.7559\n",
            "Epoch 2/10\n",
            "308/308 [==============================] - 27s 87ms/step - loss: 1.6281 - acc: 0.7584 - val_loss: 1.6918 - val_acc: 0.7595\n",
            "Epoch 3/10\n",
            "308/308 [==============================] - 27s 86ms/step - loss: 1.6613 - acc: 0.7608 - val_loss: 1.7405 - val_acc: 0.7580\n",
            "Epoch 4/10\n",
            "308/308 [==============================] - 26s 85ms/step - loss: 1.6841 - acc: 0.7651 - val_loss: 1.8078 - val_acc: 0.7674\n",
            "lr 0.08857063677568827 b1 0.7265080871745688 b2 0.6628457134279763 n_hiddens 372 batch_size 334 embedding_dim 189 en_dropout 0.4249982088943396 de_dropout 0.3398478068941738\n",
            "Epoch 1/10\n",
            "189/189 [==============================] - 32s 123ms/step - loss: 3.7367 - acc: 0.7050 - val_loss: 1.5900 - val_acc: 0.7657\n",
            "Epoch 2/10\n",
            "189/189 [==============================] - 20s 107ms/step - loss: 1.5847 - acc: 0.7706 - val_loss: 1.6171 - val_acc: 0.7728\n",
            "Epoch 3/10\n",
            "189/189 [==============================] - 20s 107ms/step - loss: 1.6355 - acc: 0.7723 - val_loss: 1.6590 - val_acc: 0.7752\n",
            "Epoch 4/10\n",
            "189/189 [==============================] - 20s 108ms/step - loss: 1.6976 - acc: 0.7729 - val_loss: 1.7277 - val_acc: 0.7767\n",
            "lr 0.0028760899584410193 b1 0.7334312865588856 b2 0.9341215051090186 n_hiddens 48 batch_size 192 embedding_dim 96 en_dropout 0.3086579782727112 de_dropout 0.49606742902546846\n",
            "Epoch 1/10\n",
            "329/329 [==============================] - 25s 54ms/step - loss: 2.3701 - acc: 0.6782 - val_loss: 1.5742 - val_acc: 0.7502\n",
            "Epoch 2/10\n",
            "329/329 [==============================] - 16s 48ms/step - loss: 1.4242 - acc: 0.7722 - val_loss: 1.3342 - val_acc: 0.7857\n",
            "Epoch 3/10\n",
            "329/329 [==============================] - 15s 46ms/step - loss: 1.2487 - acc: 0.7955 - val_loss: 1.2044 - val_acc: 0.8043\n",
            "Epoch 4/10\n",
            "329/329 [==============================] - 14s 44ms/step - loss: 1.1464 - acc: 0.8087 - val_loss: 1.1237 - val_acc: 0.8141\n",
            "Epoch 5/10\n",
            "329/329 [==============================] - 16s 50ms/step - loss: 1.0751 - acc: 0.8171 - val_loss: 1.0709 - val_acc: 0.8197\n",
            "Epoch 6/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 1.0205 - acc: 0.8232 - val_loss: 1.0251 - val_acc: 0.8253\n",
            "Epoch 7/10\n",
            "329/329 [==============================] - 14s 44ms/step - loss: 0.9763 - acc: 0.8284 - val_loss: 0.9852 - val_acc: 0.8310\n",
            "Epoch 8/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 0.9381 - acc: 0.8332 - val_loss: 0.9549 - val_acc: 0.8356\n",
            "Epoch 9/10\n",
            "329/329 [==============================] - 15s 45ms/step - loss: 0.9056 - acc: 0.8373 - val_loss: 0.9281 - val_acc: 0.8395\n",
            "Epoch 10/10\n",
            "329/329 [==============================] - 15s 46ms/step - loss: 0.8772 - acc: 0.8410 - val_loss: 0.9062 - val_acc: 0.8423\n",
            "best_params [0.0028760899584410193, 0.7334312865588856, 0.9341215051090186, 48, 192, 96, 0.3086579782727112, 0.49606742902546846]\n",
            "lr 0.04238064913974372 b1 0.829922237961483 b2 0.8271039498988201 n_hiddens 185 batch_size 391 embedding_dim 170 en_dropout 0.2392713682353102 de_dropout 0.2268610592144788\n",
            "Epoch 1/10\n",
            "162/162 [==============================] - 26s 116ms/step - loss: 1.7773 - acc: 0.7336 - val_loss: 1.2973 - val_acc: 0.7786\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 16s 98ms/step - loss: 1.2114 - acc: 0.7892 - val_loss: 1.1852 - val_acc: 0.7972\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 95ms/step - loss: 1.1253 - acc: 0.8020 - val_loss: 1.1646 - val_acc: 0.8036\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 16s 96ms/step - loss: 1.0940 - acc: 0.8063 - val_loss: 1.1693 - val_acc: 0.8052\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 16s 96ms/step - loss: 1.0787 - acc: 0.8086 - val_loss: 1.1743 - val_acc: 0.8056\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 15s 94ms/step - loss: 1.0697 - acc: 0.8096 - val_loss: 1.1852 - val_acc: 0.8066\n",
            "lr 0.04862684276952181 b1 0.6396875348678882 b2 0.9641528204389789 n_hiddens 336 batch_size 154 embedding_dim 297 en_dropout 0.3454672501989144 de_dropout 0.011447366627501177\n",
            "Epoch 1/10\n",
            "410/410 [==============================] - 36s 68ms/step - loss: 2.0250 - acc: 0.7346 - val_loss: 1.4690 - val_acc: 0.7614\n",
            "Epoch 2/10\n",
            "410/410 [==============================] - 25s 61ms/step - loss: 1.3711 - acc: 0.7789 - val_loss: 1.4103 - val_acc: 0.7802\n",
            "Epoch 3/10\n",
            "410/410 [==============================] - 26s 63ms/step - loss: 1.3285 - acc: 0.7851 - val_loss: 1.4063 - val_acc: 0.7870\n",
            "Epoch 4/10\n",
            "410/410 [==============================] - 27s 66ms/step - loss: 1.3032 - acc: 0.7876 - val_loss: 1.4261 - val_acc: 0.7844\n",
            "Epoch 5/10\n",
            "410/410 [==============================] - 26s 64ms/step - loss: 1.3137 - acc: 0.7878 - val_loss: 1.4847 - val_acc: 0.7856\n",
            "Epoch 6/10\n",
            "410/410 [==============================] - 25s 61ms/step - loss: 1.3207 - acc: 0.7884 - val_loss: 1.4932 - val_acc: 0.7845\n",
            "lr 0.009356619761583062 b1 0.5332853760659995 b2 0.7985972254756912 n_hiddens 421 batch_size 60 embedding_dim 157 en_dropout 0.42917553109835965 de_dropout 0.360774491217183\n",
            "Epoch 1/10\n",
            "1050/1050 [==============================] - 52s 44ms/step - loss: 1.2394 - acc: 0.8083 - val_loss: 1.0283 - val_acc: 0.8401\n",
            "Epoch 2/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 1.0212 - acc: 0.8424 - val_loss: 0.9933 - val_acc: 0.8477\n",
            "Epoch 3/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9743 - acc: 0.8471 - val_loss: 0.9738 - val_acc: 0.8507\n",
            "Epoch 4/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 0.9512 - acc: 0.8493 - val_loss: 0.9908 - val_acc: 0.8514\n",
            "Epoch 5/10\n",
            "1050/1050 [==============================] - 45s 43ms/step - loss: 0.9812 - acc: 0.8477 - val_loss: 0.9939 - val_acc: 0.8522\n",
            "Epoch 6/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9266 - acc: 0.8508 - val_loss: 0.9483 - val_acc: 0.8567\n",
            "Epoch 7/10\n",
            "1050/1050 [==============================] - 45s 42ms/step - loss: 0.8982 - acc: 0.8552 - val_loss: 0.9917 - val_acc: 0.8564\n",
            "Epoch 8/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.9093 - acc: 0.8561 - val_loss: 1.0104 - val_acc: 0.8599\n",
            "Epoch 9/10\n",
            "1050/1050 [==============================] - 44s 42ms/step - loss: 0.8996 - acc: 0.8576 - val_loss: 1.0145 - val_acc: 0.8573\n",
            "lr 0.0457281561797704 b1 0.5076710156515364 b2 0.5199289398437946 n_hiddens 165 batch_size 225 embedding_dim 129 en_dropout 0.1406144712333035 de_dropout 0.45555442338770247\n",
            "Epoch 1/10\n",
            "280/280 [==============================] - 25s 68ms/step - loss: 1.4679 - acc: 0.7771 - val_loss: 1.2018 - val_acc: 0.8130\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 16s 58ms/step - loss: 1.2358 - acc: 0.8104 - val_loss: 1.2081 - val_acc: 0.8154\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 16s 59ms/step - loss: 1.2749 - acc: 0.8106 - val_loss: 1.2691 - val_acc: 0.8141\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 16s 58ms/step - loss: 1.3564 - acc: 0.8092 - val_loss: 1.3323 - val_acc: 0.8130\n",
            "lr 0.01679811924217257 b1 0.9959429750827256 b2 0.7594863749500774 n_hiddens 231 batch_size 502 embedding_dim 267 en_dropout 0.3497943935602377 de_dropout 0.1712040816938234\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 24s 133ms/step - loss: 12.2860 - acc: 0.6638 - val_loss: 65.2469 - val_acc: 0.5983\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 14s 112ms/step - loss: 734.9335 - acc: 0.3042 - val_loss: 2500.8958 - val_acc: 0.1876\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 14s 114ms/step - loss: 7291.9438 - acc: 0.3178 - val_loss: 14923.3516 - val_acc: 0.2686\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 14s 113ms/step - loss: 20190.1211 - acc: 0.1384 - val_loss: 14675.0283 - val_acc: 0.1471\n",
            "lr 0.09265086317330892 b1 0.8800318284057217 b2 0.9425648260243342 n_hiddens 8 batch_size 292 embedding_dim 202 en_dropout 0.00891600351209293 de_dropout 0.2948899342456032\n",
            "Epoch 1/10\n",
            "216/216 [==============================] - 24s 71ms/step - loss: 2.0993 - acc: 0.6928 - val_loss: 1.6373 - val_acc: 0.7355\n",
            "Epoch 2/10\n",
            "216/216 [==============================] - 12s 54ms/step - loss: 1.6070 - acc: 0.7368 - val_loss: 1.5688 - val_acc: 0.7426\n",
            "Epoch 3/10\n",
            "216/216 [==============================] - 12s 54ms/step - loss: 1.5582 - acc: 0.7400 - val_loss: 1.5364 - val_acc: 0.7451\n",
            "Epoch 4/10\n",
            "216/216 [==============================] - 12s 55ms/step - loss: 1.5304 - acc: 0.7425 - val_loss: 1.5176 - val_acc: 0.7467\n",
            "Epoch 5/10\n",
            "216/216 [==============================] - 12s 55ms/step - loss: 1.5162 - acc: 0.7437 - val_loss: 1.5163 - val_acc: 0.7467\n",
            "Epoch 6/10\n",
            "216/216 [==============================] - 13s 60ms/step - loss: 1.5076 - acc: 0.7443 - val_loss: 1.5129 - val_acc: 0.7472\n",
            "Epoch 7/10\n",
            "216/216 [==============================] - 13s 58ms/step - loss: 1.5031 - acc: 0.7446 - val_loss: 1.5112 - val_acc: 0.7485\n",
            "Epoch 8/10\n",
            "216/216 [==============================] - 12s 56ms/step - loss: 1.5001 - acc: 0.7445 - val_loss: 1.5187 - val_acc: 0.7474\n",
            "Epoch 9/10\n",
            "216/216 [==============================] - 12s 56ms/step - loss: 1.4978 - acc: 0.7449 - val_loss: 1.5133 - val_acc: 0.7478\n",
            "Epoch 10/10\n",
            "216/216 [==============================] - 12s 58ms/step - loss: 1.4959 - acc: 0.7449 - val_loss: 1.5237 - val_acc: 0.7474\n",
            "lr 0.08068888142242694 b1 0.6897667126065324 b2 0.6593224222592495 n_hiddens 495 batch_size 387 embedding_dim 157 en_dropout 0.09281511616558086 de_dropout 0.09893348002157276\n",
            "Epoch 1/10\n",
            "163/163 [==============================] - 35s 174ms/step - loss: 4.2851 - acc: 0.6987 - val_loss: 1.6232 - val_acc: 0.7527\n",
            "Epoch 2/10\n",
            "163/163 [==============================] - 26s 162ms/step - loss: 1.5616 - acc: 0.7601 - val_loss: 1.5860 - val_acc: 0.7614\n",
            "Epoch 3/10\n",
            "163/163 [==============================] - 26s 158ms/step - loss: 1.5456 - acc: 0.7668 - val_loss: 1.5965 - val_acc: 0.7720\n",
            "Epoch 4/10\n",
            "163/163 [==============================] - 26s 158ms/step - loss: 1.5534 - acc: 0.7738 - val_loss: 1.6337 - val_acc: 0.7736\n",
            "Epoch 5/10\n",
            "163/163 [==============================] - 26s 159ms/step - loss: 1.5835 - acc: 0.7752 - val_loss: 1.6768 - val_acc: 0.7743\n",
            "lr 0.09368401832374376 b1 0.6441522779323736 b2 0.7202581827799663 n_hiddens 147 batch_size 304 embedding_dim 154 en_dropout 0.1984830083082012 de_dropout 0.09426206797341496\n",
            "Epoch 1/10\n",
            "208/208 [==============================] - 31s 97ms/step - loss: 1.6601 - acc: 0.7458 - val_loss: 1.4539 - val_acc: 0.7706\n",
            "Epoch 2/10\n",
            "208/208 [==============================] - 16s 76ms/step - loss: 1.4491 - acc: 0.7717 - val_loss: 1.5041 - val_acc: 0.7694\n",
            "Epoch 3/10\n",
            "208/208 [==============================] - 16s 76ms/step - loss: 1.4806 - acc: 0.7734 - val_loss: 1.5496 - val_acc: 0.7717\n",
            "Epoch 4/10\n",
            "208/208 [==============================] - 15s 73ms/step - loss: 1.5147 - acc: 0.7747 - val_loss: 1.5977 - val_acc: 0.7703\n",
            "lr 0.042692283278584466 b1 0.6253716592985747 b2 0.5118043348713013 n_hiddens 55 batch_size 232 embedding_dim 221 en_dropout 0.2098795539821759 de_dropout 0.15286758968866554\n",
            "Epoch 1/10\n",
            "272/272 [==============================] - 26s 60ms/step - loss: 1.5628 - acc: 0.7640 - val_loss: 1.2489 - val_acc: 0.8026\n",
            "Epoch 2/10\n",
            "272/272 [==============================] - 14s 51ms/step - loss: 1.2165 - acc: 0.8077 - val_loss: 1.1931 - val_acc: 0.8108\n",
            "Epoch 3/10\n",
            "272/272 [==============================] - 13s 49ms/step - loss: 1.1813 - acc: 0.8128 - val_loss: 1.1857 - val_acc: 0.8127\n",
            "Epoch 4/10\n",
            "272/272 [==============================] - 14s 52ms/step - loss: 1.1783 - acc: 0.8132 - val_loss: 1.2055 - val_acc: 0.8120\n",
            "Epoch 5/10\n",
            "272/272 [==============================] - 14s 52ms/step - loss: 1.1949 - acc: 0.8126 - val_loss: 1.2465 - val_acc: 0.8093\n",
            "Epoch 6/10\n",
            "272/272 [==============================] - 15s 55ms/step - loss: 1.2098 - acc: 0.8117 - val_loss: 1.2694 - val_acc: 0.8091\n",
            "lr 0.024942993924761403 b1 0.816646713153006 b2 0.5758239227333691 n_hiddens 299 batch_size 345 embedding_dim 258 en_dropout 0.28320602438051407 de_dropout 0.006364671290011947\n",
            "Epoch 1/10\n",
            "183/183 [==============================] - 32s 118ms/step - loss: 1.8320 - acc: 0.7312 - val_loss: 1.4160 - val_acc: 0.7705\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 19s 106ms/step - loss: 1.3323 - acc: 0.7758 - val_loss: 1.3254 - val_acc: 0.7756\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2669 - acc: 0.7813 - val_loss: 1.3012 - val_acc: 0.7809\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2382 - acc: 0.7847 - val_loss: 1.2951 - val_acc: 0.7823\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 1.2307 - acc: 0.7867 - val_loss: 1.3297 - val_acc: 0.7815\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 19s 105ms/step - loss: 1.2393 - acc: 0.7880 - val_loss: 1.3414 - val_acc: 0.7842\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 19s 107ms/step - loss: 1.2456 - acc: 0.7891 - val_loss: 1.3687 - val_acc: 0.7856\n",
            "lr 0.029361976229069386 b1 0.8722124105253817 b2 0.7990420122442797 n_hiddens 471 batch_size 127 embedding_dim 80 en_dropout 0.46692086319982246 de_dropout 0.424282790315655\n",
            "Epoch 1/10\n",
            "497/497 [==============================] - 44s 70ms/step - loss: 2.1539 - acc: 0.7357 - val_loss: 1.5056 - val_acc: 0.7663\n",
            "Epoch 2/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.4992 - acc: 0.7694 - val_loss: 1.5014 - val_acc: 0.7733\n",
            "Epoch 3/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.5303 - acc: 0.7730 - val_loss: 1.5535 - val_acc: 0.7763\n",
            "Epoch 4/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.5718 - acc: 0.7737 - val_loss: 1.5835 - val_acc: 0.7772\n",
            "Epoch 5/10\n",
            "497/497 [==============================] - 32s 65ms/step - loss: 1.6382 - acc: 0.7732 - val_loss: 1.6622 - val_acc: 0.7770\n",
            "lr 0.015886169959430787 b1 0.5930491086706786 b2 0.9364600597831465 n_hiddens 312 batch_size 273 embedding_dim 166 en_dropout 0.1266038983112921 de_dropout 0.47534027805897144\n",
            "Epoch 1/10\n",
            "231/231 [==============================] - 30s 97ms/step - loss: 1.4377 - acc: 0.7726 - val_loss: 0.9330 - val_acc: 0.8339\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.7914 - acc: 0.8508 - val_loss: 0.7378 - val_acc: 0.8631\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 20s 88ms/step - loss: 0.6159 - acc: 0.8744 - val_loss: 0.6561 - val_acc: 0.8763\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.5213 - acc: 0.8871 - val_loss: 0.6221 - val_acc: 0.8813\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.4640 - acc: 0.8944 - val_loss: 0.6002 - val_acc: 0.8846\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.4263 - acc: 0.8991 - val_loss: 0.6023 - val_acc: 0.8859\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.4004 - acc: 0.9024 - val_loss: 0.6051 - val_acc: 0.8852\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.3837 - acc: 0.9042 - val_loss: 0.6089 - val_acc: 0.8865\n",
            "best_params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n",
            "lr 0.05273769712037634 b1 0.771045744824429 b2 0.7738235870602771 n_hiddens 496 batch_size 72 embedding_dim 248 en_dropout 0.20062518730634193 de_dropout 0.09248141898419032\n",
            "Epoch 1/10\n",
            "875/875 [==============================] - 51s 51ms/step - loss: 1.8287 - acc: 0.7521 - val_loss: 1.8660 - val_acc: 0.7652\n",
            "Epoch 2/10\n",
            "875/875 [==============================] - 42s 47ms/step - loss: 1.9630 - acc: 0.7664 - val_loss: 2.0742 - val_acc: 0.7653\n",
            "Epoch 3/10\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 2.1886 - acc: 0.7659 - val_loss: 2.3230 - val_acc: 0.7642\n",
            "Epoch 4/10\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 2.4191 - acc: 0.7661 - val_loss: 2.5864 - val_acc: 0.7688\n",
            "lr 0.055601445446007314 b1 0.5916481064642332 b2 0.8536670972253568 n_hiddens 357 batch_size 176 embedding_dim 249 en_dropout 0.01406991540158703 de_dropout 0.39708082363877617\n",
            "Epoch 1/10\n",
            "358/358 [==============================] - 34s 78ms/step - loss: 1.5491 - acc: 0.7695 - val_loss: 1.3245 - val_acc: 0.7953\n",
            "Epoch 2/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.3409 - acc: 0.7935 - val_loss: 1.3572 - val_acc: 0.7964\n",
            "Epoch 3/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.3710 - acc: 0.7945 - val_loss: 1.3979 - val_acc: 0.7968\n",
            "Epoch 4/10\n",
            "358/358 [==============================] - 25s 70ms/step - loss: 1.4169 - acc: 0.7941 - val_loss: 1.4399 - val_acc: 0.7970\n",
            "lr 0.044215098079003395 b1 0.8786494412321751 b2 0.5257896988483647 n_hiddens 384 batch_size 454 embedding_dim 209 en_dropout 0.12885558872455527 de_dropout 0.020384087026159292\n",
            "Epoch 1/10\n",
            "139/139 [==============================] - 28s 156ms/step - loss: 4.2794 - acc: 0.6652 - val_loss: 4.4400 - val_acc: 0.7061\n",
            "Epoch 2/10\n",
            "139/139 [==============================] - 19s 140ms/step - loss: 4.0948 - acc: 0.7077 - val_loss: 3.9730 - val_acc: 0.7096\n",
            "Epoch 3/10\n",
            "139/139 [==============================] - 19s 138ms/step - loss: 4.0021 - acc: 0.7112 - val_loss: 4.0801 - val_acc: 0.7095\n",
            "Epoch 4/10\n",
            "139/139 [==============================] - 19s 135ms/step - loss: 4.6564 - acc: 0.6864 - val_loss: 4.6833 - val_acc: 0.6761\n",
            "Epoch 5/10\n",
            "139/139 [==============================] - 19s 134ms/step - loss: 5.1929 - acc: 0.7021 - val_loss: 5.6499 - val_acc: 0.6954\n",
            "lr 0.06399230927272871 b1 0.8022081836309816 b2 0.6193060601324336 n_hiddens 9 batch_size 429 embedding_dim 143 en_dropout 0.32337078773127215 de_dropout 0.19331651587228493\n",
            "Epoch 1/10\n",
            "147/147 [==============================] - 21s 85ms/step - loss: 2.7600 - acc: 0.5891 - val_loss: 2.1981 - val_acc: 0.7047\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 10s 68ms/step - loss: 1.9318 - acc: 0.7138 - val_loss: 1.7526 - val_acc: 0.7213\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.6939 - acc: 0.7297 - val_loss: 1.6521 - val_acc: 0.7357\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.6173 - acc: 0.7393 - val_loss: 1.5937 - val_acc: 0.7441\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5707 - acc: 0.7475 - val_loss: 1.5548 - val_acc: 0.7524\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.5391 - acc: 0.7523 - val_loss: 1.5328 - val_acc: 0.7555\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5202 - acc: 0.7545 - val_loss: 1.5238 - val_acc: 0.7573\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 10s 69ms/step - loss: 1.5058 - acc: 0.7559 - val_loss: 1.5089 - val_acc: 0.7576\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 11s 72ms/step - loss: 1.4938 - acc: 0.7574 - val_loss: 1.5000 - val_acc: 0.7589\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 10s 68ms/step - loss: 1.4863 - acc: 0.7581 - val_loss: 1.5007 - val_acc: 0.7603\n",
            "lr 0.06915002475804598 b1 0.9260851442606486 b2 0.6656940653868313 n_hiddens 212 batch_size 507 embedding_dim 148 en_dropout 0.06237765532307099 de_dropout 0.20756265346428165\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 24s 128ms/step - loss: 3.0013 - acc: 0.6962 - val_loss: 3.7629 - val_acc: 0.7154\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 13s 108ms/step - loss: 3.5388 - acc: 0.7148 - val_loss: 3.2693 - val_acc: 0.7200\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.5486 - acc: 0.7154 - val_loss: 3.3086 - val_acc: 0.7173\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 14s 110ms/step - loss: 3.8897 - acc: 0.7131 - val_loss: 3.5053 - val_acc: 0.7194\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 4.2286 - acc: 0.7094 - val_loss: 4.1605 - val_acc: 0.7138\n",
            "lr 0.05438078409563382 b1 0.797069514311043 b2 0.8301929923331611 n_hiddens 191 batch_size 437 embedding_dim 112 en_dropout 0.02218607384415172 de_dropout 0.2345667169178683\n",
            "Epoch 1/10\n",
            "145/145 [==============================] - 25s 118ms/step - loss: 1.8315 - acc: 0.7221 - val_loss: 1.3374 - val_acc: 0.7713\n",
            "Epoch 2/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.2338 - acc: 0.7848 - val_loss: 1.2122 - val_acc: 0.7924\n",
            "Epoch 3/10\n",
            "145/145 [==============================] - 15s 103ms/step - loss: 1.1584 - acc: 0.7955 - val_loss: 1.2064 - val_acc: 0.7968\n",
            "Epoch 4/10\n",
            "145/145 [==============================] - 15s 106ms/step - loss: 1.1330 - acc: 0.8000 - val_loss: 1.2036 - val_acc: 0.7991\n",
            "Epoch 5/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.1174 - acc: 0.8031 - val_loss: 1.2183 - val_acc: 0.8012\n",
            "Epoch 6/10\n",
            "145/145 [==============================] - 15s 102ms/step - loss: 1.1152 - acc: 0.8047 - val_loss: 1.2330 - val_acc: 0.8017\n",
            "Epoch 7/10\n",
            "145/145 [==============================] - 15s 103ms/step - loss: 1.1137 - acc: 0.8055 - val_loss: 1.2429 - val_acc: 0.8024\n",
            "lr 0.033229711654843945 b1 0.6758362206652616 b2 0.5421071153171824 n_hiddens 474 batch_size 124 embedding_dim 175 en_dropout 0.2203264138955085 de_dropout 0.10858212483705315\n",
            "Epoch 1/10\n",
            "509/509 [==============================] - 43s 71ms/step - loss: 1.5089 - acc: 0.7781 - val_loss: 1.4467 - val_acc: 0.7902\n",
            "Epoch 2/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.4814 - acc: 0.7914 - val_loss: 1.5569 - val_acc: 0.7892\n",
            "Epoch 3/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.5894 - acc: 0.7877 - val_loss: 1.6942 - val_acc: 0.7818\n",
            "Epoch 4/10\n",
            "509/509 [==============================] - 33s 65ms/step - loss: 1.6896 - acc: 0.7866 - val_loss: 1.8069 - val_acc: 0.7849\n",
            "lr 0.09371685820879909 b1 0.6131284379504724 b2 0.8654584831833052 n_hiddens 271 batch_size 235 embedding_dim 127 en_dropout 0.06330525067808912 de_dropout 0.4253666884019714\n",
            "Epoch 1/10\n",
            "269/269 [==============================] - 31s 83ms/step - loss: 1.8972 - acc: 0.7443 - val_loss: 1.4226 - val_acc: 0.7778\n",
            "Epoch 2/10\n",
            "269/269 [==============================] - 20s 76ms/step - loss: 1.4256 - acc: 0.7794 - val_loss: 1.4321 - val_acc: 0.7801\n",
            "Epoch 3/10\n",
            "269/269 [==============================] - 20s 74ms/step - loss: 1.4442 - acc: 0.7806 - val_loss: 1.4782 - val_acc: 0.7803\n",
            "Epoch 4/10\n",
            "269/269 [==============================] - 20s 73ms/step - loss: 1.4776 - acc: 0.7801 - val_loss: 1.5299 - val_acc: 0.7829\n",
            "lr 0.02534581039601982 b1 0.9644285184515715 b2 0.9541757548488676 n_hiddens 395 batch_size 408 embedding_dim 154 en_dropout 0.4695161901310664 de_dropout 0.020722358810335628\n",
            "Epoch 1/10\n",
            "155/155 [==============================] - 32s 159ms/step - loss: 2.8059 - acc: 0.6562 - val_loss: 1.9671 - val_acc: 0.7274\n",
            "Epoch 2/10\n",
            "155/155 [==============================] - 22s 144ms/step - loss: 1.5823 - acc: 0.7502 - val_loss: 1.4475 - val_acc: 0.7652\n",
            "Epoch 3/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.3072 - acc: 0.7746 - val_loss: 1.3300 - val_acc: 0.7802\n",
            "Epoch 4/10\n",
            "155/155 [==============================] - 22s 140ms/step - loss: 1.1962 - acc: 0.7825 - val_loss: 1.2911 - val_acc: 0.7821\n",
            "Epoch 5/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.1338 - acc: 0.7872 - val_loss: 1.2679 - val_acc: 0.7851\n",
            "Epoch 6/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0902 - acc: 0.7910 - val_loss: 1.2587 - val_acc: 0.7853\n",
            "Epoch 7/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0599 - acc: 0.7939 - val_loss: 1.2562 - val_acc: 0.7906\n",
            "Epoch 8/10\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 1.0367 - acc: 0.7959 - val_loss: 1.2608 - val_acc: 0.7905\n",
            "Epoch 9/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 1.0128 - acc: 0.7990 - val_loss: 1.2467 - val_acc: 0.7916\n",
            "Epoch 10/10\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 0.9887 - acc: 0.8021 - val_loss: 1.2456 - val_acc: 0.7970\n",
            "lr 0.017128197680272673 b1 0.9758117650440161 b2 0.9319366006289951 n_hiddens 236 batch_size 512 embedding_dim 95 en_dropout 0.40416444616136715 de_dropout 0.09073304852829539\n",
            "Epoch 1/10\n",
            "124/124 [==============================] - 24s 138ms/step - loss: 1.9817 - acc: 0.7106 - val_loss: 1.5866 - val_acc: 0.7512\n",
            "Epoch 2/10\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.4751 - acc: 0.7630 - val_loss: 1.4215 - val_acc: 0.7699\n",
            "Epoch 3/10\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.3033 - acc: 0.7764 - val_loss: 1.3351 - val_acc: 0.7777\n",
            "Epoch 4/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.2057 - acc: 0.7836 - val_loss: 1.2881 - val_acc: 0.7835\n",
            "Epoch 5/10\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.1418 - acc: 0.7889 - val_loss: 1.2639 - val_acc: 0.7844\n",
            "Epoch 6/10\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.0979 - acc: 0.7924 - val_loss: 1.2459 - val_acc: 0.7876\n",
            "Epoch 7/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.0623 - acc: 0.7959 - val_loss: 1.2376 - val_acc: 0.7891\n",
            "Epoch 8/10\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.0305 - acc: 0.7984 - val_loss: 1.2266 - val_acc: 0.7891\n",
            "Epoch 9/10\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.0091 - acc: 0.8002 - val_loss: 1.2260 - val_acc: 0.7913\n",
            "Epoch 10/10\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 0.9892 - acc: 0.8027 - val_loss: 1.2158 - val_acc: 0.7926\n",
            "final best params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final LSTM model**\n",
        "\n",
        "Based on best hyperparameter from hyperparameter tuning process, we build our final LSTM model with them. "
      ],
      "metadata": {
        "id": "yJk8zfZUvwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate, b1, b2, hidden_units, batch_size, embedding_dim, encoder dropout, decoder dropout\n",
        "print('final best params',best_params)\n",
        "LSTM_final_lr = best_params[0]\n",
        "LSTM_final_b1 = best_params[1]\n",
        "LSTM_final_b2 = best_params[2]\n",
        "hidden_units = best_params[3]\n",
        "LSTM_final_batch = best_params[4]\n",
        "embedding_dim = best_params[5]\n",
        "LSTM_final_enD = best_params[6]\n",
        "LSTM_final_deD = best_params[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2OcnXDk2j-O",
        "outputId": "ed038dc8-4fb9-4422-c59e-ed5b857b11a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final best params [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = [0.015886169959430787, 0.5930491086706786, 0.9364600597831465, 312, 273, 166, 0.1266038983112921, 0.47534027805897144]"
      ],
      "metadata": {
        "id": "fmBRCRYMmOEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # Embedding layer\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # Exclude padding 0 in operation\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # To return state value\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # Return hidden state and cell state\n",
        "encoder_states = [state_h, state_c] # Save encoder's hidden state and cell state\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # Embedding layer\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # exclude padding 0 in operation\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# To return state value, return_state = True\n",
        "# To predict word for every time step, return_sequences = True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# Use encoder's hidden state as initial hidden state \n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# predict word bsaed on softmax activation function for all results from every time step\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model's input and output \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=LSTM_final_lr, beta_1=LSTM_final_b1, beta_2=LSTM_final_b2), \n",
        "              loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model_weights_path = './gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt'\n",
        "\n",
        "save_best_weights = ModelCheckpoint(\n",
        "  model_weights_path, monitor='val_loss', mode='min',\n",
        "  save_weights_only=True, save_best_only=True, verbose=1, \n",
        ")\n",
        "\n",
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "        validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "        batch_size=LSTM_final_batch, callbacks=[save_best_weights,EarlyStopping(monitor='val_loss', patience = 10)], epochs=50) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QXSULWUUfEc",
        "outputId": "ad1dac30-5704-496c-97f7-af56fdb0b43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 1.4139 - acc: 0.7765\n",
            "Epoch 1: val_loss improved from inf to 0.94407, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 30s 97ms/step - loss: 1.4139 - acc: 0.7765 - val_loss: 0.9441 - val_acc: 0.8318\n",
            "Epoch 2/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.7850 - acc: 0.8515\n",
            "Epoch 2: val_loss improved from 0.94407 to 0.75577, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 19s 84ms/step - loss: 0.7850 - acc: 0.8515 - val_loss: 0.7558 - val_acc: 0.8588\n",
            "Epoch 3/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.6010 - acc: 0.8763\n",
            "Epoch 3: val_loss improved from 0.75577 to 0.68255, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.6010 - acc: 0.8763 - val_loss: 0.6825 - val_acc: 0.8708\n",
            "Epoch 4/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.4932 - acc: 0.8911\n",
            "Epoch 4: val_loss improved from 0.68255 to 0.64944, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.4932 - acc: 0.8911 - val_loss: 0.6494 - val_acc: 0.8761\n",
            "Epoch 5/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.4242 - acc: 0.9008\n",
            "Epoch 5: val_loss improved from 0.64944 to 0.63949, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.4242 - acc: 0.9008 - val_loss: 0.6395 - val_acc: 0.8792\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.3791 - acc: 0.9073\n",
            "Epoch 6: val_loss improved from 0.63949 to 0.63508, saving model to ./gdrive/MyDrive/AI/teamProject/LSTM/weights/weights.ckpt\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.3791 - acc: 0.9073 - val_loss: 0.6351 - val_acc: 0.8798\n",
            "Epoch 7/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.3463 - acc: 0.9118\n",
            "Epoch 7: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 85ms/step - loss: 0.3463 - acc: 0.9118 - val_loss: 0.6410 - val_acc: 0.8812\n",
            "Epoch 8/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.3246 - acc: 0.9150\n",
            "Epoch 8: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 85ms/step - loss: 0.3246 - acc: 0.9150 - val_loss: 0.6488 - val_acc: 0.8812\n",
            "Epoch 9/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.3078 - acc: 0.9179\n",
            "Epoch 9: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 85ms/step - loss: 0.3078 - acc: 0.9179 - val_loss: 0.6711 - val_acc: 0.8808\n",
            "Epoch 10/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2953 - acc: 0.9197\n",
            "Epoch 10: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.2953 - acc: 0.9197 - val_loss: 0.6781 - val_acc: 0.8801\n",
            "Epoch 11/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2862 - acc: 0.9215\n",
            "Epoch 11: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.2862 - acc: 0.9215 - val_loss: 0.6915 - val_acc: 0.8793\n",
            "Epoch 12/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2788 - acc: 0.9226\n",
            "Epoch 12: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.2788 - acc: 0.9226 - val_loss: 0.6997 - val_acc: 0.8806\n",
            "Epoch 13/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2711 - acc: 0.9241\n",
            "Epoch 13: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 87ms/step - loss: 0.2711 - acc: 0.9241 - val_loss: 0.7184 - val_acc: 0.8809\n",
            "Epoch 14/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2685 - acc: 0.9244\n",
            "Epoch 14: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.2685 - acc: 0.9244 - val_loss: 0.7202 - val_acc: 0.8804\n",
            "Epoch 15/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2642 - acc: 0.9254\n",
            "Epoch 15: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.2642 - acc: 0.9254 - val_loss: 0.7318 - val_acc: 0.8797\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.2591 - acc: 0.9266\n",
            "Epoch 16: val_loss did not improve from 0.63508\n",
            "231/231 [==============================] - 20s 86ms/step - loss: 0.2591 - acc: 0.9266 - val_loss: 0.7468 - val_acc: 0.8790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n"
      ],
      "metadata": {
        "id": "No8ze4gUhXTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder \n",
        "# Tensor for previous step's state \n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Reuse Embedding layer that was used in training phase \n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict next word.. \n",
        "# Previous time step's state -> current time step's initial state  \n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# Predict word in every time step \n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Updated decoder \n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "lBppG7vpUsFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # Iterate loop until stop_condition becomes True\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    \n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        \n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        \n",
        "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        \n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            \n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "oRu34aJ-WA_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Result"
      ],
      "metadata": {
        "id": "btMGRg6ShaKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "for seq_index in [3, 55, 1020, 300, 1001]:\n",
        "    reference = [[]]\n",
        "    candidate = []\n",
        "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "\n",
        "    input_sentence = seq_to_src(encoder_input_train[seq_index])\n",
        "    answer_sentence = seq_to_tar(decoder_input_train[seq_index])\n",
        "    translated_sentence = decoded_sentence[1:-5]\n",
        "\n",
        "    print(\"Input Sentence :\", input_sentence)\n",
        "    print(\"Answer Sentence: \", answer_sentence)\n",
        "    print(\"Translated Sentence :\", translated_sentence)\n",
        "\n",
        "    print(rouge.get_scores([translated_sentence], [answer_sentence], avg=True))\n",
        "    reference[0].extend(answer_sentence.split())\n",
        "    candidate.extend(translated_sentence.split())\n",
        "    print('BLEU-1 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))))\n",
        "    print('BLEU-2 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))))\n",
        "    print('BLEU-3 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))))\n",
        "    print('BLEU-4 score: {:.5f}'.format(bleu.sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))))\n",
        "\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "-9YuQOqh43_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4327a0b7-84fd-4421-c814-0487c6507ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 321ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Input Sentence : yeah you re right . \n",
            "Answer Sentence:  ouais vous avez raison . \n",
            "Translated Sentence : ouais vous avez raison . \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Input Sentence : i know tom is sensible . \n",
            "Answer Sentence:  je sais que tom est raisonnable . \n",
            "Translated Sentence : je sais que tom est raisonnable . \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Input Sentence : he gave the dog a bone . \n",
            "Answer Sentence:  il donna un os au chien . \n",
            "Translated Sentence : il est devenu un peu emeche . \n",
            "{'rouge-1': {'r': 0.42857142857142855, 'p': 0.42857142857142855, 'f': 0.4285714235714286}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.42857142857142855, 'p': 0.42857142857142855, 'f': 0.4285714235714286}}\n",
            "BLEU-1 score: 0.42857\n",
            "BLEU-2 score: 0.00000\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Input Sentence : i m sincere . \n",
            "Answer Sentence:  je suis sincere . \n",
            "Translated Sentence : je suis la . \n",
            "{'rouge-1': {'r': 0.75, 'p': 0.75, 'f': 0.749999995}, 'rouge-2': {'r': 0.3333333333333333, 'p': 0.3333333333333333, 'f': 0.3333333283333334}, 'rouge-l': {'r': 0.75, 'p': 0.75, 'f': 0.749999995}}\n",
            "BLEU-1 score: 0.75000\n",
            "BLEU-2 score: 0.33333\n",
            "BLEU-3 score: 0.00000\n",
            "BLEU-4 score: 0.00000\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Input Sentence : who do you live with ? \n",
            "Answer Sentence:  avec qui vis tu ? \n",
            "Translated Sentence : avec qui vis tu ? \n",
            "{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}\n",
            "BLEU-1 score: 1.00000\n",
            "BLEU-2 score: 1.00000\n",
            "BLEU-3 score: 1.00000\n",
            "BLEU-4 score: 1.00000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to previous LSTM model, we can see that in some case shows better performance with high difference. And some case shows lower performance relatively small difference. \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1Sc11jo7iYAcnHdiVDCi8-Xnw15jVexr6\" height = 350 width = 500>\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1hNKPiXj5ACMFf24Dp1ymDkH5urTOZwDZ\" height = 350 width = 450>"
      ],
      "metadata": {
        "id": "BrbKNdXsGbO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Result and Insights**\n",
        "\n",
        "Our final application is making voice with translated text."
      ],
      "metadata": {
        "id": "tp2GbEBYCjTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### inference with random braille text "
      ],
      "metadata": {
        "id": "nDBAZeq4IRRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzhBgKWUcHex",
        "outputId": "f1f8ad20-2688-4849-92bf-13546fbecbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "muN38XTRZGto",
        "outputId": "87b28809-e9d2-46ec-f573-6dee51753a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.3.0-py3-none-any.whl (26 kB)\n",
            "Collecting requests~=2.28.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 561 kB/s \n",
            "\u001b[?25hCollecting six~=1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting click~=8.1.3\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.0->gTTS) (1.24.3)\n",
            "Installing collected packages: six, requests, click, gTTS\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.1.3 gTTS-2.3.0 requests-2.28.1 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sErXhiB_ZHU5",
        "outputId": "b54011c7-ff0f-4700-8c8a-255d0f1ae0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.8.16)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Hi, everybody. Playing with Python is fun!!!\" # put translated text here\n",
        "\n",
        "tts = gTTS(text=text, lang='en')\n",
        "tts.save(\"helloEN.mp3\")"
      ],
      "metadata": {
        "id": "B9-oWqrMZjYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load('./helloEN.mp3')\n",
        "pygame.mixer.music.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "PbBH4_2calzZ",
        "outputId": "8507cd1c-afc5-4d0e-ced5-8d4cbcd5d0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-01d12ed32b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./helloEN.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: ALSA: Couldn't open audio device: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "[1] Han, Lifeng. \"Machine translation evaluation resources and methods: A survey.\" arXiv preprint arXiv:1605.04515 (2016).\n",
        "\n",
        "[2] Chimalamarri, Santwana, and Dinkar Sitaram. \"Linguistically enhanced word segmentation for better neural machine translation of low resource agglutinative languages.\" International Journal of Speech Technology 24.4 (2021): 1047-1053.\n",
        "\n",
        "[3] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n",
        "\n",
        "[4] Papineni, Kishore, et al. \"Bleu: a method for automatic evaluation of machine translation.\" Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.\n",
        "\n",
        "[5] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "pe68r1Q1EYcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Member's contribution statement**"
      ],
      "metadata": {
        "id": "A3apgZJC7PCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Debugging experience worth sharing**"
      ],
      "metadata": {
        "id": "WMhSVCTg7S6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Github repository with the commit history**"
      ],
      "metadata": {
        "id": "NPnwleLT7W5_"
      }
    }
  ]
}